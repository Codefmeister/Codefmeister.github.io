<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Codefmeister</title>
    <link>https://codefmeister.github.io/post/</link>
    <description>Recent content in Posts on Codefmeister</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://codefmeister.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Harris 角点</title>
      <link>https://codefmeister.github.io/p/harris-%E8%A7%92%E7%82%B9/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/harris-%E8%A7%92%E7%82%B9/</guid>
      <description>Moravec Detector Moravec角点检测是第一个提出兴趣点(interest points)的Paper。它的主要思想是：以每个像素为中心，有一个固定的滑动窗口。该方法计算并在八个方向上（纵横以及斜对角）搜索每个像素的最小强度变化，如果最小值大于给定阈值，则检测出感兴趣点。
其可以用数学公式表达为：$E(u, v)=\sum_{x, y} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}$
  $E(u,v)$代表像素中心$(x,y)$在偏移量$(u,v)$的方向上的强度变化。
  $w(x,y)$是一个指示函数，当$(x,y)$在滑动窗口内时，为1，若在滑动窗口外，则为0.
  $I(x,y)$指的是在像素点(x,y)处的光强或者说是灰度值。
  值得说明的是，这个式子是Harris总结的。
Harris Corner Detector Moravec存在着许多不足。非常重要的一点就是：由于对于灰度值变化的梯度判断是离散的进行在8个方向，所以不具有旋转不变性；同时还会出现误判，尤其是当一条线不平行于这八个方向时，线上的点也会被误检测为角点。
为了进一步改进Moravec角点检测，Harris提出了著名的Harris角点检测。
$$ \begin{aligned} &amp;amp;E(u, v)=\sum_{(x, y)} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}\
&amp;amp;\approx \sum_{(x, y)} w(x, y)\left[I(x, y)+\frac{\partial I}{\partial x}(x, y) u+\frac{\partial I}{\partial y}(x, y) v-I(x, y)\right]^{2}\qquad\text { (一阶泰勒展开) }\
&amp;amp;\approx \sum_{(x, y)} w(x, y)\left[\frac{\partial I}{\partial x}(x, y) u+\frac{\partial I}{\partial y}(x, y) v\right]^{2} \qquad \text { (消除重复项) }\</description>
    </item>
    
    <item>
      <title>Stochastic Matrix, Doubly Stochastic Matrix, Permutation Matrix</title>
      <link>https://codefmeister.github.io/p/stochastic-matrix-doubly-stochastic-matrix-permutation-matrix/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/stochastic-matrix-doubly-stochastic-matrix-permutation-matrix/</guid>
      <description>Stochastic Matrix  Definition:
In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain. Each of its entries is a nonnegative real number representing a probability. It is also called a probability matrix, transition matrix, substitution matrix, or Markov matrix. There are several different definitions and types of stochastic matrices
A right stochastic matrix is a real square matrix, with each row summing to 1.</description>
    </item>
    
    <item>
      <title>RPM-Net论文阅读笔记</title>
      <link>https://codefmeister.github.io/p/rpm-net%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/rpm-net%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>Abstract ICP算法可以解决刚体点云配准问题，其首先进行hard-assignment，寻找空间中的最近点对的对应关系。其次再解决最小二乘问题。 基于空间最近距离的hard-assignment 对于初始的位姿以及噪音和离群点十分敏感，所以鲁棒性不高，经常会收敛到局部最优。
在此Paper中，作者提出了RPM-Net，一个对于初始刚体变换不敏感，同时更加鲁棒的，基于深度学习的点云配准方法。为了达到这个目的，作者使用了可微分的Sinkhorn层，随后利用从空间坐标和局部几何结构中学习得到的混合特征，退火(annealing)得到点对间的soft assignment软匹配结果。 同时，为了进一步的提高配准的精度，我们引入了一个二次网络，用于预测最优的退火(annealing)参数。 不同于目前已有的方法，RPM-Net可以处理缺失对应点对关系和部分重叠的点云。实验结果表明，RPM-Net可以达到state-of-art。
Introduction 点云配准问题，是指给定的两帧未知对应点对关系的点云，寻找刚体变换关系，将其配准在一起。不论是获得点对之间的对应关系，还是得到刚体变换参数，都会使剩下的问题微不足道。
ICP，广泛应用。 对初始变换关系和噪音和离群点敏感，容易收敛到局部最优。ICP算法的深度学习实现(Deep Closest Point)通过深度学习得到的特征来进行对应点对关系，使得其对初始位姿不敏感，但其仍对outliers不鲁棒， 同时对于部分重叠的点云无法很好的工作。
为了解决ICP的问题，人们提出了许多方法。其中非常突出的一篇便是&amp;quot;RPM&amp;quot;，Robust Point Matching，它首先对点对对应关系进行soft assignment， 然后逐步通过确定的退火策略一步步harden 对应关系。纵然RPM比ICP更加鲁棒，但其仍然对于初始刚体变化十分敏感，容易陷入局部最优，原因在于其点对对应关系只是单独的从空间距离中得到的。另一方面，基于特征的方法避免了初始位姿的问题，其通过挖掘独特的keypoint，同时使用特征描述符对keypoint局部几何特征进行描述。使用这些keypoint进行match， 然后使用鲁棒的RANSAC（随机抽样一致性）策略，来鲁棒地计算出刚体变换关系。此类方法只对几何特征显著的点云效果很好。
此Paper中提出了，基于深度学习的RPM策略，RPM-Net： 一个端到端的可微分的深度网络，不但保留了RPM对于噪音和离群点的鲁棒性，同时从学习到的特征距离而不是spatial的点对对应关系来对初始化进行脱敏处理。为了达到此目的，我们设计了一个特征提取网络，从逐点的空间坐标以及几何特征中，计算得到其混合特征。随后使用Sinkhorn层与退火策略，从混合特征中得到soft assignment.空间坐标与几何属性的混合，与从数据中的学习过程，改进了点对关系。这对初始化刚体变换关系进行了脱敏处理，同时增加了对于缺失对应点对关系以及部分重叠的点云之间的配准能力。类似于ICP算法以及其变种，RPM-Net也是迭代地对刚体变换进行求精。进一步，我们引入了一个子网络，基于当前的配准状态，来预测最优的退火参数。也就是说，我们的退火策略并不是固定好的某个模式，而是在学习过程中动态生成的。因为使用了混合特征，我们的算法可以在很小的几次迭代后就收敛。
其贡献:
 其用于配准的网络架构 其引入的用于预测退火参数的子网络 提出一个改良的倒角距离度量Modified Chamfer distance metric， 用于度量部分重叠的配准质量。 对比实验  Background 该工作是基于RPM框架完成的，这里将简单介绍RPM的工作。
首先定义一个Match Matrix匹配矩阵$\mathbf{M}={0,1}^{J \times K}$, 来表示点的对应关系的分配。其中每个元素： $$ m_{j k}=\left{\begin{array}{ll} 1 &amp;amp; \text { if point } \mathbf{x}_{j} \text { corresponds to } \mathbf{y}_{k} \
0 &amp;amp; \text { otherwise } \end{array}\right. \tag{1} $$</description>
    </item>
    
    <item>
      <title>FCGF论文阅读笔记</title>
      <link>https://codefmeister.github.io/p/fcgf%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/fcgf%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>0. Abstract 从三维点云或者扫描帧中提取出几何特征是许多任务例如配准，场景重建等的第一步。现有的领先的方法都是将low-level的特征作为输入，或者在有限的感受野上提取得到基于patch的特征。本文提出的是一个全卷积几何特征提取网络，名为fully-convolutional geometric features。 通过一个3D的全卷积网络的一次pass，即可得到几何特征。 同时提出了一个新的度量学习的loss函数，可以显著的提高网络的性能。 FCGF的几何特征十分紧凑，可以捕捉广阔的上下文空间，并缩放到大型场景中。在室内（3D Match）和室外（KITTI）数据集上进行验证的结果显示，其达到了state-of-art的精确率，而且并不需要数据的预处理，并且很紧凑（32维的特征），比其余最精确的方法快290倍。
1. Introduction 寻找几何意义上的点对关系是很多三维任务十分重要的一步。因此，大量的工作集中于设计三维特征，以捕捉具有可判别性的局部几何机构，来建立点对关系。
基于学习的三维特征由于其鲁棒性与出色的性能表现，在最近得到了广泛的关注。现有的基于学习的特征提取工作大都依赖于低阶的几何特征作为输入，例如角度偏差，点的分布，或者体积距离函数等。随后，对每个兴趣点(point of interest)提取一个三维patch，然后通过多层感知机或者三维卷积层将之映射到一个低维的特征空间。这个过程计算代价高昂，而且只能够提取得到降采样后兴趣点处的特征，因此会降低后续配准步骤中的空间分辨率。
上述的基于patch的处理过程效率很低，因为其中间网络的激活结果并没有在相邻的patch上进行复用。用2D卷积进行类比，对某个兴趣点提取其三维patch与对某个像素块提取其周围的一个像素patch类似。不仅如此，现有的pipeline仅局限于对空间范围有限的patch进行卷积，限制了空间语境的解读。
不同于上文所述，我们应用一个可以作用于整个输入的三维卷积操作，而不需要裁剪片段，该操作是通过将卷积转换为全卷积的子项来完成的（convolution组装得到一个fully convolution）。 相似的，我们将MLP中的全连接层用一系列卷积层来替代，其卷积核的size为1x1x1。 全卷积网络与非全卷积网络相比，可以捕捉更广阔的上下文，更快，内存效率更高。其原因在于中间的激活结果在重叠的区域上进行了复用。
尽管有这些优势，全卷积网络因为三维数据的(一些)特点并未在三维特征提取中得到广泛的应用。一个对三维数据进行卷积的卷积网络的标准输入代表是一个稠密的四维tensor，其中三维是空间维度，还有一个特征维。这种表示方式对内存消耗很大，很多voxel都是空的。
在本文的工作中，采用了一种稀疏的tensor表示法。同时，针对全卷积上的度量学习，提出了一个新的loss函数。因为观察到全卷积特征不同于传统的度量学习的假设，传统的度量学习由于锚点都是随机采样的，所以假设样本是独立同分布的，而在全卷积网络中，相邻的点的特征是高度相关的。并不符合独立同分布的假设，所以需要重新设计loss函数。同时，该方法并不需要对数据进行低阶的预处理，或者提取三维patch，可以快速的生成高分辨率的， 具有state-of-art的判别潜力的特征。
FCGF在室内室外数据集上均进行了测试，可以达到state-of-art的性能表现，比最快的快9倍，比最好的快290倍。
2. Related Work Hand-craft 3D feature：早期对三维特征的描述集中在手工的，能够有区别的（有基于feature鉴别的潜力 discriminatively）的对局部几何特征进行刻画的描述符。Spin Images [16] use a projection of adjacent points onto the tangent plane. USC [29] uses covariance matrices of point pairs. SHOT [26] creates a 3D histogram of normal vectors. PFH [24] and FPFH [23] build an oriented histogram using pairwise geometric properties.</description>
    </item>
    
    <item>
      <title>Learning Multiview 3D point Cloud Registration</title>
      <link>https://codefmeister.github.io/p/learning-multiview-3d-point-cloud-registration/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/learning-multiview-3d-point-cloud-registration/</guid>
      <description>Learning multiview 3D point cloud registration Abstract 提出了一种全新的，端到端的，可学习的多视角三维点云配准算法。 多视角配准往往需要两个阶段：第一个阶段进行初始化配准，给定点云各帧之间两两的初始化刚体变换关系；第二个阶段在全局意义上进行不断精细化处理。前者往往由于点云之间的低重叠率，对称性，或者重复的场景片段而导致配准精度较差。因此，紧随其后的全局优化（Global Refinement）的目标就是在多个点云帧中建立一种循环一致性(cyclic consistency).
而此文章提出了一种算法，将两个阶段融合在一起进行端对端的交替学习。在公认的基准数据集上的实验评估表明，其方法显著优于state-of-art，而且又是端到端可训练的，所需要的计算资源也更少。此外，其还进行了详细的分析和烧蚀试验去验证其方法的novel part.
1. Introduction 三维计算机视觉的下游任务，如语义分割和目标检测，通常需要场景的整体表示。因此，将仅覆盖环境一小部分的单个点云配准和融合为一个全局一致的完整表示的能力是十分重要的，而且在增强现实和机器人技术中有着不少用例。相邻片段之间的双视角配准是一个被深入研究过的问题，传统的基于几何约束[51, 66, 56] 和手工设计的特征描述符[37, 27, 54, 69]的配准方法在某种程度上取得了成功。然而， 近些年，用于双视角三维点云配准的局部描述符的研究聚焦于深度学习方法[67: 3DMatch, 38, 21: Ppfnet, 64, 19: Ppf-foldnet, 28: perfect match]，这些方法成功捕捉并编码了隐藏在手工特征符下的证据。在此基础上，一些全新的端到端的双视角点云配准方法最近被提出[62: DCP, 42: Deepvcp]。 虽然双视角配准在很多任务中展现出了不错的性能，但对场景中的多个点云帧进行配准时，其存在一些概念上的缺陷：(1) 相邻点云之间的低重合率会导致不精确或者错误的匹配。(2) 点云配准必须依赖于非常局部的特征，对于3D场景结构简单或者重复结构较多的情况十分有害。(3)在两两配准之后，需要进行单独的处理来将所有双视角配准结果组合为一个全局表示。与双视角配准相比，应用于无组织的点云片段上的全局一致的多视角配准方法能够更充分的从深度学习技术取得的进步中获益。 现有的领先方法仍然常常依赖于双视角映射的良好初始化（良好的初值），然后再在后续的步骤中通过一系列解耦的步骤进行全局优化。这种分层处理的一大缺点在于，姿态图所有节点上的全局噪声分布在配准结束后远不是随机的。也就是说，由于配准结果和初始的双视角映射高度相关，会存在着不可忽视的误差。
在这篇论文中，作者提出了第一个端到端的，数据驱动的多视角点云配准算法。其方法以可能存在重叠关系的点云集合为输入，对每个点云帧输出一个刚体变换矩阵。我们从传统的两阶段方法中跳脱出来，让各个阶段彼此分离，直接学习以一种全局一致的方式对所有点云帧进行配准。
其工作的主要贡献在于：
  将传统的两阶段方法用端到端的神经网络的方式阐述，在其前传过程中，主要解决了两个可微分的最优化问题：(i)对两两点云之间刚体变换参数估计的Procrustes问题。(ii) 刚体变换同步的谱松弛(spectral relaxation)问题
  提出了一个置信度估计模块，其使用了一个新颖的overlap pooling layer重叠池化层来预测估算得到的双视角刚体变换参数的可信度。
  将多视角三维点云配准问题转换为迭代重加权最小二乘问题(IRLS)，迭代地优化两两配准之间的刚体变换估计和绝对坐标意义下的刚体变换估计（全局）。
  因为以上所提到的工作，所提出的多视角点云配准算法是(i) 计算效率很高 (ii) 可以达到更加精确的配准结果，因为残差会以一种迭代的方式被送回双视角配准网络中去。 (iii)不论是双视角配准还是多视角配准，都比现有的方法的精度要高，效果要好。
2. Related Work Pairwise registration: 传统的双视角配准pipeline包含两个阶段： the coarse alignment stage（粗配准）， 为相对刚体变换参数提供一个初始估计；the refinement stage 通过迭代最小化配准误差，不断优化刚体变换参数。</description>
    </item>
    
    <item>
      <title>PointConv论文阅读笔记</title>
      <link>https://codefmeister.github.io/p/pointconv%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/pointconv%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>Abstract 本文发表于CVPR。 其主要内容正如标题，是提出了一个对点云进行卷积的Module，称为PointConv。由于点云的无序性和不规则性，因此应用卷积比较困难。
其主要的思路是，将卷积核当做是一个由权值函数和密度函数组成的三维点的局部坐标的非线性函数。通过MLP学习权重函数，然后通过核密度估计得到密度函数。
还有一个主要的贡献在于，使用了一种高效计算的方法，转换了公式的计算分时，使得PointConv的卷积操作变得memory efficient，从而加深网络的深度。
This paper first published on CVPR. In this paper, author proposed a novel convolution operation which can be directly used on point cloud. As we all know, unlike image whose pixels are fixed, point cloud is irregular and unordered. So directly extend convolution operation into 3D pointcloud can be difficult.
Author treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight function and density function.</description>
    </item>
    
    <item>
      <title>torch中Dataset的构造与解读</title>
      <link>https://codefmeister.github.io/p/torch%E4%B8%ADdataset%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch%E4%B8%ADdataset%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E8%AF%BB/</guid>
      <description>torch中Dataset的构造与解读 Dataset的构造 要自定义自己的数据集，首先需要继承Dataset(torch.utils.data.Dataset)类.
继承Dataset类之后，必须重写三个方法:__init__(), __getitem__(), __len__()
class ModelNet40(Dataset): def __init__(self, xxx): ... def __getitem__(self, item): ... def __len()__(self): ... 解读 单看上面的构造结构与三个需要重写的方法可能会一头雾水。我们详细分析其作用：
  __init__的作用 __init__的作用与所有构造函数都一样，初始化一个类的实例。定义类的实际属性，如点云数据集中的unseen, guassian_noise等，是True还是False， 取出所有数据存储为成员变量等等。
  __getitem__的作用 __getitem__的作用是，根据item的值取出数据。 item实际上就是索引值，会由Dataloader自动从0一直递增到__len__中取出的值。
  __len__的作用 __len__的作用是，相当于返回整体数据data的shape[0]， 即给item的递增指定一个范围。
  例子 class ModelNet40(Dataset): def __init__(self, num_points, partition=&amp;#39;train&amp;#39;, gaussian_noise=False, unseen=False, factor=4): self.data, self.label = load_data(partition) self.num_points = num_points self.partition = partition self.gaussian_noise = gaussian_noise self.unseen = unseen self.label = self.label.squeeze() self.factor = factor if self.</description>
    </item>
    
    <item>
      <title>numpy.cross函数解析</title>
      <link>https://codefmeister.github.io/p/numpy.cross%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/numpy.cross%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</guid>
      <description>numpy.cross 语法 numpy.cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None)
功能 Return the cross product of two (arrays of) vectors. The cross product of a and b in :math:R^3 is a vector perpendicular to both a and b. If a and b are arrays of vectors, the vectors are defined by the last axis of a and b by default, and these axes can have dimensions 2 or 3. Where the dimension of either a or b is 2, the third component of the input vector is assumed to be zero and the cross product calculated accordingly.</description>
    </item>
    
    <item>
      <title>np.clip作用解析</title>
      <link>https://codefmeister.github.io/p/np.clip%E4%BD%9C%E7%94%A8%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/np.clip%E4%BD%9C%E7%94%A8%E8%A7%A3%E6%9E%90/</guid>
      <description>np.clip分析 语法 np.clip(a, a_min, a_max, out=None, **kwarys)
作用 对数组a进行裁剪，小于a_min 的数用a_min代替， 大于a_max的数用a_max代替。
需要注意的是，函数本身并不会对a_min和a_max之间的大小进行检查。
参数  a: 待裁剪的数组 a_min: 下界 a_max: 上界  示例 &amp;gt;&amp;gt;&amp;gt; a = np.arange(10)&amp;gt;&amp;gt;&amp;gt; print(a)array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&amp;gt;&amp;gt;&amp;gt; b = np.clip(a, 3, 7)&amp;gt;&amp;gt;&amp;gt; print(b)array([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])官方文档 Clip (limit) the values in an array.
Given an interval, values outside the interval are clipped tothe interval edges.</description>
    </item>
    
    <item>
      <title>np.random中各函数一览</title>
      <link>https://codefmeister.github.io/p/np.random%E4%B8%AD%E5%90%84%E5%87%BD%E6%95%B0%E4%B8%80%E8%A7%88/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/np.random%E4%B8%AD%E5%90%84%E5%87%BD%E6%95%B0%E4%B8%80%E8%A7%88/</guid>
      <description>np.random.uniform() 语法 `numpy.random.uniform(low=0.0, high=1.0, size = None)
作用 返回一个均匀分布的采样结果。 左闭右开区间[low, high).
返回数组的shape与size相同。
参数  low: 下界，默认为0.0 high: 上界，默认为1.0 size: 返回数组的shape， 默认为None，即返回一个单值  举例 &amp;gt;&amp;gt;&amp;gt; a = np.random.uniform(1,2,(3,4))&amp;gt;&amp;gt;&amp;gt; print(a)array([[1.81297209, 1.79414559, 1.24677702, 1.44857774],[1.9171547 , 1.84473086, 1.33114168, 1.95953694],[1.66085822, 1.30895404, 1.1047299 , 1.6256421 ]])np.random.randn() 语法 numpy.random.randn(d0, d1, ..., dn)
作用 返回一个shape为(d0, d1, ..., dn)的正态分布采样。分布的均值为0,方差为1. 如果没有提供参数，则返回单个值的采样。
举例 &amp;gt;&amp;gt;&amp;gt; a = np.random.randn(5,6)&amp;gt;&amp;gt;&amp;gt; print(a)array([[-0.47617937, -0.43465103, 0.14896871, 0.21132357, 0.2143598 ,-0.03354328],[-0.</description>
    </item>
    
    <item>
      <title>torch.detach()</title>
      <link>https://codefmeister.github.io/p/torch.detach/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.detach/</guid>
      <description>torch.detach() 作用 Returns a new Tensor, detached from the current graph.
返回一个新的Tensor， 从原有的图中剥离。
The result will never require gradient.
并且该Tensor不自动计算梯度。</description>
    </item>
    
    <item>
      <title>glob 函数作用分析</title>
      <link>https://codefmeister.github.io/p/glob-%E5%87%BD%E6%95%B0%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/glob-%E5%87%BD%E6%95%B0%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
      <description>Python 中 glob 作用简介 glob库中有两个函数:glob.glob()， glob.iglob(). 其作用是：遍历给定文件夹下所有符合条件的文件。
常用的匹配符有：
 * 代表所有 ? 代表满足单个字符 []代表满足list中指定的字符  glob.glob(path, *, recursive=False) 在这里只介绍最基础的用法，用到的时候再深究。
partition = &amp;#34;train&amp;#34; path_list = glob.glob(os.path.join(DATA_DIR, &amp;#39;modelnet40_ply_hdf5_2048&amp;#39;, &amp;#39;ply_data_%s*.h5&amp;#39; % partition)) 上述代码片段的意思是，寻找&amp;quot;DATADIR/modelnet40_ply_hdf5_2048/&amp;ldquo;下所有叫做&amp;quot;ply_data_train*.h5&amp;quot;的文件，并将其打包为一个**列表list**返回。
如可能返回的是结果是：
[&amp;ldquo;ply_data_train0.h5&amp;rdquo;, &amp;ldquo;ply_data_trainTx.h5&amp;rdquo;]
glob.iglob(path, *, recursive=False) 作用与上面的函数一致。但是返回的不是list,而是一个iterable的迭代器。</description>
    </item>
    
    <item>
      <title>torch.stack作用分析</title>
      <link>https://codefmeister.github.io/p/torch.stack%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.stack%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
      <description>torch.stack作用分析 语法  torch.stack(tensors, dim=0, *, out=None) &amp;ndash;&amp;gt; Tensor
 作用  Concatenates a sequence of tensors along a new dimension.
All tensor need to be of the same size
 将一个序列的tensor在新的一维上concatenate起来，所有tensor的shape需要相同。
Parameters  tensors(sequence of Tensors) &amp;ndash; sequence of tensors to concatenate dim(int) &amp;ndash; dimension to insert. Has to be between 0 and the number of dimensions of concanated tensors.  Keyword Arguments  out(Tensor, optional) &amp;ndash; the output tensor.</description>
    </item>
    
    <item>
      <title>torch.nn.parameter.Parameter分析</title>
      <link>https://codefmeister.github.io/p/torch.nn.parameter.parameter%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.nn.parameter.parameter%E5%88%86%E6%9E%90/</guid>
      <description>torch.nn.parameter.Parameter 作用  a kind of Tensor that is to be considered a module parameter.
 Parameter是一种可以作为模型参数的Tensor.
 Parameters are Tensor subclasses, that have a very special property when used with Module S &amp;mdash;-when they&amp;rsquo;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn&amp;rsquo;t have such effect.
 Parameter是Tensor的子类，同时拥有一种非常特殊的性质：当他们与Module S一起使用时，也就是说当它们作为Module参数进行使用时，它们会自动添加到Module的参数列表中，并且出现在parameters()迭代器里。(这样就可以自动计算梯度等)
构造参数  data(Tensor)&amp;ndash; parameter tensor requires_grad(bool, optional)&amp;ndash; if the parameter requires gradient.</description>
    </item>
    
    <item>
      <title>DCP论文阅读笔记</title>
      <link>https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>DCP论文阅读笔记 论文  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution 基于ICP迭代最近点算法，提出基于深度学习的DCP算法。解决了ICP想要采用深度学习方法时遇到的一系列问题。
我们先回顾一下ICP算法的基本步骤：
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data 概括起来就是： 寻找最近点对关系，使用SVD求解刚体变换。如此循环往复。
结合论文，个人理解将ICP算法扩展到深度学习存在着以下的难点（可能存在各种问题，笔者深度学习的相关知识很薄弱）：
 首先，点对关系如果是确定的话，沿着网络反向传播可能存在问题。 SVD分解求解刚体变换，如何求梯度？(Confirmed by paper)  而文章克服了这些问题，主要有如下贡献：
 提出了能够解决传统ICP算法试图推广时存在的困难的一系列子网络架构。 提出了能进行pair-wise配准的网络架构 评估了在采用不同设置的情况下的网络表现 分析了是global feature有用还是local feature对配准更加有用  网络架构 模型包含三个部分：
(1) 一个将输入点云映射到高维空间embedding的模块，具有扰动不变性（指DGCNN当点云输入时点的前后顺序发生变化，输出不会有任何改变） 或者 刚体变换不变性（指PointNet对于旋转平移具有不变的特性）。该模块的作用是寻找两个输入点云之间的点的对应关系. 可选的模块有PointNet（Focus于全局特征）， DGCNN（结合局部特征和全局特征）。</description>
    </item>
    
    <item>
      <title>torch.unsqueeze()解读</title>
      <link>https://codefmeister.github.io/p/torch.unsqueeze%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.unsqueeze%E8%A7%A3%E8%AF%BB/</guid>
      <description>torch.unsqueeze()函数解读 语法 torch.unsqueeze(input, dim) --&amp;gt; Tensor
Parameters   input(Tensor) &amp;ndash; the input tensor
  dim(int) &amp;ndash; the index at which to insert the singleton dimension
  功能  Return a new tensor with a dimension of size one inserted at the specified position.
 返回一个新的tensor，在指定的位置插入维度为1的一维。
 The returned tensor shares the same underlaying data with this tensor.
 返回的这一Tensor在内存中是和原Tensor共享一个内存数据的。(可以用contiguous来重新分配)
 A dim value within the range [-input.dim() - 1, input.</description>
    </item>
    
    <item>
      <title>DGCNN论文解读</title>
      <link>https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Mon, 21 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</guid>
      <description>DGCNN 前言 因为关心的领域主要是配准，对于分类等网络的架构设计分析并没有侧重太多，主要侧重的是EdgeConv的思想。
论文  Dynamic Graph CNN for Learning on Point Clouds, Wang, Yue and Sun, Yongbin.
 核心思想:关于EdgeConv 将点云表征为一个图，${\rm{G}}(V,\xi )$ ,点云的每一个点对应图中的一个结点，而图中的每一条边对应的是点之间的特征feature，称为Edge-feature。举个例子，最简单的情景，可以通过KNN来构建图。Edge Feature用$e_{ij}$来表示，定义为： $$ e_{ij} = h_{\Theta}(x_i,x_j) $$ $$ h_{\Theta}: {R^F} \times {R^F} \to {R^{F&#39;}} $$ $h_{\Theta}$是一个非线性的映射，拥有一系列可学习的参数。
提出了一个名为EdgeConv的神经网络模块Module，该模块基于卷积神经网络，可以适应在点云上的高阶任务。EdgeConv的对于第i个顶点的输出为：
其中$□$代表的是一个对称聚合函数，如$\Sigma, max$。
可以将上述描述类比为在图像上的卷积操作。我们把$x_i$看作是中心像素点，而$x_j:(i,j) \in \xi$可以看做是围绕在点$x_i$周围的像素($x_j$事实上就是和$x_i$之间存在着feature edge的点）。所以类比这样的卷积操作，Edge-Conv可以将n个点的$F$维点云通过“卷积”转换为具有n个点的$F&#39;$维的点云。 所以选择$h$和$□$就变得十分关键。它会直接影响EdgeConv的性能特性。
一些其他的选择在下一个小part中讨论。在本文中，作者采用的： $$ h_{\Theta}(x_i,x_j) = {\bar h}_{\Theta}(x_i, x_j - x_i) $$
从这个表达式可以非常明显的看出，既结合了全局形状结构，也结合了局部的结构信息。Global shape structure通过$x_i$捕捉，local neighborhood information通过$x_j - x_i$来捕捉。
更具体一点的说，通过如下两个公式来计算edge_feature以及x&#39;： $$ e_{ijm}&#39; = ReLU(\theta_m \cdot(x_j - x_i) + \phi_m \cdot x_i) $$ $$ x_{im}&#39; = \mathop {\max }\limits_{j:(i,j) \in \xi }e_{ijm}&#39; $$</description>
    </item>
    
    <item>
      <title>图解Transformer</title>
      <link>https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/</link>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/</guid>
      <description>图解Transformer  Reference: The Illustrated Transformer
 本文自译用于加深理解与印象。
关于注意力机制，可以参考先前的Seq2Seq Model with Attention
Transformer是论文Attention is All You Need提出的。在这篇文章中，我们将尝试把事情弄得简单一点，逐个介绍概念，以便更好理解。
A High-Level Look 我们首先把模型看作是一个黑箱。在机器翻译领域的应用中，输入一种语言的一个句子，会输出另外其翻译结果。 揭开盖子，我们能够看到一个编码组件encoding component，一个解码组件decoding component，还有其之间的连接关系connections。 编码组件是一堆编码器构成的（Paper中堆叠了六个编码器，六个并没有什么说法，你也可以尝试其他数字）。解码组件也是由一堆解码器构成的（数量与编码器相同）。 所有编码器在结构上都是相同的，然而他们并不共享参数（或权重）。 每一个都可以被拆分为两个子层sub-layers。 编码器的输入首先流过self-attention层，self-attention层可以帮助我们在对某个特定的词进行编码的时候同时关注到句子中其他位置单词的影响。
self-attention层的输出被送往feed-foward neural network，即前馈神经网络层。完全相同的前馈网络，独立地作用于每一个位置position上。
解码器也有上述这两个层，但除此以外，在这两层之间，还有一个attention layer，帮助解码器更加关注输入句子中相关的部分。（作用类似于Seq2Seq中的注意力机制的作用。） Bringing The Tensor Into The Picture 现在，我们已经了解了模型的主要组件，下面让我们开始研究各种矢量/张量以及它们如何在这些组件之间流动，以将经过训练的模型的输入转换为输出。
首先我们将每一个输入单词通过embedding algorithm转换为一个词向量。 嵌入过程只发生在最底部的encoder。对于所有的编码器Encoder，他们都接受一个size为512的向量列表作为输入。只不过对于最底部的Encoder，其输入为单词经过嵌入后得到的词向量，而其他的Encoder的输入，是其下方一层Encoder的输出。列表的size是一个我们可以设定的超参数——通常来讲它会是我们训练集中最长的一个句子的长度。
在将输入序列中的单词进行Embedding之后，他们中的每一个都会流过编码器的两层。 从这里我们可以看到一个Transformer非常重要的特性，那便是每一个位置上的单词在Encoder中自己的路径上各自流动。在self-attention层中，这些路径之间存在相互依赖。而前馈层feed-forward中彼此间并无依赖。所以在流经前馈层的时候，可以进行并行化处理。
下面我们将举一个短句的例子，然后观察sub-layer上发生了什么。
Now We&amp;rsquo;re Encoding 像我们先前提到的，一个编码器接收一个向量列表作为输入。这个向量列表首先被送往self-attention层，然后再送往feed-forward前馈层。处理结束后将其output送往下一个Encoder。  每个位置的单词都被送往一个self attention层，然后再穿过一个前馈神经网络——每个向量独立穿过这个完全相同的网络。
 Self-Attention at a High Level self-attention是Paper中提出的一个全新概念，不要被其简单的命名给迷惑。
假设我们输入了如下一个句子，并试图进行翻译：
&amp;ldquo;The animal didn&amp;rsquo;t cross the street because it was too tired&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Conv1d与Conv2d</title>
      <link>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</link>
      <pubDate>Wed, 16 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</guid>
      <description>Conv1d与Conv2d 本文分为几个部分来详解Conv2d与Conv1d。主要侧重于Conv2d
前言 本文记于2020年12月15日，起因是DGCNN中部分卷积使用了二维卷积，部分卷积使用了一维卷积。加之之前对Conv2d与Conv1d属于一种迷迷糊糊的状态，趁着这个机会弄清楚。
Conv2d原理（二维卷积层） 二维互相关运算 互相关运算与卷积运算 虽然卷积层得名于卷积(convolution)运算，但所有框架在实现卷积层的底层，都采用的是互相关运算。实际上，卷积运算与互相关运算类似，为了得到卷积运算的输出，我们只需要将核数组左右翻转并上下翻转，然后再与输入数组做互相关运算。所以这两种运算虽然类似，但是输出并不相同。
但是由于深度学习中核数组都是学习得到的，所以卷积层无论使用互相关运算还是卷积运算，都不影响模型预测时的输出。也就是说我们用卷积运算学出的核数组与用互相关运算学出的核数组两者之间可以通过上下翻转，左右翻转来相互转换。所以在框架乃至于绝大部分深度学习文献中，都使用互相关运算来代替了卷积运算。
互相关运算 在二维卷积层中，一个二维输入数组和一个二维核(kernel)数组通过互相关运算输出一个二维数组。举个例子来解释二维互相关运算：
假设输入数组的高和宽均为3， 核数组的高和宽均为2，该数组在卷积运算中又称为卷积核或者过滤器(filter)。 19是这样得出的： $19 = 0\times0 + 1\times1 + 3\times2 + 4\times3$ 。
卷积窗口从输入数组的最左上方开始，按照从左往右，从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和。得到输出数组中对应位置的元素。
二维卷积层 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包含了卷积核和标量偏差。我们在训练模型的时候，通常先对卷积层进行随机的初始化，然后不断迭代卷积核和偏差。
卷积窗口形状为$p \times q$的卷积层称为$p \times q$卷积层。
特征图与感受野 二维卷积层输出的二维数组可以看做是输入在空间维度上(宽和高)上某一级的表征，也叫特征图(feature map)。影响元素$x$的前向计算的所有可能输入区域(甚至可能大于输入的实际尺寸)叫做$x$的感受野(receptive field)。以上图为例，图中输入的阴影部分的四个元素就是输出数组中阴影部分元素的感受野。如果我们将该输出再和一个$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么$z$在输入上的感受野包含全部的9个元素。
可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。
填充与步幅 卷积层的输出形状由输入形状和卷积核窗口形状决定，通过填充与步幅，我们可以改变给定形状的输入和卷积层下的输出形状。
填充 填充padding是指在输入高和宽的两侧填充元素(通常是0元素)。如下图： 假设输入形状为$n_h \times n_w$， 卷积核窗口形状是$k_h \times k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是： $$ (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1) $$ 很多情况下我们会设置$p_h = k_h -1$和$p_w = k_w - 1$来使得输入输出具有相同的高和宽。</description>
    </item>
    
    <item>
      <title>LeakyReLU函数解析</title>
      <link>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 14 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</guid>
      <description>LeakyReLU 语法  CLASS torch.nn.LeakyReLU(negative_slope: float = 0.01, inplace: bool = False)
 作用 Element-wise
对于每个x，应用函数如图： 函数图像 </description>
    </item>
    
    <item>
      <title>MATrICP论文解读</title>
      <link>https://codefmeister.github.io/p/matricp%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Thu, 03 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/matricp%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</guid>
      <description>MATrICP 论文  Improved techniques for multi-view registration with motion averaging
Li, Zhongyu Zhu, Jihua Lan, Ke Li, Chen Fang, Chaowei
 核心思想 将Trimmed ICP与运动平均算法结合起来，应用到多视角聚类上。
算法步骤 1. 估算各帧之间的重叠百分比$\xi_{i,j}$ 总的来说，估算各帧之间的重叠百分比主要分为两步：
(1) 对于每一帧，计算其$d_{threshold}$
(2) 计算出每一帧的$d_{threshold}$之后，使用该参数计算该帧与其他帧的重叠百分比。
1.0 背景知识 ObjFunc：
1.1 计算$d_{threshold}$ 对于第i帧的每一个点，可以在其他所有帧中寻找到N-1个对应点（通过NN），假设第i帧有$N_i$个点，那么一共会有$N_i * (N-1)$个点对与距离。因为我们是要进行多视角配准的，相当于把当前帧作为源scan，其他所有帧组成的模型作为目标模板进行配准。所以将这些所有距离按照从小到大进行排序，然后依次对于每一个距离，计算该距离以及之前所有距离对应的ObjectFunction值。可以使用cumsum操作。结果是得到同样长度的ObjectFunction值的数组，取其中的最小值，该目标函数最小值对应着一个距离$d_i$，这个距离$d_i$就可以作为第i帧的$d_{threshold}$，用于第i帧与其他帧（第j帧）的重叠率估算。
1.2 计算第i帧与其他帧的重叠百分比 对于第$i$帧，我们现在有其$d_{threshold}$。那么求$\xi_{i,j}$，即为：使用NN寻找点对pair$(P_i,P_j)$，然后从小到大排列，取$d &amp;lt; d_{threshold}$的部分。假设有$N_j^{&#39;}$个点对满足要求。那么重叠百分比$\xi_{i,j} = N_j^{&#39;} / N_j$，$N_j$为第j帧的点。
2. 根据估算得到的${\xi_{i,j}}$，选择重叠率高的scan pair，应用TrICP算法求解其relative Motion $M_{i,j}$ 2. 应用运动平均算法 在应用Motion Average前，我们已经有了初始的Global Motion以及一系列Relative Motion。
运动平均的主要思想是，将relative Motion看作是global Motion的某种组合。先求出$\Delta M_{i,j}$，将其转换为李代数对应的6x1的向量。然后通过Average的思想，求出global Motion的变化值。
2.1 计算relative motion $M_{i,j}$的变化值 通过global motion，可以求出$\Delta M_{i,j}$ $$ \Delta M_{i,j} = M_i^0 M_{i,j} {(M_j^0)}^{-1} $$</description>
    </item>
    
    <item>
      <title>np.transpose()详解</title>
      <link>https://codefmeister.github.io/p/np.transpose%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Sat, 21 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/np.transpose%E8%AF%A6%E8%A7%A3/</guid>
      <description>ndarray的转置(transpose) 对于A是由np.ndarray表示的情况：
可以直接使用命令A.T。
也可以使用命令A.transpose()。
A.T 与 A.transpose()对比 结论: 在默认情况下，两者效果相同，但transpose()可以指定交换的axis维度。
对于一维数组，两者均不改变，返回原数组。
对于二维数组，默认进行标准的转置操作。
对于多维数组A,A.shape为(a,b,c,d,...,n)，则转置后的shape为(n,...,d,c,b,a)。
对于.transpose()，可以指定转置后的维度。语法：A.transpose((axisOrder1,...,axisOrderN))，其效果等同于np.transpose(A,(axisOrder1,...,axisOrderN)),(axisOrder)中是想要得到的索引下标顺序。效果详见例子。
Example： 二维默认情况下： A = np.array([[1,2],[3,4]]) print(A) print(A.T) print(A.transpose()) 结果如下：
多维默认情况下： a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]]) print(a.shape) print(a.T.shape) print(a.transpose().shape) 结果如下：
指定维度情况： a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]]) print(a.shape) print(a.transpose(1,2,0).shape) A = np.transpose(a,(1,2,0)) print(A.shape) 结果如下：
从截图中可以看出，a.transpose(1,2,0)与np.transpose(a,(1,2,0))效果相同。代码段中给出的axes是(1,2,0)，这决定了transpose后的数组，其shape在第一个维度即shape[0]上是原来的shape[1]，第二维shape[1]是原来的shape[2]，第三维shape[2]是原来的shape[0]。所以原shape为(3,2,4)。新的shape为(2,4,3)。</description>
    </item>
    
    <item>
      <title>np.unique()解读</title>
      <link>https://codefmeister.github.io/p/np.unique%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Sat, 21 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/np.unique%E8%A7%A3%E8%AF%BB/</guid>
      <description>np.unique()官方文档分析以及举例 1.1 官方文档及解读 numpy.unique 语法：numpy.unique(ar, return_index=False, return_inverse=False, return_counts=False, axis=None)
作用：找到array中不重复（独一无二）的元素
返回值：默认返回不重复元素的sorted排好序的从小到大的数组。可选的返回值有：
 输入数组提供不重复值(unique)元素的索引下标(如果有多个返回第一个) 利用unique数组重构原有的input数组所需要的的索引下标 该unique元素在input数组中的出现次数，相当于count  Parameter
  ar：array like
输入的数组，除非特别指定axis，数组将被展平为1-D形式进行处理。
  return_index: bool, optional
如果为True，返回输入数组提供不重复值(unique)元素的索引下标(如果有多个返回第一个)
  return_inverse: bool, optional
如果为True，返回利用unique数组重构原有的input数组所需要的的索引下标
  axis: int or None, optional
进行操作的维度。如果为None，数组将被展平作为一维数组处理，如果指定了axis，则以该维索引构成的子数组作为元素，将整个数组视为一维数组进行处理。如果axis被使用，则不支持Object Array以及structured arrays。
  Returns
  unique: ndarray
排好序(从小到大)的unique值
  unique_indices: ndarray, optional
unique数组中对应位置的value值第一次在input数组中出现的下标值。当return_index = True时返回。
  unique_inverse: ndarray, optional
利用unique数组重构源输入input数组所需要的索引下标。当return_inverse = True的时候返回。
  unique_count: ndarray, optional 每个unique values在原数组中出现的次数，当return_counts=True时返回。</description>
    </item>
    
    <item>
      <title>numpy求解范数--numpy.linalg.norm</title>
      <link>https://codefmeister.github.io/p/numpy%E6%B1%82%E8%A7%A3%E8%8C%83%E6%95%B0-numpy.linalg.norm/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/numpy%E6%B1%82%E8%A7%A3%E8%8C%83%E6%95%B0-numpy.linalg.norm/</guid>
      <description>numpy.linalg.norm 语法 numpy.linalg.norm(x,ord=None,axis=None,keepdims=False)
Parameters  x: array_like   Input array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.
 X是输入的array, array的情况必须是以下三种情况之一:
 axis未指定，ord指定。此时x必须是一维或二维数组 axis指定，x任意 axis未指定，ord未指定，此时x任意，返回值为x被展平后的一维向量x.ravel的二范数。   ord：{non-zero int, inf, -inf, &amp;lsquo;fro&amp;rsquo;, &amp;lsquo;nuc&amp;rsquo;}, optional   Order of the norm (see table under Notes). inf means numpy&amp;rsquo;s inf object. The default is None.</description>
    </item>
    
    <item>
      <title>parse_args传参</title>
      <link>https://codefmeister.github.io/p/parse_args%E4%BC%A0%E5%8F%82/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/parse_args%E4%BC%A0%E5%8F%82/</guid>
      <description>python中parse_args以及namespace 声明：本笔记记录的是使用parse_args在函数内部进行传参，并非在命令行进行输入。所有操作均需先进行import argparse。
通过以下操作，可以在传参时直接传入args这个namespace，而不是具体的某个参数。
创建argparse对象，设置参数以及默认值 使用argparse.ArgumentParser()创建对象，使用argparser.add_argument()操作设置参数以及默认值。
Example: parser = argparse.ArgumentParser(&#39;Exampe&#39;) parser.add_argument(&#39;--NDArray&#39;,type=np.ndarray,default= NDArray) argument中的参数类型丰富多样，可以是任何数据类型。使用--name来设置参数名，使用type = 设置类型，使用default设置初始化后的默认值。
parser转换为name_space 使用命令parser.parse_args()，即可将一个ArgumentParser转换为name_space.转换为namespace后，可以对先前设置的argument通过.name的方式类似属性一样进行访问，同样可以进行赋值，存取等操作。
args = parser.parse_args() print(args.NDArray) a = np.array([[1,2],[3,4]]) args.NDArray = a namespace 的一些操作 在初始化ArgumentParser时，我们可能忘记添加某些argument，这就导致在转换为namespace后缺少某些attribute.
我们可以对args使用.__setattr(name,value)设置新的属性值。开辟之后就可以使用.attr的方式进行赋值存取。 同样，我们可以使用.__contains__(attribute_name)判断args这个namespace是否含有该属性。
args.__setattr__(&#39;cloudArray&#39;,[]) args.cloudArray.append(1024) if args.__contains__(&#39;cloudArray&#39;): print(&#39;namespace args contains attr&#39;) </description>
    </item>
    
    <item>
      <title>torch.optim解读</title>
      <link>https://codefmeister.github.io/p/torch.optim%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.optim%E8%A7%A3%E8%AF%BB/</guid>
      <description>本文参考 lr_scheduler介绍 以及 PyTorch optim文档
 1 概述 1.1 PyTorch文档：torch.optim解读  下图是optim的文档
  TORCH.OPTIM torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.
 torch.optim简介
torch.optim是PyTorch实现的一个包，里面有各种各样的优化算法，大部分常用的优化算法都已经被支持，接口也十分通用，所以可以用来集成实现更加复杂的系统。
 How to use an optimizer To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</description>
    </item>
    
    <item>
      <title>MatrixCookBook Chapter1:矩阵的基础知识</title>
      <link>https://codefmeister.github.io/p/matrixcookbook-chapter1%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Sun, 15 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/matrixcookbook-chapter1%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid>
      <description>矩阵的基础知识（转置，逆，迹，行列式）  References: MatrixCookBook(Version 2012) Chapter1
 Chapter1: Basics 1 Basics 注：${A^H}$是A的Transposed and complex conjugated matrix (Hermitian)，即转置复共轭矩阵。
1.1 矩阵的迹(Trace) 式子(11)表明矩阵的迹是主对角线元素的和。
式子(12)表明矩阵的迹是矩阵的特征值的和。
式子(13)表明矩阵的迹等于其转置矩阵的迹。
式子(14)表明AB的迹等于BA的迹。
式子(15)表明A+B的迹等于A的迹加B的迹。 式子(16)表明ABC的迹等于BCA的迹等于CAB的迹。
式子(17)表明一个nx1的向量a，a的转置乘以a所得的常数等于a乘以a的转置所得矩阵的迹。
1.2 行列式(Determinant) 前提：此处的A是nxn矩阵。
式子(18)表明矩阵的行列式等于特征值的连乘积。
式子(19)表明cA的行列式等于A的行列式的${c^n}$倍。
式子(20)表明矩阵的行列式等于其转置矩阵的行列式。
式子(21)表明矩阵AB的行列式等于矩阵A的行列式乘以矩阵B的行列式。
式子(22)表明矩阵${A^{-1}}$的行列式等于矩阵A的倒数。
式子(23)表明矩阵${A^n}$的行列式等于矩阵A的行列式的n次幂。
式子(24)表明如果u和v是nx1向量，那么${I+uv^T}$的行列式等于${1+u^Tv}$的值。
式子(25)表明如果A是2x2矩阵，I+A的行列式等于${1+det(A)+Tr(A)}$,即1+A的行列式+A的迹。
式子(26)表明如果A是3x3矩阵，I+A的行列式等于${1+det(A)+Tr(A)+\frac{1}{2}Tr(A)^2-\frac{1}{2}Tr(A^2)}$。
式子(27)不表。
式子(28)表示对于微小扰动$\varepsilon$，可以将$\varepsilon A$近似作为2x2形式处理：
1.3 特例：2x2矩阵 2x2矩阵有着以上的性质与结论。</description>
    </item>
    
    <item>
      <title>model.eval()作用分析</title>
      <link>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 14 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
      <description>model.eval() model.eval() 作用等同于 self.train(False)
简而言之，就是评估模式。而非训练模式。
在评估模式下，batchNorm层，dropout层等用于优化训练而添加的网络层会被关闭，从而使得评估时不会发生偏移。
总结 在对模型进行评估时，应该配合使用with torch.no_grad() 与 model.eval()：
 loop: model.train() # 切换至训练模式 train…… model.eval() with torch.no_grad(): Evaluation end loop </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Iverson Bracket Iverson Bracket 又称 艾弗森括号。常用方括号来表示。满足括号内的条件则值为1， 不满足条件则值为0.
Example $$ \hat{\mathbf{R}}, \hat{\mathbf{t}}=\arg \max {\mathbf{R}^{\prime}, \mathbf{t}^{\prime}} \sum{i}^{|C|} \llbracket\left|\mathbf{R}^{\prime} x_{i}+\mathbf{t}^{\prime}-y_{i}\right|&amp;lt;\tau \rrbracket $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy.arange() 语法 x = numpy.arange(start,end,step,dtype=None)
Parameters说明  start: Optional，起始值，默认值为0。 end: 结束值(不含)。 step: Optional，步长，默认值为1。 dtype：Optional，默认为None，从其他输入值中推测。  功能 [start,end)的左闭右开区间内，每隔一个step取一次值。return值是ndarray。 对于浮点数来说，length = ceil((end - start)/step)，由于浮点数的上溢，此条规则可能会导致在浮点数情况下，最后一个element比end长。
Note 如果使用非整数步长（譬如0.1），结果往往不一致（原因见上），所以在这种情况下推荐使用numpy.linspace。
Example &amp;gt;&amp;gt;&amp;gt; np.arange(3) array([0, 1, 2]) &amp;gt;&amp;gt;&amp;gt;np.arange(3.0) array([ 0., 1., 2.]) &amp;gt;&amp;gt;&amp;gt;np.arange(3,7) array([3, 4, 5, 6]) &amp;gt;&amp;gt;&amp;gt;np.arange(3,7,2) array([3, 5]) </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy.repeat() 作用 可以用于重复数组中的元素
语法 numpy.repeat(a, repeats, axis=None)
参数解读 Parameters   a : array_like
Input array. repeats : int or array of ints
The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis. axis : int, optional
The axis along which to repeat values. By default, use the flattened input array, and return a flat output array.    a: array_like</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy.tile 语法 numpy.tile(A,reps)
作用  Construct an array by repeating A the number of times given by reps.
If reps has length d, the result will have dimension of max(d, A.ndim).
If A.ndim &amp;lt; d, A is promoted to be d-dimensional by prepending new axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication, or shape (1, 1, 3) for 3-D replication. If this is not the desired behavior, promote A to d-dimensions manually before calling this function.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy.concatenate 语法 numpy.concatenate((a1,a2,...), axis=0, out=None, dtype=None, casting=&amp;quot;same_kind&amp;quot;)
作用 将一个数组序列在指定的维度上进行连接join
Parameter  a1,a2,&amp;hellip; : sequence of array_like   The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default)
 数组序列的shape在除了axis指定维度以外的所有维度上都应该相同。axis默认为第一个维度，即axis=0。
 axis : int, optional   The axis along which the arrays will be joined. If axis is None, arraysare flattened before use. Default is 0.
 axis指定了数组进行join操作的维度。默认为0，即第一维。如果axis=None，那么数组将会先展平，再进行join。
 out : ndarray, optional   If provided, the destination to place the result.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy求矩阵的特征值与特征向量(np.linalg.eig) 语法 np.linalg.eig(a)
功能  Compute the eigenvalues and right eigenvectors of a square array.
 求方阵(n x n)的特征值与右特征向量
Parameters  a : (&amp;hellip;, M, M) array   Matrices for which the eigenvalues and right eigenvectors will be computed
 a是一个矩阵Matrix的数组。每个矩阵M都会被计算其特征值与特征向量。
Returns  w : (&amp;hellip;, M) array   The eigenvalues, each repeated according to its multiplicity. The eigenvalues are not necessarily ordered. The resulting array will be of complex type, unless the imaginary part is zero in which case it will be cast to a real type.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>numpy中求矩阵的逆与伪逆 numpy中求矩阵的逆：numpy.linalg.inv()
numpy中求矩阵的伪逆: numpy.linalg.pinv()
numpy中求矩阵的逆（numpy.linalg.inv) 使用命令numpy.linalg.inv(Matrix)
功能  Compute the (multiplicative) inverse of a matrix.
Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0]).
 计算一个方阵的逆，使之满足$AA^{-1}=A^{-1}A=I$
Parameters  a : (&amp;hellip;, M, M) array_like
Matrix to be inverted.  a是输入的要计算逆的矩阵数组。
Returns  ainv : (&amp;hellip;, M, M)
ndarray or matrix (Multiplicative) inverse of the matrix a.  返回的是对应的逆矩阵的数组。
Raises  LinAlgError
If a is not square or inversion fails.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>#! https://zhuanlan.zhihu.com/p/435196716
PRML 第一章习题解 1.1 多项式最小二乘法的封闭解 已知： $E=\frac{1}{2} \sum_{n=1}^{N}\left{\left(\sum_{j=0}^{M} \omega_{j} x_{n}^{j}\right)-t_{n}\right}^{2}$ ，由于Sigma函数的特殊性，我们考虑对每一项进行求解：
对于$E_{n}=\frac{1}{2}\left(\sum_{j=0}^{N} \omega_{j} x_{n}^{j}-t_{n}\right)^{2}$，令$q_{n}=\sum_{j=0}^{n} \omega_{j} x_{n}^{j}-t_{n}$,则有：$E_{n}=\frac{1}{2} q_{n}^{2}$
则： $$ \begin{aligned} \frac{\partial E_{n}}{\partial \omega_{i}}=\frac{\partial E_{n}}{\partial q_{n}} \cdot \frac{\partial q_{n}}{\partial \omega_{i}} &amp;amp;=q_{n} \cdot x_{n}^{i} \
&amp;amp;=\left(\sum_{j=0}^{M} \omega_{j} \cdot x_{n}^{j}-t_{n}\right) \cdot x_{n}^{i} \
&amp;amp;=\sum_{j=0}^{M} \omega_{j} \cdot x_{n}^{i+j}-t_{n} \cdot x_{n}^{i} \end{aligned} $$
那么对于$ E=\sum_{n=1}^{N} E_{n} $，有：
$$ \begin{aligned} \frac{\partial E}{\partial \omega_{i}} &amp;amp;=\sum_{n=1}^{N} \frac{\partial E_{n}}{\partial \omega_{i}} \
&amp;amp;=\sum_{n=1}^{N}\left(\sum_{j=0}^{M} \omega_{j} \cdot x_{n}^{i+j}-t_{n} \cdot x_{n}^{i}\right) \</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>标量、向量、矩阵求导 向量对向量求导 对向量进行求导时，除了遵循雅克比矩阵外，常常容易会被矩阵的排列所混淆。
在做PRML1.9时，遇到了行向量对列向量求导的问题，着实让我迷惑了许久。
查阅资料后，得出如下结论，记录在此：
  排列方式均可，但需保持一致。无需太过纠结。
  可以直接按照如下公式进行求导，因为如果你选择其他排列方式，结果就是差一个转置而已。
  $$ \frac{\partial A \vec{x}}{\partial \vec{x}}=A^{T} $$ $$ \frac{\partial A \vec{x}}{\partial \vec{x}^{T}}=A $$ $$ \frac{\partial\left(\vec{x}^{T} A\right)}{\partial \vec{x}}=A $$
可以看出来，多转置则结果也会多一个转置。
不同文献给出了不同的结果——超越矩阵，存在争议。
标量对向量求偏导 记$ y=\vec{x}^{T} \cdot A \cdot \vec{x} $， 则有：
$$ \frac{\partial y}{\partial \vec{x}}=\frac{\partial\left(\vec{x}^{T} \cdot A \cdot \vec{x}\right)}{\partial \vec{x}}=\left(A^{T}+A\right) \cdot \vec{x} $$
若$A$为对称阵，则： $$ \frac{\partial\left(\vec{x}^{T} A \vec{x}\right)}{\partial \vec{x}}=2 A \vec{x} $$
标量$det(A)$对方阵$A$求导： $$ \frac{\partial|A|}{\partial A}=\left(A^{*}\right)^{T}=|A| \cdot\left(A^{-1}\right)^{T} $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>python如何使得list中的元素是ndarray 在一个代码实现时，会想要类似MATLAB一样，拥有一个struct类型的数组，数组中的每个cell都是一个ndarray。因为在某些维度上shape不同，所以不能整合为一个大的ndarray。
要实现上述需求，可以进行如下操作:
targetList = [] targetList.append(ndarray) 这样即可获得由ndarray构成的list，可以进一步对list进行concatenate等操作再次整合。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Python中李群SO(3)与李代数so(3)之间指数映射与实现源码 调用scipy.linalg.expm() 对于李群SE(3)、SO(3)，和与其对应的李代数se(3),so(3)。指数映射是十分重要的。
在Python中我们可以调用：scipy.linalg.expm()来将李代数$\xi $对应的反对称矩阵${\hat \xi }$映射到其对应的旋转矩阵$R$。
源码示例 import scipy.linalg.expm as expm def SkewFun(a): &amp;quot;&amp;quot;&amp;quot; got the corresponded antiSymmetric Matrix of the Lie algebra :param a: Lie algebra :return: antiSymmetric Matrix &amp;quot;&amp;quot;&amp;quot; if len(a) == 3: A = np.array([[0, -a[2], a[1]], [a[2], 0, -a[0]], [-a[1], a[0], 0] ]) return A if len(a) == 2: A = np.array([a[1], -a[0]]) return A exit(-1) def so3ToSO3(xi): return expm(SkewFun(xi) </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Python判断不可变对象（字符串，整数，浮点数，数组）相等的办法以及其底层实现原理 Python中，判断不可变对象是否相等与Java十分不同。
结论 Python中判断两个字符串相等：既可以使用&amp;quot;==&amp;quot;，又可以使用&amp;quot;is&amp;quot;。
判断整数，浮点数以及Tuple时，最好使用&amp;quot;==&amp;quot;
a = &#39;abc&#39; b = &#39;abc&#39; print(a is b) print(a == b) 这个问题可以进一步引申至Python的底层实现原理上。
原理 整数 Python在底层实现中，一切都是对象。包括整数等也是对象。这些基本的不可变对象在python里会被频繁的引用,创建,如果不能重用的话，极易导致效率瓶颈,所以python引入了整数对象池的机制。
Python中，对于[-5,256]的整数，创建了整数对象池，创建范围内的小整数会自动引用对象池中的整数对象。
a = -5 b = -5 print(a is b) print(a == b) a1 = -6 b1 = -6 print(a1 is b1) print(a1 == b1) 从运行结果图上可以看出，-5由于是小整数，引用整数池中的对象，所以是同一个对象,a is b为True。而-6由于不在范围内，每次都会新建一个对象，所以是两个对象，a is b为False。
同理可以测试256,257。前者为True，后者为False。
浮点数 由于浮点数有无穷多个，所以浮点数并没有常量池。在创建浮点数对象时会直接新建一个对象。
a = 0.0 b = 0.0 print(a is b) print(a == b) 从结果中可见，两者值相等，但并不是指向同一内存地址。
字符串 Python中存在着intern机制。由于字符串是不可变对象，它对字符串维护着一个字典，每次新建一个字符串变量时，会先查询字典中是否已经有该字符串值。如果有，直接引用。如果没有再新建。这个机制决定了字符串值相等，则一定指向相同的对象。
Tuple 对于元组，虽然其是不可变对象，但在底层实现无intern机制，就是单纯的一个可以迭代的数组，存放着元素。每次创建都会开辟地址。所以新建两个值相同的变量会创建两个对象。使用&amp;quot;==&amp;quot;判断。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>python中判断列表为空 判断列表为空是一个非常基础的问题。但是也有很多写法。
方法1：len() list = [] if len(list) == 0: print(&#39;list is empty&#39;) 方法2：直接使用if判断 list = [] if not list: print(&#39;list is empty&#39;) 直接使用list作为判断标准，则空列表相当于False
方法3：使用==进行判断 EmptyList = [] list = [] if list==EmptyList: print(&#39;list is empty&#39;) 注意: Python中与Java不同。Java中==用于判断两个变量是否指向同一个对象，即地址是否相同。但是Python中不是，Python中，==用于判断两个变量的值相等。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>python中怎么表示自然底数e和浮点数精度epsilon 自然底数e可以直接使用math.e表示。
浮点数精度epsilon可以使用np.spacing(1)来表征epsilon，等效于MATLAB中的eps.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>PyTorch中默认维度 PyTorch中默认维度: B C H W。
即Batch_size，Channel, Height, Width</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Mechanics of Seq2Seq Models With Attention  Reference: Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)
 前言 Sequence-to-sequence模型在深度学习领域取得了很多成就。
这文章真的牛逼。
有视频不翻译了。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>AdaptiveMaxPool AdaptiveMaxPool是PyTorch中提供的自适应池化层。
其主要特殊的地方在于： 无论输入Input的size是多少，输出的size总为指定的size。
AdaptiveMaxPool1d() m = nn.AdaptiveMaxPool1d(3) input = torch.randn(4,3,7) output = m(input) # output的size为(4,3,3) AdaptiveMaxPool2d() m = nn.AdaptiveMaxPool2d((3,6)) input = torch.randn(2,64,8,9) output = m(input) # output的size为(2,64,3,6) AdaptiveMaxPool3d() 同理</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>torch.topk 语法  torch.topk(input, k, dim=None, largest=True, sorted=True, *, out = None)
 作用 返回输入tensorinput中，在给定的维度dim上k个最大的元素。
如果dim没有给定，那么选择输入input的最后一维。
如果largest = False，那么返回k个最小的元素。
返回一个namedtuple类型的元组(values, indices)，其中indices是指元素在原数组中的索引。
sorted = True， 则返回的k个元素是有序的。
Parameters   input (Tensor) &amp;ndash; the input tensor
输入的张量
  k (int) &amp;ndash; the k in &amp;ldquo;top-k&amp;rdquo;
返回的k的值
  dim(int, optional) &amp;ndash; the dimension to sort along
指定的排序的维度, dim若为-1，文档未说明，但是根据实操效果，应该是对最后一维进行search。
如shape为Batch_size x p x q，返回结果为Batch_size x p x k。
  largest(bool, optional) &amp;ndash; controls whether to return largest or smallest elements</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>如何设置随机种子 设置随机种子 应该为torch, numpy,以及Python设置随机种子，并提高torch卷积精度。
def set_seed(seed): random.seed(seed) np.random.seed(seed) os.environ[&amp;#39;PYTHONHASHSEED&amp;#39;] = str(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False 设置随机种子之后，仍然发现训练结果不同？考虑是数据采样的问题 背景： 数据集需要对数据进行随机采样。从全体数据中sample得到部分。
踩坑： 在DataLoader中，num_worker会影响在已经设置好的随机种子下，对数据的采样结果，导致每次拿到的数据均不同。
解决方法： 删除num_worker.
原始： 相同seed下，每次提取的数据都不一致。
trainLoader = DataLoader(dataset, batch_size=10, shuffle=True, num_worker=1) 修改：相同seed下，每次随机得到的数据一致。
trainLoader = DataLoader(dataset, batch_size=10, shuffle=True) 实验对比发现，即使num_worker=1，仍然会导致无法复现。原因分析，num_worker是用于数据提取的多线程数，多线程情况下线程同步问题会导致随机种子在多次实验中波动。删除num_worker即可。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>#! https://zhuanlan.zhihu.com/p/430893923
A Baseline for detecting misclassified and out-of-distribution examples in nerual networks 论文阅读笔记 本文发表在ICLR2017，作者Dan Hendrycks. 是不确定性最朴素的baseline做法。
Motivation 为了检测样本是否被误分类以及样本是否属于OOD样本，作者提出了一个简单的baseline做法，即使用softmax输出的最大预测概率作为依据来辨别。实验发现，正确分类的样本输出会拥有比错误分类样本或者OOD样本更大的softmax输出。
two question:
 error and success precdiction in- and out-of-distribution detection  Metrics 作者使用AUROC 和 AUPR作为指标。
AUROC以真阳性率与假阳性率为横纵坐标画图。反应模型的查全率。perfect model 其AUROC应该为100%，一个完全随机模型其AUROC为50%。 当正负例的base rate有很大不同时，AUROC不那么完美。
而AUPR以精度和召回率为横纵坐标画图，反应模型的查准率。 一个perfect model其AUPR应该为100%。
Method 对于任意一个样本，计算其softmax的最大输出概率。以此为依据计算AUROC，AUPR，可以发现，正常样本下的AUROC、AUPR显著高于异常样本下的AUROC、AUPR。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>#! https://zhuanlan.zhihu.com/p/427073494
Evidential Deep Learning to Quantify Classification Uncertainty 论文阅读笔记 Motivation 在当下的机器学习以及深度学习领域，重要的不是Accuracy，而是更重要的鲁棒性、安全性等方面。
问题：当我们在MNIST上训练一个手写体识别网络的时候，网络可以达到很好的分类识别性能，但是当我们给网络输入一个猫的图片的时候，尽管这个样本网络并不熟悉，网络还是会输出一个标签，而不是简单的告诉我们，他不认识或者说他不知道。这类样本称为OOD(out-of-distribution )的样本，由此，我们引入Uncertainty的概念。
Subjective Logic 主观逻辑  在此我仅就个人理解对主观逻辑在多元分类问题下进行形式化描述，对此有兴趣的读者可以进一步阅读书籍《Subjective Logic》
 不同于普通的概率学，我们引入主观逻辑Subjective Logic对模型的不确定性进行度量。
对于多元分类问题，其有$K$个标签，对于样本$i$，我们预测输出概率$\mathbf{p}=[p_1, p_2, &amp;hellip;,p_k]$，则我们对每个标签所属的概率$p_k$赋予一个belief mass，记为$b_k$，则： $$ u+\sum_{k=1}^{K} b_{k}=1 $$
其中$u$为uncertainty, 且对于$k = 1, &amp;hellip; ,K$，$u \geq 0$, $b_k \geq 0$。
如何获得$b_k$此刻便成为了主要问题。为此，我们引入证据evidence，记为$e_k$. 则： $$ b_{k}=\frac{e_{k}}{S} \quad \text { and } \quad u=\frac{K}{S} $$
其中$S=\sum_{i=1}^{K}\left(e_{i}+1\right)$. 需要注意的是，事实上，$S=\sum_{i=1}^{K}\left(e_{i}+a_i\right)$. $a_i$为先验。在此，其实对先验进行了简化处理。在此处，直接取1，其实代表了没有预知信息，所以取均匀分布。
对一个样本的belief assignment可以与一个Dirichlet 分布对应起来。$\alpha_{k}=e_{k}+1$. 狄利克雷分布度量了各个概率$p_k$的概率分布，即二阶概率。狄利克雷分布由$\boldsymbol{\alpha}=\left[\alpha_{1}, \cdots, \alpha_{K}\right]$进行参数化。 $$ D(\mathbf{p} \mid \boldsymbol{\alpha})= \begin{cases}\frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^{K} p_{i}^{\alpha_{i}-1} &amp;amp; \text { for } \mathbf{p} \in \mathcal{S}_{K} \ 0 &amp;amp; \text { otherwise }\end{cases} $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Gaussian Processes 高斯过程  Lecture Link: https://www.youtube.com/watch?v=MfHKW5z-OOA&amp;amp;list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&amp;amp;index=9
 Guassian 基础知识 首先引入简单的高斯分布。
如下图，左右都有很多二元数据$x=\left[\begin{array}{l}x_{1} \ x_{2}\end{array}\right] \in \mathbb{R}^{2}$。我们想要用一个分布去拟合两个变量的分布。所以我们使用一个二元高斯分布。不论左右，其均值都为$\mu=\left[\begin{array}{l}\mu_{1} \ \mu_{2}\end{array}\right] =\left[\begin{array}{l}0 \ 0\end{array}\right]$，但是协方差矩阵却不一样。对于左边的分布，如果我们知道$x$增大，我们并不能获得关于$y$的任何多余信息，也就是说两个变量没有关联，所以对应的协方差为$0$. 对于右边的分布，随着$x$的增大，$y$也在增大，所以右图看起来像一个椭圆。所以两者的协方差为不妨设为$0.5$.
协方差covariance某种程度上等价于互相关系数correlation。协方差矩阵中对应位置数值大小反应了两个变量之间的相关关系，协方差为$0$代表没有关联，协方差为正代表正相关。
这里我们列出一些涉及的公式：
协方差： $$ \begin{aligned} \operatorname{cov}(X, Y) &amp;amp;=\mathrm{E}[(X-\mathrm{E}[X])(Y-\mathrm{E}[Y])] \
&amp;amp;=\mathrm{E}[X Y-X \mathrm{E}[Y]-\mathrm{E}[X] Y+\mathrm{E}[X] \mathrm{E}[Y]] \
&amp;amp;=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y]-\mathrm{E}[X] \mathrm{E}[Y]+\mathrm{E}[X] \mathrm{E}[Y] \
&amp;amp;=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y] \end{aligned} $$
相关系数： $$ \rho_{X, Y}=\operatorname{corr}(X, Y)=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{\mathrm{E}\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}} =\frac{\mathrm{E}(X Y)-\mathrm{E}(X) \mathrm{E}(Y)}{\sqrt{\mathrm{E}\left(X^{2}\right)-\mathrm{E}(X)^{2}} \cdot \sqrt{\mathrm{E}\left(Y^{2}\right)-\mathrm{E}(Y)^{2}}} $$
现在我们换一种视角来看高斯分布，我们在三维空间里看这个二元高斯分布$\left[\begin{array}{l}x_{1} \ x_{2}\end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{l}\mu_{1} \ \mu_{2}\end{array}\right],\left[\begin{array}{ll}\Sigma_{11}, &amp;amp; \Sigma_{12} \ \Sigma_{21}, &amp;amp; \Sigma_{22}\end{array}\right]\right)$，z轴代表其两个变量的联合概率密度，那么整个高斯分布在三维空间中的形状是钟形。从随机变量$X_1$的某个特定值处切一刀，可以得到$X = x_1$时，变量$X_2$的条件概率分布。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>#! https://zhuanlan.zhihu.com/p/428553023
On Calibration of Modern Neural Networks 论文阅读笔记  本篇笔记主要记录Calibration的相关知识，以及作者的一些insight。如何使网络calibrating的一些方法并没有涉及。
 摘要 作者发现，在深度神经网络盛行的当下，虽然网络的精度(Accuracy)大大提升了，但是网络的标定性（Calibration）很差。
What is Calibration? $$ \mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)=p, \quad \forall p \in[0,1] $$ 上式中，$\hat Y$是预测输出，$Y$是真实标签。$\hat P$为网络输出该标签的信心confidence。$P(Y|P)$表示网络对输出Y有信心P。
即：网络输出的confidence应当等于真实的概率。以二分类问题举例，对于一个分类网络，如果以最终输出预测的值作为信心confidence，它应当等于分类正确的概率。换言之：让模型的softmax输出能真实的反映决策的置信度。
Reliability Diagrams 可靠性直方图是用来描绘模型可靠性的一类直方图。 完美标定的(perfect calibrated) 模型应当是一条对角线。
在有限的样本个数下，我们根据模型输出的prediction将样本分为$M$组，分别计算其Accuracy与confidence。如果我们用$B_m$指代第$m$个样本集合。那么有： $$ \operatorname{acc}\left(B_{m}\right)=\frac{1}{\left|B_{m}\right|} \sum_{i \in B_{m}} \mathbf{1}\left(\hat{y}_{i}=y_{i}\right) $$ 其中$\hat y$为预测输出，$y$ 为真实样本标签。 $$ \operatorname{conf}\left(B_{m}\right)=\frac{1}{\left|B_{m}\right|} \sum_{i \in B_{m}} \hat{p}_{i} $$ 其中$\hat p_i$为第$i$个样本模型预测的信心confidence。
那么可以以此画出Reliability Diagrams。如下图：
中间为gap，即confidence与精度之间的差距。完美的标定模型其可靠性图应当是一条对角线。confidence能够精确反应预测精度。
Expected Calibration Error(ECE) 期望标定误差 $$ \mathrm{ECE}=\sum_{m=1}^{M} \frac{\left|B_{m}\right|}{n}\left|\operatorname{acc}\left(B_{m}\right)-\operatorname{conf}\left(B_{m}\right)\right| $$
可用于度量标定。在图中表现为gap的均值。
Maximum Calibration Error(MCE) 最大标定误差 $$ \mathrm{MCE}=\max {m \in{1, \ldots, M}}\left|\operatorname{acc}\left(B{m}\right)-\operatorname{conf}\left(B_{m}\right)\right| $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>泛化误差上界定理的证明 马尔科夫不等式 内容 如果$X$是只取非负值的随机变量，则对于任意$a &amp;gt; 0$，有： $$ P{X \geq a} \leq \frac{E(X)}{a} $$
简证 $$ \begin{aligned} E(X)=\int_{0}^{\infty} x f(x) d x &amp;amp;=\int_{0}^{a} x f(x) d x+\int_{a}^{\infty} x f(x) d x \
&amp;amp; \geq \int_{a}^{\infty} x f(x) d x \
&amp;amp; \geq \int_{a}^{\infty} a f(x) d x \
&amp;amp;=a \int_{a}^{\infty} f(x) d x \ &amp;amp; =a P{X \geq a} \end{aligned} $$
引理 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>#! https://zhuanlan.zhihu.com/p/425388698
Dirichlet Distribution 狄利克雷分布 Definition 简介 狄利克雷分布，又称多元Beta分布，是一类在实数域以正单纯形(standard simplex)为支撑集(support)的高维连续分布，是Beta分布在高维情形的推广。
在Bayesian inference里，Dirichlet分布是多项分布的共轭先验。
背景知识：Beta分布 Beta分布是定义在$[0, 1]$区间内的连续概率分布族，它由两个参数$\alpha, \beta$所指定。这两个参数作为随机变量的指数出现，并且控制分布的形状。Beta分布的多元推广为狄利克雷分布。
Beta分布是伯努利分布，二项分布的共轭先验， 如下讨论的分布也称第一类beta分布。
Beta 分布的定义 概率密度函数 $$ \begin{aligned} f(x ; \alpha, \beta) &amp;amp;=\text { constant } \cdot x^{\alpha-1}(1-x)^{\beta-1} \
&amp;amp;=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u} \
&amp;amp;=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1} \
&amp;amp;=\frac{1}{\mathrm{~B}(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} \end{aligned} $$
其中$\Gamma(z)$ 是Gamma函数。Beta函数为：$\mathrm{B}(x, y)=\int_{0}^{1} t^{x-1}(1-t)^{y-1} d t$
Beta分布与二项分布 Beta分布是二项分布的共轭先验。但除此之外，我们观察Beta分布的概率密度函数，会发现与二项分布的概率质量函数十分相似： $$ P{X=k}=\left(\begin{array}{c} n \
k \end{array}\right) p^{k}(1-p)^{n-k} $$
在二项分布中，概率$p$作为参数，随机变量为$\mathbb{X}$。 但在Beta分布中，概率作为随机变量，而不是参数。
也就是说，Beta分布是概率的概率分布。其前面的常数项$\frac{1}{\mathbf{B}(\alpha, \beta)}$,其作用便是为了让整个概率密度函数的积分等于1，满足概率密度函数的积分约束。
关于$\alpha, \beta$ $\alpha, \beta$是用来控制Beta分布形状的参数，其取值往往与某一事件发生而观测到的evidence有关。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>为什么要有Tensor.contiguous() Tensor.contiguous()作用  Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.
 作用在官方文档里，描述的看似清晰但又模棱两可。例如x是一个Tensor，x.contiguous()的作用就是返回一个在内存中连续的Tensor，其data与Tensorx一致。如果源x本来就在内存中连续的话，那就返回其本身。
为什么要有Tensor.contiguous()?  Reference: StackOverflow&amp;ndash;Why do we need contiguous?
 在PyTorch中，有些对Tensor的操作并不实际改变tensor的内容，而只是改变如何根据索引检索到tensor的byte location的方式。
这些操作有：
 narrow(), view(), expand(), transpose()，permute()
 例如： 当我们调用transpose()时，PyTorch并不会生成一个具有新的layout（大概可以翻译为布局）的新tensor。该操作仅仅改变了tensor中的meta information（元信息），所以offset和stride可以正确作用于新的shape。但是转置后的tensor和源tensor在事实上是共享同一块内存空间的。
&amp;gt;&amp;gt;&amp;gt; x = torch.randn(3,2) &amp;gt;&amp;gt;&amp;gt; print(x) tensor([[ 0.9181, 1.4266], [-0.1432, -0.7514], [ 0.9809, -0.5079]]) &amp;gt;&amp;gt;&amp;gt; print(x[0,0]) tensor(0.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>克罗内克内积 Kronecker product $\otimes$ 1.1 概述 克罗内克内积是一种特殊的张量积。任何两个形状的矩阵都可以进行克罗内克内积操作。
1.2 定义 Definition $A \otimes B$的定义：A是mxn矩阵，B是pxq矩阵。$A \otimes B$是mp x nq的分块矩阵。 例子： 1.3 性质 1.3.1 双线性结合律 1.3.2 不满足交换律 1.3.3 混合乘积性 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>协方差矩阵详解以及numpy计算协方差矩阵(np.cov) 协方差矩阵详解 均值，标准差与方差 由简单的统计学基础知识，我们有如下公式：
\bar X{\rm{ = }}\frac{{\sum\limits_{i = 1}^n {{X_i}} }}{{\rm{n}}} S = \sqrt {\frac{{\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2}} }}{{n - 1}}} {S^2} = \frac{{\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2}} }}{{n - 1}} 其中$\bar X$是样本均值，反映了n个样本观测值的整体大小情况。
$S$是样本标准差，反应的是样本的离散程度。标准差越大，数据越分散。
$S^2$是样本方差，是$S$的平方。
均值虽然可以在一定程度上反应数据的整体大小，但是仍然不能反应数据的内部离散程度。而标准差和方差弥补了这一点。
但是标准差和方差都是针对一维数组的，即1 x d数组。该数组的行代表的是一个随机变量（可理解为属性），如工资等。每一列代表一个观测值。如果一个事物具有多种属性，即有多个随机变量，那么我们会得到一个var_num x d数组。该数组的每一行都是一个随机变量（属性），每一列代表着一个在这些属性维度上的观测值样本。如果我们想要分析该事物，那么仅仅将其剥离为单独的1 x d去求其标准差是不够的，我们还需要关注这些随机变量（属性）variable内部之间的联系。如工资和年龄的联系，工资和技术水平的联系等。
所以便自然而然的引入了协方差。
协方差 两个随机变量的协方差反映了这两个随机变量一致的分散程度有多大。
通俗的讲，协方差反映了两个随机变量的正负相关关系。
由方差的公式，我们可以类比得出协方差的公式：
{\mathop{\rm var}} (X) = {S^2} = \frac{{\sum\limits_{i = 1}^n {({X_i} - \bar X)({X_i} - \bar X)} }}{{n - 1}} {\mathop{\rm cov}} (X,Y) = \frac{{\sum\limits_{i = 1}^n {({X_i} - \bar X)({Y_i} - \bar Y)} }}{{n - 1}} = E((X - E(X))(Y - E(Y))) 相关系数$\rho$与协方差直接有如下关系：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>循环神经网络 前言 我们知道，在n元语法中，时间步$t$的词$w_t$基于前面所有词的条件概率只考虑了最近时间步的$n-1$个词。如果要考虑比$t-(n-1)$更早时间步的词对$w_t$的可能影响，我们需要增大n。
其下介绍的循环神经网络，它并未刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。
不含隐藏状态的神经网络 考虑一个含单隐藏层的多层感知机，给定样本数为$n$、输入个数（特征数或者特征向量维度）为$d$的小批量数据样本$X \in R^{n \times d}$，设隐藏层的激活函数为$\phi$，那么隐藏层的输出$H \in R^{n \times h}$计算为：
H = \phi(XW_{xh} + b_h) 其中隐藏层权重参数$W_{xh} \in R^{d \times h}$， 隐藏层偏差参数$b_h \in R^{1 \times h}$，$h$为隐藏单元个数。上式相加的两项形状不同，因此按广播机制相加，将隐藏变量$H$作为输出层的输入，且输出个数为$q$（如分类问题中的类别数），输出层的输出为：
O = HW_{hq} + b_q 其中输出变量$O \in R^{n \times q}$，输出层权重参数$W_{hq} \in R^{h \times q}$，输出层偏差参数$b_q \in R^{1 \times q}$。如果是分类问题，我们可以使用$softmax(O)$来计算输出类别的概率分布。
含隐藏状态的循环神经网络 现在我们考虑输入数据存在时间相关性的情况，假设$X_t \in R^{n \times d}$是序列中时间步t的小批量输入，$H_t \in R^{n \times h}$是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量$H_{t-1}$，并引入一个新的权重参数$W_{hh} \in R^{h \times h}$，该参数用于描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步t的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定。
H_t = \phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h) 与多层感知机相比，我们在这里添加了$H_{t-1}W_{hh}$一项。由上式中相邻时间步的隐藏变量$H_t$,$H_{t-1}$之间的关系可知，这里的隐藏变量能够捕捉截止到当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或者记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络(recurrent neural network)。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Affinity Layer 仿射变换层 Definition Affinity layer，即Full-connected layer（仿射层或全连通层）是一层人工神经网络，其中所有包含的节点连接到后续层的所有节点。仿射层通常用于卷积神经网络和递归神经网络。受限玻尔兹曼机是仿射层或全连接层的一个例子。
对于每一个到仿射(全连接)层的连接，节点的输入是前一层输出的线性组合，带有附加的偏置。然后通过激活函数传递输入来计算节点的输出。数学上，这表示为: $$ y=f(W x+b) $$
$f$代表激活函数，$W,b$为可学习参数，$y$是输出，$x$是输入。
总结 Affinity Layer，就是 Full-connected Layer，就是仿射层，也是我们熟知的全连接层。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Affinity Matrix  reference: DeepAI(Link: https://deepai.org/)
 What is an Affinity Matrix? Affinity Matrix， 也叫做 Similarity Matrix。即关联矩阵，或称为相似度矩阵，是一项重要的统计学技术，是一种基本的统计技术，用于组织一组数据点之间的彼此相似性。相似度(similarity)类似于距离(distance)，但它不满足度量性质，两个相同的点的similarity scores为1，而在metric下将为0。
相似度量的典型例子是余弦相似度(cosine similarity)和Jaccard相似度(Jaccard Similarity)。这些相似性度量可以解释为两个点相关的概率。例如，如果两个数据点的坐标很接近，那么它们的余弦相似度分数(或各自的“相似度”分数)将非常接近于1。
Cosine Similarity 概念 余弦相似性度量内积空间中两个非零向量之间夹角的余弦。这种相似性度量特别关注方向，而不是大小。简而言之，在相同方向上对齐的两个余弦向量相似性度量为1，而两个垂直对齐的向量相似性度量为0。如果两个向量是截然相反的，这意味着它们的方向是完全相反的(即背对背)，那么相似性度量是-1。
计算公式： $$ \text { similarity }=\cos (\theta)=\frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}||\mathbf{B}|}=\frac{\sum_{i=1}^{n} A_{i} B_{i}}{\sqrt{\sum_{i=1}^{n} A_{i}^{2}} \sqrt{\sum_{i=1}^{n} B_{i}^{2}}}, $$
输出将产生一个从-1到1的值，表示相似性。其中-1是不相似的，0是正交的(垂直的)，1表示完全相似。
Jaccard Similarity Jaccard指数，也被称为Jaccard相似系数，是用来衡量样本集的相似性和多样性的指标。即图像目标识别领域非常常见的评价指标IoU。
计算公式 $$ J(A, B)=\frac{|A \cap B|}{|A \cup B|}=\frac{|A \cap B|}{|A|+|B|-|A \cap B|} $$
可见集合相似度度量使用IoU(Jaccard Similarity)</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>概述 在概率论与统计学中，狄利克雷分布 Dirichlet distribution 常被简记为$Dir(\alpha)$,是基于一个正实数向量$\alpha参数的连续多元概率分布族。狄利克雷分布是对贝塔分布 beta distribution的多元泛化，所以它也被称为多元贝塔分布 multivariate beta distribution(MBD)。
狄利克雷分布被广泛作为贝叶斯统计的先验分布使用。同时，狄利克雷分布也是分类分布Categorical distribution 和多项分布categorical distribution 的共轭先验。
狄利克雷分布的无限维推广就是狄利克雷过程Dirichlet process
#</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Matrix Differentiation（矩阵求导）  References: Matrix Differentiation,Rabdak J.Barnes 
 注： 本文直接从Matrix Differentiation开始记录，之前的乘法等基础部分不表。
Convention 3 m维向量对n维向量求导所得的结果是一个mxn矩阵,即Jacobian Matrix。 具体形式见上公式。
命题5 Proposition 5 即：Ax对x求导，结果为A
Proof 命题6 Proposition 6 即：y=Ax，而x是z的函数，那么便有$\frac{{\partial {\rm{y}}}}{{\partial z}} = A\frac{{\partial x}}{{\partial z}}$
Proof 命题7 Proposition 7 对于$\alpha = y^TAx$分别对x和y求导的结论。
Proof 命题8 Proposition 8 对于$\alpha = x^TAx$对x求导的结论。
Proof 命题9 Proposition 9 即命题8的特例，A是对称矩阵。
命题10 Proposition 10 即$\alpha = y^Tx$，而y和x均为向量z的函数，对z求导的结果。
Proof 命题11 Proposition 11 命题10的特例，$y=x$
命题12 Proposition 12 对于$\alpha = y^TAx$,x和y都是向量z的函数，对z求导的结果。
Proof 命题13 Proposition 13 命题12的特例：$y=x$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Corner Feature Detector(Intensity-Based) 基于光强比较的角点检测，直接比较光强（像素灰度值），而不计算梯度。所以实时性更好，所需的存储空间更小。
SUSAN 角点检测 SUSAN 全称Smallest univalue segment assimilating nucleus，最小核同值区。提出者Smith与Brady, 1997.
SUSAN 使用一个圆形模板和一个圆的中心点，通过圆的中心点象素值与模板圆内其他象素值的比较，统计出与圆中心点象素值近似的象素数量，当这样的象素数量小于某一阈值时，则该圆中心点就被认为是角点。
 圆形模板：通常是半径为3.5，37个像素的圆形 圆形模板中心点：圆心位置的像素 最小核同值区：像素值与圆心位置像素值接近的区域和（颜色接近的区域）  有两种划分（了解）：
平滑划分：$c\left(\vec{r}, \vec{r}{0}\right)=e^{-\left(\frac{I(r)-I\left(r{0}\right)}{t}\right)^{6}}$
直接划分：$c\left(\vec{r}, \vec{r}{0}\right)=\left{\begin{array}{ll}1 &amp;amp; \text { if }\left|I(\vec{r})-I\left(\vec{r}{0}\right)\right| \leq t \ 0 &amp;amp; \text { if }\left|I(\vec{r})-I\left(\vec{r}_{0}\right)\right|&amp;gt;t\end{array}\right.$
像素个数：$n\left(x_{0}, y_{0}\right)=\sum_{(x, y) \neq\left(x_{0}, y_{0}\right)} c(x, y)$
像素个数与阈值$g$进行比较，以此判断角点。 $$ R\left(\vec{r}{0}\right)=\left{\begin{array}{cc} g-n\left(\vec{r}{0}\right) &amp;amp; n\left(\vec{r}_{0}\right)&amp;lt;g \
0 &amp;amp; \text { otherwise } \end{array}\right. $$
FAST 角点检测 实时性好，不具有旋转不变性。
主要思想：比较中心像素与圆内（这里的圆内指的是圆边经过的像素）16个像素，如果圆内存在n个相邻的像素块都比中心像素的亮度$I_p$加上一个阈值$t$亮，或者都比$I_p - t$暗，则就判断其为角点。n通常选择为12. FAST uses binary comparison with each pixel along a circle pattern against the central pixel。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Introduction Image Matching是什么？  As a critical and fundamental problem in these complicated tasks, image matching, also known as image registration or correspondence, aims to identify then correspond the same or similar structure/content fromtwo ormore images.
 图像匹配，也被称为图像配准或图像对应，起目的在于识别出两幅或更多图像中的相同或相似结构/内容，然后将它们对应起来。
Image Matching 派生的具体任务  sparse feature matching 稀疏特征匹配 dense matching（like image registration and stereo matching） 稠密匹配 patch matching（retrieval） 2-D and 3-D point set registration graph matching  Image Matching 通常如何构成 Image matching 通常由两个部分构成，即陪匹配特征的性质与匹配策略，分别表示用什么特征进行匹配，如何进行匹配。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>语言模型 前言 DCP用到了Transformer，其很多实现的Motivation都来自于Sq2Sq的启发。所以重新回顾学习语言模型。
语言模型 语言模型是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。 我们可以把一段自然语言文本看做一段离散的时间序列。假设一段长度为T的文本中的词依次为$w_1,w_2,...,w_T$， 那么在离散的时间序列中，$w_t(1 \le t \le T )$可以看做在时间步t的输出。给定一个长度为T的词的序列$w_1,w_2,...,w_T$。语言模型将计算该序列的概率：
P(w_1,w_2,...,w_T) 语言模型的计算 假设序列$w_1,w_2,...,w_T$中的各个词是依次生成的，我们有：
P(w_1,w_2,...w_T) = \prod\nolimits_{t = 1}^{\rm{T}} {P(w_t|w_1,...,w_{t-1})} 例如，一段含有四个词的文本序列的概率：
P(w_1,w_2,w_3,w_4) = P(w_1)P(w2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3) 为了计算语言模型，我们需要计算词的概率，以及一个词在给定的前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，词的概率可以通过该词在训练数据集中的相对词频来计算。例如，$P(w_1)$可以计算为$w_1$在训练数据集中的词频与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如$P(w_2|w_1)$可以计算为$w_1,w_2$两词相邻的频率与$w_1$词频的比值，即$P(w_1,w_2)$与$P(w_1)$之比。以此类推。
n元语法 当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。n元语法通过马尔可夫假设（并不一定成立）简化了语言模型的计算。这里的马尔科夫假设是指一个词的出现如果只与前面n个词相关，即n阶马尔科夫链。如果$n=1$，那么有：$P(w_3|w_1,w_2) = P(w_3|w_2)$。如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：
P(w_1,w_2,...,w_T) \approx \prod\nolimits_{t=1}^{\rm{T}} {P(w_t|w_{t-(n-1)},...,w_{t-1})} 以上称为n元语法(n-grams)。它是基于n-1阶马尔科夫链的概率语言模型。当n分别为1，2和3时，我们将其分别称作一元语法(unlgram)、二元语法(blgram)和三元语法(trlgram)。例如，长度为4的序列$w_1,w_2,w_3,w_4$在一元语法，二元语法和三元语法中的概率分别为： 当n较小时，n元语法往往并不准确。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Reference: Wikipedia:Bayesian_inference
   Bayesian inference is a method of statistical inference in which Bayes&#39; theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>边际似然函数 统计学中，边际似然函数（marginal likelihood function 或 integrated likelihood）是一种似然函数，其中某些参数变量被边缘化。在贝叶斯统计的背景下，它常常代指证据evidence或模型证据model evidence。
概念 给定一组独立同分布的数据点$X = ({x_1}, \ldots ,{x_n})$,其中${x_i} \sim p({x_i}|\theta )$,$p({x_i}|\theta )$是一个概率分布，其参数为$\theta$，其中$\theta$本身就是一个随机变量，可以用一个概率分布来描述，即$\theta \sim p(\theta |\alpha )$。而边际似然函数就是求概率$p(X|\alpha)$是多少，其中参数$\theta$被边缘化（marginalized out)而消失:
p(X|\alpha ) = \int_\theta {p(X|\theta )p(\theta |\alpha )d\theta } 上述定义是在贝叶斯统计下提出的。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>Sinkhorn算法 Sinkhorn 算法描述了任意一个正矩阵(元素均为正)与双随机矩阵之间的关系。
简略描述  Relations between arbitrary positive matrices and Doubly stochastic matrices. 如果A是一个正矩阵，那么通过交替的进行行归一化和列归一化，可以将其转换为一个转移矩阵(双随机矩阵)。
  if A is a positive Matrix, alternately applies row-normalization and column-normalization will convert A to a doubly stochastic matrices.
 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>今日收获 收获  可以使用TopK来代替NearestNeighbor，可以加快计算的速度。  问题 今天的主要问题还是：KNN的反向传播问题。不论是KNN还是TopK，都无法反向传播梯度，为整体训练带来困难。
找到两个非常有用的网页：
  https://discuss.pytorch.org/t/use-topk-as-a-selection-but-fail-to-autograd/58004
  https://discuss.pytorch.org/t/k-nearest-neighbor-in-pytorch/59695
  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://codefmeister.github.io/p/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/</guid>
      <description>高斯混合模型 混合模型概述  In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with &amp;ldquo;mixture distributions&amp;rdquo; relate to deriving the properties of the overall population from those of the sub-populations, &amp;ldquo;mixture models&amp;rdquo; are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.</description>
    </item>
    
    <item>
      <title>强化学习纲要(IntroToRL)笔记</title>
      <link>https://codefmeister.github.io/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BA%B2%E8%A6%81introtorl%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BA%B2%E8%A6%81introtorl%E7%AC%94%E8%AE%B0/</guid>
      <description>What is reinforcement learning a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex and uncertain environment.
Difference between Reinforcement Learning and Supervised Learning  Sequential data as input(not i.i.d) 序列化输入 The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. Trial-and-error exploration(balance between exploration and exploitation) There is no supervisor, only a reward signal, which is also delayed.</description>
    </item>
    
  </channel>
</rss>
