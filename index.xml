<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Codefmeister</title>
        <link>https://codefmeister.github.io/</link>
        <description>Recent content on Codefmeister</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 02 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://codefmeister.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Harris 角点</title>
        <link>https://codefmeister.github.io/p/harris-%E8%A7%92%E7%82%B9/</link>
        <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/harris-%E8%A7%92%E7%82%B9/</guid>
        <description>&lt;h1 id=&#34;moravec-detector&#34;&gt;Moravec Detector&lt;/h1&gt;
&lt;p&gt;Moravec角点检测是第一个提出兴趣点(interest points)的Paper。它的主要思想是：以每个像素为中心，有一个固定的滑动窗口。该方法计算并在八个方向上（纵横以及斜对角）搜索每个像素的最小强度变化，如果最小值大于给定阈值，则检测出感兴趣点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Moravec_fig1.png&#34; alt=&#34;Moravec示意图&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;其可以用数学公式表达为：$E(u, v)=\sum_{x, y} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$E(u,v)$代表像素中心$(x,y)$在偏移量$(u,v)$的方向上的强度变化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w(x,y)$是一个指示函数，当$(x,y)$在滑动窗口内时，为1，若在滑动窗口外，则为0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$I(x,y)$指的是在像素点(x,y)处的光强或者说是灰度值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;值得说明的是，这个式子是Harris总结的。&lt;/p&gt;
&lt;h1 id=&#34;harris-corner-detector&#34;&gt;Harris Corner Detector&lt;/h1&gt;
&lt;p&gt;Moravec存在着许多不足。非常重要的一点就是：由于对于灰度值变化的梯度判断是离散的进行在8个方向，所以不具有旋转不变性；同时还会出现误判，尤其是当一条线不平行于这八个方向时，线上的点也会被误检测为角点。&lt;/p&gt;
&lt;p&gt;为了进一步改进Moravec角点检测，Harris提出了著名的Harris角点检测。&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;E(u, v)=\sum_{(x, y)} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}\&lt;br&gt;
&amp;amp;\approx \sum_{(x, y)} w(x, y)\left[I(x, y)+\frac{\partial I}{\partial x}(x, y) u+\frac{\partial I}{\partial y}(x, y) v-I(x, y)\right]^{2}\qquad\text { (一阶泰勒展开) }\&lt;br&gt;
&amp;amp;\approx \sum_{(x, y)} w(x, y)\left[\frac{\partial I}{\partial x}(x, y) u+\frac{\partial I}{\partial y}(x, y) v\right]^{2} \qquad \text { (消除重复项) }\&lt;br&gt;
&amp;amp;=\sum_{x, y} w(x, y)\left(u^{2} f_{x}^{2}(x, y)+2 u v f_{x}(x, y) f_{y}(x, y)+v^{2} f_{y}^{2}(x, y)\right)\text {  }\&lt;br&gt;
&amp;amp;\begin{array}{l}
=\sum_{x, y} w(x, y)\left(u^{2} I_{x}^{2}+2 u v I_{x} I_{y}+v^{2} I_{y}^{2}\right)\qquad \text { (简化) } \&lt;br&gt;
=\left[\begin{array}{ll}
u &amp;amp; v
\end{array}\right]\left(\sum w(x, y)\left[\begin{array}{cc}
I_{x}^{2} &amp;amp; I_{x} I_{y} \&lt;br&gt;
I_{x} I_{y} &amp;amp; I_{y}^{2}
\end{array}\right]\right)\left[\begin{array}{l}
u \&lt;br&gt;
v
\end{array}\right] \&lt;br&gt;
=\left[\begin{array}{l}
u \&lt;br&gt;
v
\end{array}\right]^{T} H\left[\begin{array}{l}
u \&lt;br&gt;
v
\end{array}\right]
\end{array}
\end{aligned}
$$
可以看到，E的变化主要与H的大小相关。所以只需要分析H的变化就可以得到E的变化趋势。&lt;/p&gt;
&lt;p&gt;图像梯度：$\Delta I(x, y)=\left(\frac{\partial I}{\partial x}(x, y), \frac{\partial I}{\partial y}(x, y)\right)$&lt;/p&gt;
&lt;p&gt;Harris矩阵：$H=\left[\begin{array}{cc}\sum_{(x, y)} w(x, y)\left(\frac{\partial I}{\partial x}(x, y)\right)^{2} &amp;amp; \left.\sum_{(x, y)} w(x, y)\left(\frac{\partial I}{\partial x}(x, y)\right) \frac{\partial I}{\partial y}(x, y)\right) \ \left.\sum_{(x, y)} w(x, y)\left(\frac{\partial I}{\partial x}(x, y)\right) \frac{\partial I}{\partial y}(x, y)\right) &amp;amp; \sum_{(x, y)} w(x, y)\left(\frac{\partial I}{\partial y}(x, y)\right)^{2}\end{array}\right]$&lt;/p&gt;
&lt;p&gt;w：表示权重，可以是0/1， 也可以是以点为中心的高斯权重。&lt;/p&gt;
&lt;p&gt;图像的水平梯度与垂直梯度：$I_{x}=\frac{\partial I(x+u, y+v)}{\partial x} \quad I_{y}=\frac{\partial I(x+u, y+v)}{\partial y}$&lt;/p&gt;
&lt;h2 id=&#34;harris矩阵的特征值分析&#34;&gt;Harris矩阵的特征值分析&lt;/h2&gt;
&lt;p&gt;对于图像：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平坦区域：梯度方向各异，但是梯度幅值变化不大&lt;/li&gt;
&lt;li&gt;线性边缘：梯度幅值改变较大，梯度方向改变不大&lt;/li&gt;
&lt;li&gt;角点：梯度方向和梯度幅值变化都较大&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mathbf{H}$就是Harris矩阵。对其进行奇异值分解，两个特征值分别反映互相垂直方向上的梯度变化情况，分别代表最快和最慢的方向。特征值大的变化快，特征值小的变化慢。&lt;/p&gt;
&lt;p&gt;进行特征值分解：$S V D(H)=U \sum V,\left(\lambda_{1}, \lambda_{2}\right), \quad \lambda_{1}&amp;gt;\lambda_{2}$&lt;/p&gt;
&lt;p&gt;那么就有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征值都比较大，则窗口中含有角点。&lt;/li&gt;
&lt;li&gt;特征值一个比较大，一个比较小，则窗口中含有边缘线。&lt;/li&gt;
&lt;li&gt;特征值都比较小，则处在平坦区域。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Morris_fig1.png&#34; alt=&#34;Morris检测&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;harris角点准则&#34;&gt;Harris角点准则&lt;/h2&gt;
&lt;p&gt;$$\operatorname{det} H=\lambda_{1} \lambda_{2}$$&lt;/p&gt;
&lt;p&gt;$$\operatorname{trace} H=\lambda_{1}+\lambda_{2}$$&lt;/p&gt;
&lt;p&gt;所以某点的响应函数：&lt;/p&gt;
&lt;p&gt;$R=\operatorname{det}(H)-\operatorname{ktrace}(H)^{2}=\lambda_{1} \lambda_{2}-k\left(\lambda_{1}+\lambda_{2}\right)^{2}, k=0.04$&lt;/p&gt;
&lt;p&gt;利用C与阈值比较判断，来判断其是否是角点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$R&amp;lt;0$  边缘点&lt;/li&gt;
&lt;li&gt;$R \approx 0$ 平坦点&lt;/li&gt;
&lt;li&gt;$R &amp;gt; 0$ 角点&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;shi-tomasi-角点检测&#34;&gt;Shi-Tomasi 角点检测&lt;/h1&gt;
&lt;p&gt;在Harris角点检测的基础上，Shi和Tomasi 在1993的一篇论文《Good Features to track》中提出了基于Harris角点检测的Shi-Tomasi方法。&lt;/p&gt;
&lt;p&gt;Harris角点检测的稳定性与k值有关，而k是个经验值。不好设定。&lt;/p&gt;
&lt;p&gt;在此基础上，Shi与Tomasi发现，焦点的稳定性与矩阵的娇小的特征值有关，直接使用较小的特征值作为分数。&lt;/p&gt;
&lt;p&gt;所以：
$R=\min \left(\lambda_{1}, \lambda_{2}\right)$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Stochastic Matrix, Doubly Stochastic Matrix, Permutation Matrix</title>
        <link>https://codefmeister.github.io/p/stochastic-matrix-doubly-stochastic-matrix-permutation-matrix/</link>
        <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/stochastic-matrix-doubly-stochastic-matrix-permutation-matrix/</guid>
        <description>&lt;h1 id=&#34;stochastic-matrix&#34;&gt;Stochastic Matrix&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition:&lt;/p&gt;
&lt;p&gt;In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain. Each of its entries is a nonnegative real number representing a probability. It is also called a probability matrix, transition matrix, substitution matrix, or Markov matrix. There are several different definitions and types of stochastic matrices&lt;/p&gt;
&lt;p&gt;A right stochastic matrix is a real square matrix, with each row summing to 1.&lt;/p&gt;
&lt;p&gt;A left stochastic matrix is a real square matrix, with each column summing to 1.&lt;/p&gt;
&lt;p&gt;A doubly stochastic matrix is a square matrix of nonnegative real numbers with each row and column summing to 1&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在数学上，随机矩阵Stochastic Matrix 是一个用于描述马尔科夫链转移的方阵，其每个元素都是一个非负实值，代表一个概率。 它也被称为 概率矩阵 Probability Matrix, 转移矩阵Transition Matrix, 替代矩阵 Substitution Matrix, 或者 马尔科夫矩阵 Markov Matrix.&lt;/p&gt;
&lt;h2 id=&#34;right-stochastic-matrix&#34;&gt;Right stochastic matrix&lt;/h2&gt;
&lt;p&gt;Right stochastic matrix 是一个实值方阵，其每一行的和是1.&lt;/p&gt;
&lt;h2 id=&#34;left-stochastic-matrix&#34;&gt;Left stochastic matrix&lt;/h2&gt;
&lt;p&gt;Left stochastic matrix 是一个实值方阵，其每一列的和是1.&lt;/p&gt;
&lt;h2 id=&#34;doubly-stochastic-matrix&#34;&gt;Doubly stochastic matrix&lt;/h2&gt;
&lt;p&gt;Doubly stochastic matrix 是一个方阵，其行列的和均为1.&lt;/p&gt;
&lt;h1 id=&#34;permutation-matrix&#34;&gt;Permutation Matrix&lt;/h1&gt;
&lt;p&gt;Permutation Matrix， 置换矩阵，是一种特殊的Doubly stochastic matrix. 其元素是0与1，置换矩阵的每一行和每一列都恰好有一个1，其余元素都是0&lt;/p&gt;
</description>
        </item>
        <item>
        <title>RPM-Net论文阅读笔记</title>
        <link>https://codefmeister.github.io/p/rpm-net%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/rpm-net%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;ICP算法可以解决刚体点云配准问题，其首先进行hard-assignment，寻找空间中的最近点对的对应关系。其次再解决最小二乘问题。 基于空间最近距离的hard-assignment 对于初始的位姿以及噪音和离群点十分敏感，所以鲁棒性不高，经常会收敛到局部最优。&lt;/p&gt;
&lt;p&gt;在此Paper中，作者提出了RPM-Net，一个对于初始刚体变换不敏感，同时更加鲁棒的，基于深度学习的点云配准方法。为了达到这个目的，作者使用了可微分的Sinkhorn层，随后利用从空间坐标和局部几何结构中学习得到的混合特征，退火(annealing)得到点对间的soft assignment软匹配结果。 同时，为了进一步的提高配准的精度，我们引入了一个二次网络，用于预测最优的退火(annealing)参数。 不同于目前已有的方法，RPM-Net可以处理缺失对应点对关系和部分重叠的点云。实验结果表明，RPM-Net可以达到state-of-art。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;点云配准问题，是指给定的两帧未知对应点对关系的点云，寻找刚体变换关系，将其配准在一起。不论是获得点对之间的对应关系，还是得到刚体变换参数，都会使剩下的问题微不足道。&lt;/p&gt;
&lt;p&gt;ICP，广泛应用。 对初始变换关系和噪音和离群点敏感，容易收敛到局部最优。ICP算法的深度学习实现(Deep Closest Point)通过深度学习得到的特征来进行对应点对关系，使得其对初始位姿不敏感，但其仍对outliers不鲁棒， 同时对于部分重叠的点云无法很好的工作。&lt;/p&gt;
&lt;p&gt;为了解决ICP的问题，人们提出了许多方法。其中非常突出的一篇便是&amp;quot;RPM&amp;quot;，Robust Point Matching，它首先对点对对应关系进行soft assignment， 然后逐步通过确定的退火策略一步步harden 对应关系。纵然RPM比ICP更加鲁棒，但其仍然对于初始刚体变化十分敏感，容易陷入局部最优，原因在于其点对对应关系只是单独的从空间距离中得到的。另一方面，基于特征的方法避免了初始位姿的问题，其通过挖掘独特的keypoint，同时使用特征描述符对keypoint局部几何特征进行描述。使用这些keypoint进行match， 然后使用鲁棒的RANSAC（随机抽样一致性）策略，来鲁棒地计算出刚体变换关系。此类方法只对几何特征显著的点云效果很好。&lt;/p&gt;
&lt;p&gt;此Paper中提出了，基于深度学习的RPM策略，RPM-Net： 一个端到端的可微分的深度网络，不但保留了RPM对于噪音和离群点的鲁棒性，同时从学习到的特征距离而不是spatial的点对对应关系来对初始化进行脱敏处理。为了达到此目的，我们设计了一个特征提取网络，从逐点的空间坐标以及几何特征中，计算得到其混合特征。随后使用Sinkhorn层与退火策略，从混合特征中得到soft assignment.空间坐标与几何属性的混合，与从数据中的学习过程，改进了点对关系。这对初始化刚体变换关系进行了脱敏处理，同时增加了对于缺失对应点对关系以及部分重叠的点云之间的配准能力。类似于ICP算法以及其变种，RPM-Net也是迭代地对刚体变换进行求精。进一步，我们引入了一个子网络，基于当前的配准状态，来预测最优的退火参数。也就是说，我们的退火策略并不是固定好的某个模式，而是在学习过程中动态生成的。因为使用了混合特征，我们的算法可以在很小的几次迭代后就收敛。&lt;/p&gt;
&lt;p&gt;其贡献:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其用于配准的网络架构&lt;/li&gt;
&lt;li&gt;其引入的用于预测退火参数的子网络&lt;/li&gt;
&lt;li&gt;提出一个改良的倒角距离度量Modified Chamfer distance metric， 用于度量部分重叠的配准质量。&lt;/li&gt;
&lt;li&gt;对比实验&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;p&gt;该工作是基于RPM框架完成的，这里将简单介绍RPM的工作。&lt;/p&gt;
&lt;p&gt;首先定义一个Match Matrix匹配矩阵$\mathbf{M}={0,1}^{J \times K}$, 来表示点的对应关系的分配。其中每个元素：
$$
m_{j k}=\left{\begin{array}{ll}
1 &amp;amp; \text { if point } \mathbf{x}_{j} \text { corresponds to } \mathbf{y}_{k} \&lt;br&gt;
0 &amp;amp; \text { otherwise }
\end{array}\right. \tag{1}
$$&lt;/p&gt;
&lt;p&gt;首先考虑每个点都有其对应点（即one to one）的情况，在这种情况下，$\mathbf{M}$是一个方阵。配准问题可以被表示为寻找将点云$\mathbf{X}$上的点映射到$\mathbf{Y}$的最优刚体变换关系${\mathbf{R}, \mathbf{t}}$ 和对应矩阵$\mathbf{M}$. 即：
$$
\underset{\mathbf{M}, \mathbf{R}, \mathbf{t}}{\arg \min } \sum_{j=1}^{J} \sum_{k=1}^{K} m_{j k}\left(\left|\mathbf{R} \mathbf{x}_{j}+\mathbf{t}-\mathbf{y}_{k}\right|_{2}^{2}-\alpha\right) \tag{2}
$$
且服从如下约束：
$\sum_{k=1}^{K} m_{j k}=1, \forall j;  \sum_{j=1}^{J} m_{j k}=1, \forall k; m_{j k} \in{0,1}, \forall j k$.&lt;/p&gt;
&lt;p&gt;这三个约束使得$\mathbf{M}$成为一个置换矩阵 &lt;em&gt;Permutation Matrix&lt;/em&gt;。 $\alpha$ 是控制对应关系数量的参数，用来拒绝离群点： 对于任意点对$\left(\mathbf{x}&lt;em&gt;{j}, \mathbf{y}&lt;/em&gt;{k}\right)$, 如果其距离$\left|\mathbf{R} \mathbf{x}&lt;em&gt;{j}+\mathbf{t}-\mathbf{y}&lt;/em&gt;{k}\right|&lt;em&gt;{2}^{2}&amp;lt;\alpha$， 其会被考虑为inlier， 因为此时将$m&lt;/em&gt;{jk}$设置为1，可以降低Eq.2的cost.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>FCGF论文阅读笔记</title>
        <link>https://codefmeister.github.io/p/fcgf%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/fcgf%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;0-abstract&#34;&gt;0. Abstract&lt;/h1&gt;
&lt;p&gt;从三维点云或者扫描帧中提取出几何特征是许多任务例如配准，场景重建等的第一步。现有的领先的方法都是将low-level的特征作为输入，或者在有限的感受野上提取得到基于patch的特征。本文提出的是一个全卷积几何特征提取网络，名为fully-convolutional geometric features。 通过一个3D的全卷积网络的一次pass，即可得到几何特征。 同时提出了一个新的度量学习的loss函数，可以显著的提高网络的性能。 FCGF的几何特征十分紧凑，可以捕捉广阔的上下文空间，并缩放到大型场景中。在室内（3D Match）和室外（KITTI）数据集上进行验证的结果显示，其达到了state-of-art的精确率，而且并不需要数据的预处理，并且很紧凑（32维的特征），比其余最精确的方法快290倍。&lt;/p&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;寻找几何意义上的点对关系是很多三维任务十分重要的一步。因此，大量的工作集中于设计三维特征，以捕捉具有可判别性的局部几何机构，来建立点对关系。&lt;/p&gt;
&lt;p&gt;基于学习的三维特征由于其鲁棒性与出色的性能表现，在最近得到了广泛的关注。&lt;!-- raw HTML omitted --&gt;现有的基于学习的特征提取工作大都依赖于低阶的几何特征作为输入，例如角度偏差，点的分布，或者体积距离函数等。随后，对每个兴趣点(point of interest)提取一个三维patch，然后通过多层感知机或者三维卷积层将之映射到一个低维的特征空间。&lt;!-- raw HTML omitted --&gt; 这个过程计算代价高昂，而且只能够提取得到降采样后兴趣点处的特征，因此会降低后续配准步骤中的空间分辨率。&lt;/p&gt;
&lt;p&gt;上述的基于patch的处理过程效率很低，因为其中间网络的激活结果并没有在相邻的patch上进行复用。用2D卷积进行类比，对某个兴趣点提取其三维patch与对某个像素块提取其周围的一个像素patch类似。不仅如此，现有的pipeline仅局限于对空间范围有限的patch进行卷积，限制了空间语境的解读。&lt;/p&gt;
&lt;p&gt;不同于上文所述，我们应用一个可以作用于整个输入的三维卷积操作，而不需要裁剪片段，该操作是通过将卷积转换为全卷积的子项来完成的（convolution组装得到一个fully convolution）。 相似的，我们将MLP中的全连接层用一系列卷积层来替代，其卷积核的size为1x1x1。 全卷积网络与非全卷积网络相比，可以捕捉更广阔的上下文，更快，内存效率更高。其原因在于中间的激活结果在重叠的区域上进行了复用。&lt;/p&gt;
&lt;p&gt;尽管有这些优势，全卷积网络因为三维数据的(一些)特点并未在三维特征提取中得到广泛的应用。一个对三维数据进行卷积的卷积网络的标准输入代表是一个稠密的四维tensor，其中三维是空间维度，还有一个特征维。这种表示方式对内存消耗很大，很多voxel都是空的。&lt;/p&gt;
&lt;p&gt;在本文的工作中，采用了一种稀疏的tensor表示法。同时，针对全卷积上的度量学习，提出了一个新的loss函数。因为观察到全卷积特征不同于传统的度量学习的假设，传统的度量学习由于锚点都是随机采样的，所以假设样本是独立同分布的，而在全卷积网络中，相邻的点的特征是高度相关的。并不符合独立同分布的假设，所以需要重新设计loss函数。同时，该方法并不需要对数据进行低阶的预处理，或者提取三维patch，可以快速的生成高分辨率的， 具有state-of-art的判别潜力的特征。&lt;/p&gt;
&lt;p&gt;FCGF在室内室外数据集上均进行了测试，可以达到state-of-art的性能表现，比最快的快9倍，比最好的快290倍。&lt;/p&gt;
&lt;h1 id=&#34;2-related-work&#34;&gt;2. Related Work&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Hand-craft 3D feature&lt;/strong&gt;：早期对三维特征的描述集中在手工的，能够有区别的（有基于feature鉴别的潜力 discriminatively）的对局部几何特征进行刻画的描述符。Spin Images [16] use a projection of adjacent points onto the tangent plane. USC [29] uses covariance matrices of point pairs. SHOT [26] creates a 3D histogram of normal vectors. PFH [24] and FPFH [23] build an oriented histogram using pairwise geometric properties. Guo et al. [13] provide a comprehensive review of such hand-crafted descriptors&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learning-based 3D feature&lt;/strong&gt;： 最近，注意力大都转移到了基于学习的三维特征提取。Zeng et al. [36] use a siamese convolutional network to learn 3D patch de- scriptors. Khoury et al. [17] map 3D oriented histograms to a low-dimensional feature space using multi-layer per- ceptrons. Deng et al. [7, 6] adapt the PointNet architecture for geometric feature description. Yew and Lee [34] use a PointNet to extract features in outdoor scenes.&lt;/p&gt;
&lt;p&gt;我们的工作指出了一系列先前工作的局限性。&lt;!-- raw HTML omitted --&gt;首先，所有先前的方法都需要提取一个小的三维patch，或者一系列点，然后将其映射到低维特征空间，这不但限制了网络的感受野，也使得计算效率很低。甚至是在有重叠的三维区域上，所有中间的表示都需要分别计算。 其次，使用了昂贵的低阶几何特征作为输入，会降低特征计算的速度。 最后， 将特征提取限制在一个兴趣点的子集中，降低了空间的分辨率，会降低后续工作的精确度。&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fully-convolutional networks&lt;/strong&gt;: 全卷积网络是由Long在图像领域中提出的。三维领域中常被用于语义分割。&lt;!-- raw HTML omitted --&gt;全卷积网络的广泛应用主要源于其三个优势：首先，全卷积网络效率很高，计算速度快，因为中间激活结果可以在感受野有重叠的神经元上共享。其次，全卷积网络中的神经元有着更加广阔的感受野，因为其不再被限制在分别提取和处理过的patch上。第三点，全卷积网络的输出是稠密的。非常适合需要对场景进行详细描述的任务。&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deep metric learning&lt;/strong&gt;: 深度度量学习结合了深度神经网络和传统的度量学习，来生成紧凑的embedding。The contrastive loss formulates the objective in terms of pairwise constraints [14]. There has also been significant interest in higher-order loss terms, including triplet [32], quadruplet [18], and histogram losses [30]. Due to the polynomial growth in complexity that accompanies high-order losses, many recent papers focus on triplets with hard-negative mining within a batch. Lifted structure [28] and N-pair losses [27] proposed using a softmax for mining hard negatives within a batch.&lt;/p&gt;
&lt;p&gt;在本文中，我们研究了全卷积度量学习，其基本的立足点是： 一个batch内特征是独立同分布的假设不再成立。为了解决这一问题，提出了新的loss函数。&lt;/p&gt;
&lt;h1 id=&#34;3-sparse-tensors-and-convolutions&#34;&gt;3. Sparse Tensors and Convolutions&lt;/h1&gt;
&lt;p&gt;空间中的三维点云数据往往是稀疏的，我们使用稀疏矩阵的高阶等价形式&amp;ndash;稀疏张量。数学上，我们可以将三维数据的稀疏张量表示为坐标$C$和相关特征$F$的集合：
$$
C=\left[\begin{array}{cccc}
x_{1} &amp;amp; y_{1} &amp;amp; z_{1} &amp;amp; b_{1} \&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \&lt;br&gt;
x_{N} &amp;amp; y_{N} &amp;amp; z_{N} &amp;amp; b_{N}
\end{array}\right], F=\left[\begin{array}{c}
\mathbf{f}_{1}^{T} \&lt;br&gt;
\vdots \&lt;br&gt;
\mathbf{f}_{N}^{T}
\end{array}\right]
\tag{1}
$$&lt;/p&gt;
&lt;p&gt;其中$x_{i}, y_{i}, z_{i} \in \mathbb{Z}$是第i个三维坐标, $b_i$是batch的索引，为batch processing 提供了一个额外的维度，$\mathbf{f_i}$是与第i个点相关的特征。&lt;/p&gt;
&lt;p&gt;在稀疏张量上进行卷积需要一种与传统卷积不同的定义。在离散，稠密的三维卷积(discrete, dense convolution)中，我们提取输入特征然后与dense kernel matrix相乘。用$\mathcal{V}^{n}(K)$表示n维空间中的一组偏移量，其中$K$是核尺寸。例如，在一维卷积中，$\mathcal{V}^1(3) = {-1, 0, 1}$. 那么传统的稠密离散的三维卷积可以被定义为Eq.2，其中$W_i$代表在偏移$i$处的kernel value.
$$
\mathbf{x}&lt;em&gt;{\mathbf{u}}^{\text {out }}=\sum&lt;/em&gt;{\mathbf{i} \in \mathcal{V}^{3}(K)} W_{\mathbf{i}} \mathbf{x}_{\mathbf{u}+\mathbf{i}}^{\text {in }} \text { for } \mathbf{u} \in \mathbb{Z}^{3} \tag{2}
$$&lt;/p&gt;
&lt;p&gt;这个公式乍一看可能有点懵，但实际上十分简单，$\mathbf{u} \in \mathbb{Z}^3$是欲求得特征的点的三维坐标，因为并不是在实数域上，所以是离散的(discrete)，但在每个可能位置都可以卷积，所以又是稠密的。 而$\mathbf{i} \in \mathcal{V}^3(K)$是三维意义上的偏离量，$\mathbf{i}$的可能取值例如有：(1,1,1), (1,0,1)等等。 而$\mathbf{u} + \mathbf{i}$就代表了偏移后的坐标，$\mathbf{x}_{\mathbf{u}+\mathbf{i}}^{\text{in}}$是在该偏移坐标上的输入（可能是坐标，也可能是一些低阶特征），乘以对应的kernel value，然后Sum起来，就完成了卷积操作。&lt;/p&gt;
&lt;p&gt;但与相反，稀疏张量在$\mathbf{u}$处可以求得特征，当且仅当其对应点即$\mathbf{u} + \mathbf{i}$在集合$C$中。因此，只在子集$\mathcal{N}^{n}(\mathbf{u}, K, C)=\left{\mathbf{i} \mid \mathbf{i} \in \mathcal{V}^{n}(K), \mathbf{i}+\mathbf{u} \in C\right}$上进行卷积操作就足够了。该集合$\mathcal{N}$是$\mathbf{i}$的集合，$\mathbf{i}$满足既是偏移量，与$\mathbf{u}$相加后又在$C$上有定义。如果想让$C^{\text{in}}$和$C^{\text{out}}$的坐标不同（即输入的稀疏张量指定的点的坐标与想要得到的特征的空间点坐标不同），我们可以这样定义这种泛化意义下的稀疏卷积操作，如Eq.3所示：
$$
\mathbf{x}&lt;em&gt;{\mathbf{u}}^{\prime \text { out }}=\sum&lt;/em&gt;{\mathbf{i} \in \mathcal{N}^{3}\left(\mathbf{u}, K, C^{\text {in }}\right)} W_{\mathbf{i}} \mathbf{x}_{\mathbf{u}+\mathbf{i}}^{\text {in }} \text { for } \mathbf{u} \in C^{\text {out }} \tag{3}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparse fully-convolutional features:&lt;/strong&gt; 全卷积网络纯粹由平移不变操作构成，例如卷积操作和基于元素的非线性操作。相似的，如果我们对一个稀疏张量应用一个稀疏的卷积网络，我们会得到一个稀疏的输出张量。我们将这个输出的张量叫做&amp;quot;fully convolutional features&amp;quot;。 我们使用一个带有skip connection 和 残差模块的UNet架构去提取fully convolutional features. 其架构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/FGCF_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;白色块代表着输入和输出层，每个block都由三个参数指定: kernel size, stride, channel dimensionality. 所有的卷积操作（除了最后一层）后都应用了一个batch norm和一个ReLU.&lt;/p&gt;
&lt;p&gt;如果对sparse tensor 和 dense tensor以及对应的卷积操作仍有些迷糊，可以看下图：&lt;/p&gt;
&lt;p&gt;Dense Tensor and its convolution&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/FCGF_gif1.gif&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Sparse Tensor and its convolution&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/FCGF_gif2.gif&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h1 id=&#34;4-fully-convolutional-metric-learning&#34;&gt;4. Fully-convolutional Metric Learning&lt;/h1&gt;
&lt;p&gt;在本节中，我们首先简单回顾一些标准的度量学习损失函数，以及负样本挖掘技术(negative-mining)。随后，我们描述了在全卷积设置下的度量学习，提出了全卷积特征的变种(variants for fully-convolutional features) ， 其将负样本挖掘技术整合于二元误差(contrastive loss)和三元组误差(triplet loss)中。将这种新的loss称为“hardest-contrastive”和&amp;quot;hardest-triplet&amp;quot;.&lt;/p&gt;
&lt;p&gt;度量学习的出发点有两个约束：&lt;!-- raw HTML omitted --&gt;相似的feature之间要彼此很近。$D\left(\mathbf{f}&lt;em&gt;{i}, \mathbf{f}&lt;/em&gt;{j}\right) \rightarrow 0 \quad \forall(i, j) \in \mathcal{P}$, 而不相似的特征之间的必须要有一个margin以上的距离。$D\left(\mathbf{f}&lt;em&gt;{i}, \mathbf{f}&lt;/em&gt;{j}\right)&amp;gt;m \quad \forall(i, j) \in \mathcal{N}$，&lt;!-- raw HTML omitted --&gt; 其中$D(\cdot, \cdot)$是距离度量函数。将之间的差距进行平方后就能得到一个标准的contrastive loss。但是Lin指出，positive pairs的约束太过严苛，$D\left(\mathbf{f}&lt;em&gt;{i}, \mathbf{f}&lt;/em&gt;{j}\right) \rightarrow 0 \quad \forall(i, j) \in \mathcal{P}$，可能会导致过拟合，于是将0也替换为了一个Positive Margin。
$$
L\left(\mathbf{f}&lt;em&gt;{i}, \mathbf{f}&lt;/em&gt;{j}\right)=I_{i j}\left[D\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)-m_{p}\right]_{+}^{2}+\bar{I}_{i j}\left[m_{n}-D\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)\right]_{+}^{2}
$$
其中当$(i,j) \in \mathcal{P}$ 时，$I_{ij}=1$。 否则为0。 $\bar{\cdot}$ 是逻辑非运算符。$m_p$和$m_n$是positive pair和negative pair的margin。&lt;/p&gt;
&lt;p&gt;类似的，我们可以将ranking constraint 即 $m+D\left(\mathbf{f}, \mathbf{f}&lt;em&gt;{+}\right)&amp;lt;D\left(\mathbf{f}, \mathbf{f}&lt;/em&gt;{-}\right)$转换为三元组的Loss 函数是：
$$
L\left(\mathbf{f}, \mathbf{f}&lt;em&gt;{+}, \mathbf{f}&lt;/em&gt;{-}\right)=\left[m+D\left(\mathbf{f}, \mathbf{f}&lt;em&gt;{+}\right)-D\left(\mathbf{f}, \mathbf{f}&lt;/em&gt;{-}\right)\right]_{+}^{2}
$$&lt;/p&gt;
&lt;p&gt;不管是对contrastive loss 还是对 triplet loss， 采样策略都会极大的影响性能，因为判别的边界往往是由几个非常少的难负例决定的。(hardest negatives)&lt;/p&gt;
&lt;h2 id=&#34;41-characteristics-of-fully-convolutional-features&#34;&gt;4.1 Characteristics of Fully-convolutional Features&lt;/h2&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;传统的度量学习假定： 特征feature是独立同分布的，因为batch是通过随机采样来构建的。然而，在全卷积特征提取中，相邻的feature之间是局部相关的。因此，难负例挖掘(hard-negative mining)会找到与锚点相邻的特征们，但是他们是假的负例。（难负例的定义是很难判别的负例，所以要再次送入网络进行学习。但是这里锚点相邻的特征是局部相关的，所以其在度量上接近是情有可原的，不是负例。）&lt;!-- raw HTML omitted --&gt; 所以，将假负例过滤掉是十分重要的，这里采用距离阈值进行过滤。&lt;/p&gt;
&lt;p&gt;此外，在全卷积设置下使用的特征数量比标准的度量学习算法中高出若干个数量级。因此，像标准的度量学习那样对batch内的所有成对距离是不可行的。&lt;/p&gt;
&lt;h2 id=&#34;42-hardest-contrastive-和-hardest-triplet-losses&#34;&gt;4.2 Hardest-contrastive 和 Hardest-triplet Losses&lt;/h2&gt;
&lt;p&gt;在这一节中，我们提出了用于全卷积特征学习的度量学习误差。 像其他的算法一样，我们关注的是有效的难负例挖掘。 首先我们对锚点和每一场景中待挖掘的集合进行采样。随后我们挖掘positive pair $\left(\mathbf{f}&lt;em&gt;{i}, \mathbf{f}&lt;/em&gt;{j}\right)$中$\mathbf{f}&lt;em&gt;i$和$\mathbf{f}&lt;em&gt;j$对应的最难负例$\mathbf{f}&lt;/em&gt;{i}^{-}, \mathbf{f}&lt;/em&gt;{j}^{-}$。同时去除落在对应锚点的一个确定半径中的假负例。 随后，我们对挖掘得到的四元组$\left(\mathbf{f}&lt;em&gt;{i}, \mathbf{f}&lt;/em&gt;{j}, \mathbf{f}&lt;em&gt;{i}^{-}, \mathbf{f}&lt;/em&gt;{j}^{-}\right)$使用成对损失，得到了convolutional contrastive loss.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
L_{C} &amp;amp;=\sum_{(i, j) \in \mathcal{P}}\left{\left[D\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)-m_{p}\right]_{+}^{2} /|\mathcal{P}|\right.\&lt;br&gt;
&amp;amp;+\lambda_{n} I_{i}\left[m_{n}-\min _{k \in \mathcal{N}} D\left(\mathbf{f}_{i}, \mathbf{f}_{k}\right)\right]_{+}^{2} /\left|\mathcal{P}_{i}\right| \&lt;br&gt;
&amp;amp;\left.+\lambda_{n} I_{j}\left[m_{n}-\min _{k \in \mathcal{N}} D\left(\mathbf{f}_{j}, \mathbf{f}_{k}\right)\right]_{+}^{2} /\left|\mathcal{P}_{j}\right|\right}
\end{aligned} \tag{5}
$$
其中$\mathcal{P}$是minibatch中在全卷积提取的features上所有的positive pair的集合。而$\mathcal{N}$是minibatch中全卷积feature的一个随机子集，用于负例挖掘。$I_i$是$I(i, k_i, d_t)$的缩写， 是一个指示函数。当特征$k_i$在以特征$i$为中心，$d_t$为半径的球体外时，指示函数取值为1. 反之取0. 其中$k_{i}=\operatorname{argmin}_{k \in \mathcal{N}} D\left(\mathbf{f}_{i}, \mathbf{f}_{k}\right)$， 即$k$是挖掘到的最难的负例。$\left|\mathcal{P}_{i}\right|=\sum_{(i, j) \in \mathcal{P}} I\left(i, k_{i}, d_{t}\right)$是第一项中有效的负挖掘数目。$\left|\mathcal{P}_{j}\right|$是第二项的。通过简单的对所有有效的负例pair等权的进行平均来进行归一化处理。$\lambda_n$是权重。&lt;/p&gt;
&lt;p&gt;类似的，带有最难负例挖掘的三元组的loss：
$$
\begin{aligned}
L_{T} &amp;amp;=\frac{1}{Z} \sum_{(i, j) \in \mathcal{P}}\left(I\left(i, k_{i}\right)\left[m+D\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)-\min _{k \in \mathcal{N}} D\left(\mathbf{f}_{i}, \mathbf{f}_{k}\right)\right]_{+}\right.\&lt;br&gt;
&amp;amp;\left.+I\left(j, k_{j}\right)\left[m+D\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)-\min _{k \in \mathcal{N}} D\left(\mathbf{f}_{j}, \mathbf{f}_{k}\right)\right]_{+}\right)
\end{aligned}  \tag{6}
$$
其中$Z=\sum_{(i, j) \in \mathcal{P}}\left(I\left(i, k_{i}\right)+I\left(j, k_{j}\right)\right)$， 一个归一化常数。 $\mathcal{P}$是batch中在全卷积提取的features上所有的positive pair的集合。 需要注意的是，这里延续了Hermans在其工作中提出的思路，使用了non-squared loss（即误差项没有进行平方）来缓和特征会塌陷于一点的问题。实验中我们发现，fully-convolutional hardest triplet loss会崩溃（所有特征全部收敛到一个点）。所以，我们使用了随机采样的三元组和最难负例挖掘的三元组进行混合的策略，来减小误差。这两项的权重等同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/FGCF_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h1 id=&#34;code解读&#34;&gt;Code解读&lt;/h1&gt;
&lt;p&gt;Wait to update&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Learning Multiview 3D point Cloud Registration</title>
        <link>https://codefmeister.github.io/p/learning-multiview-3d-point-cloud-registration/</link>
        <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/learning-multiview-3d-point-cloud-registration/</guid>
        <description>&lt;h1 id=&#34;learning-multiview-3d-point-cloud-registration&#34;&gt;Learning multiview 3D point cloud registration&lt;/h1&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;提出了一种全新的，端到端的，可学习的多视角三维点云配准算法。 多视角配准往往需要两个阶段：第一个阶段进行初始化配准，给定点云各帧之间两两的初始化刚体变换关系；第二个阶段在全局意义上进行不断精细化处理。前者往往由于点云之间的低重叠率，对称性，或者重复的场景片段而导致配准精度较差。因此，紧随其后的全局优化（Global Refinement）的目标就是在多个点云帧中建立一种循环一致性(cyclic consistency).&lt;/p&gt;
&lt;p&gt;而此文章提出了一种算法，将两个阶段融合在一起进行端对端的交替学习。在公认的基准数据集上的实验评估表明，其方法显著优于state-of-art，而且又是端到端可训练的，所需要的计算资源也更少。此外，其还进行了详细的分析和烧蚀试验去验证其方法的novel part.&lt;/p&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;三维计算机视觉的下游任务，如语义分割和目标检测，通常需要场景的整体表示。因此，将仅覆盖环境一小部分的单个点云配准和融合为一个全局一致的完整表示的能力是十分重要的，而且在增强现实和机器人技术中有着不少用例。相邻片段之间的双视角配准是一个被深入研究过的问题，传统的基于几何约束[51, 66, 56] 和手工设计的特征描述符[37, 27, 54, 69]的配准方法在某种程度上取得了成功。然而， 近些年，用于双视角三维点云配准的局部描述符的研究聚焦于深度学习方法[67: 3DMatch, 38, 21: Ppfnet, 64, 19: Ppf-foldnet, 28: perfect match]，这些方法成功捕捉并编码了隐藏在手工特征符下的证据。在此基础上，一些全新的端到端的双视角点云配准方法最近被提出[62: DCP, 42: Deepvcp]。 虽然双视角配准在很多任务中展现出了不错的性能，但对场景中的多个点云帧进行配准时，&lt;!-- raw HTML omitted --&gt;其存在一些概念上的缺陷：(1) 相邻点云之间的低重合率会导致不精确或者错误的匹配。(2) 点云配准必须依赖于非常局部的特征，对于3D场景结构简单或者重复结构较多的情况十分有害。(3)在两两配准之后，需要进行单独的处理来将所有双视角配准结果组合为一个全局表示。&lt;!-- raw HTML omitted --&gt; 与双视角配准相比，应用于无组织的点云片段上的全局一致的多视角配准方法能够更充分的从深度学习技术取得的进步中获益。 现有的领先方法仍然常常依赖于双视角映射的良好初始化（良好的初值），然后再在后续的步骤中通过一系列解耦的步骤进行全局优化。这种分层处理的一大缺点在于，姿态图所有节点上的全局噪声分布在配准结束后远不是随机的。也就是说，由于配准结果和初始的双视角映射高度相关，会存在着不可忽视的误差。&lt;/p&gt;
&lt;p&gt;在这篇论文中，作者提出了第一个端到端的，数据驱动的多视角点云配准算法。其方法以可能存在重叠关系的点云集合为输入，对每个点云帧输出一个刚体变换矩阵。我们从传统的两阶段方法中跳脱出来，让各个阶段彼此分离，直接学习以一种全局一致的方式对所有点云帧进行配准。&lt;/p&gt;
&lt;p&gt;其工作的主要贡献在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;将传统的两阶段方法用端到端的神经网络的方式阐述，在其前传过程中，主要解决了两个可微分的最优化问题：(i)对两两点云之间刚体变换参数估计的Procrustes问题。(ii) 刚体变换同步的谱松弛(spectral relaxation)问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提出了一个置信度估计模块，其使用了一个新颖的&lt;code&gt;overlap pooling layer&lt;/code&gt;重叠池化层来预测估算得到的双视角刚体变换参数的可信度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将多视角三维点云配准问题转换为迭代重加权最小二乘问题(IRLS)，迭代地优化两两配准之间的刚体变换估计和绝对坐标意义下的刚体变换估计（全局）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因为以上所提到的工作，所提出的多视角点云配准算法是(i) 计算效率很高 (ii) 可以达到更加精确的配准结果，因为残差会以一种迭代的方式被送回双视角配准网络中去。 (iii)不论是双视角配准还是多视角配准，都比现有的方法的精度要高，效果要好。&lt;/p&gt;
&lt;h1 id=&#34;2-related-work&#34;&gt;2. Related Work&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Pairwise registration&lt;/strong&gt;: 传统的双视角配准pipeline包含两个阶段： the coarse alignment stage（粗配准）， 为相对刚体变换参数提供一个初始估计；the refinement stage 通过迭代最小化配准误差，不断优化刚体变换参数。&lt;/p&gt;
&lt;p&gt;前者常常通过使用或者手工的，或者学习到的三维局部特征描述符结合类似RANSAC的鲁棒估计或者集合哈希来得到点对之间的对应关系。A parallel stream of works relies on establishing correspodences using the 4-point congruent sets. 在refinement stage，粗糙的刚体变换参数往往通过ICP算法的一个变种提高精度。 ICP类算法通过交替假设点集之间的对应关系和估计其新的刚体变换参数，来达到最优化的目的。但ICP对于离群点不具有鲁棒性，只有在初始的参数估计较好的情况下，才会收敛到全局最优。ICP算法往往通过添加额外的radiometric, temporal or odometry 约束来进行扩展。与我们工作同时进行的，DCP和Dvcp提出了将粗配准和精配准集成的端到端的可学习算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiview Registration&lt;/strong&gt; 多视角全局点云配准方法之目标在于通过结合多个视角提供的线索，解决双视角配准应用困难或结果很模糊的情况。第一类方法族使用一种多视角的ICP类模式对位姿和三维点对的对应关系进行优化。其中大多数都受到了增加的点对间对应关系估计复杂性的影响。为了减轻这种影响，有些方法只对运动进行优化，使用扫描来评估配准的误差。更进一步，其他的现代方法利用全局的循环一致性，只对初始时(指定的)点云间两两映射关系的集合进行优化。(通常情况下都是其他帧与第一帧进行配准) 这种有效的方法称之为同步&lt;code&gt;synchronaization&lt;/code&gt;。 来自运动的全局结构旨在通过分解旋转，平移和缩放变量来同步观测到的相对运动。[23 Deep Mapping]提出了一个使用两个网络进行全局点云配准的方法，一个网络用于位姿估计，另一个通过估计全局坐标的占用状态来对场景结构建模。&lt;/p&gt;
&lt;p&gt;或许与此文最相似的工作是[35], 作者旨在通过学习以数据驱动的加权函数来适应刚体变换同步层(transformation synchronization layer)的边缘权值。一个主要的概念上的不同在于其相对刚体变换是通过FPFH结合FGR进行估算而得的，因此不同于本文，本文的刚体变换关系是由学习得到的。此外，在每次迭代中，[35]提出的方法必须将点云转换为一个深度图像，然后再使用一个二维的卷积操作来近似一个权重函数。而本文的方法直接作用于点云，是完全可微分的，所以有助于端到端的多视角全局配准学习。&lt;/p&gt;
&lt;h1 id=&#34;3-end-to-end-multiview-3d-registration&#34;&gt;3. End-to-End Multiview 3D Registration&lt;/h1&gt;
&lt;p&gt;在这一节中，我们将所提出的多视角三维配准算法作为基于数据的函数的组合进行阐述。而用于近似这些函数的网络架构将在第4节中阐述其细节。我们首先从一个全新的基于学习的双视角点云配准算法开始，其以两个点云作为输入，输出是估计得到的刚体变换矩阵参数。(3.1节) 该方法通过使用一个可反向传播的转换同步层(transformation synchronization)拓展到多点云(3.2)。同步层的输入图作为边缘信息与相对刚体变换参数一起编码为这些成对映射的置信度，该输入图同样是使用一个新的神经网络进行估计得到的。最后，我们提出了一个IRLS模式(3.3)通过更新边的权重和两两点云间的位姿来不断细化全局配准结果。&lt;/p&gt;
&lt;p&gt;考虑一个可能存在重叠的点云集合$S = {\bold{S_i} \in \mathbb{R^{N \times 3}}, 1 \leqslant i \leqslant N_S}$. 多视角配准的任务是恢复刚体绝对位姿${ M_i^* \in SE(3)}$，给定了扫描集合，其中：&lt;/p&gt;
&lt;p&gt;$$
S E(3)=\left{\mathbf{M} \in \mathbb{R}^{4 \times 4}: \mathbf{M}=\left[\begin{array}{cc}
\mathbf{R} &amp;amp; \mathbf{t} \&lt;br&gt;
\mathbf{0}^{\top} &amp;amp; 1
\end{array}\right]\right}  \tag{1}
$$&lt;/p&gt;
&lt;p&gt;$R_i \in SO(3)$ ，$t_i \in \mathbb{R}^3$。 $S$可以通过连接信息表示为一个有限图$\mathcal{G}=(\mathcal{S}, \mathcal{E})$，其中每个点代表着一个单独的点集，边编码了$(i,j) \in \mathcal{E}$ 两个顶点间的相对旋转$R_{ij}$, 平移$t_{ij}$的信息。 相对刚体变换参数满足：$R_{ij} = R_{ji}^T$, $t_{ij} = -R_{ij}^Tt_{ji}$，同时还需满足compatibility constraint：
$$
R_{ij} \approx R_iR_j^T
$$
$$
t_{ij} \approx -R_iR_j^Tt_j + t_i
$$&lt;/p&gt;
&lt;p&gt;在当前的state-of-art工作中，图G中的边集E是使用一个单独的，辅助的配准算法进行两两配准进行初始化的。而全局场景的一致性是通过后续同步算法来实现的。 与之相反，&lt;!-- raw HTML omitted --&gt;文中提出了一种联合的方法，通过将双视角配准与刚体变换同步紧密耦合为一个完全可微分的组件，提供了一种端到端的，可学习的全局配准pipeline&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h2 id=&#34;31-pairwise-registration-of-point-clouds-双视角配准&#34;&gt;3.1 Pairwise registration of point clouds (双视角配准)&lt;/h2&gt;
&lt;p&gt;在本节中，我们提出了一个可微分，双视角配准算法，可以十分轻松地与端到端的多视角三维配准算法结合起来。用${ P,Q} := { S_i, S_j | i \ne j }$表示一对点云，其中$(P)&lt;em&gt;l =: p_l \in \mathbb{R}^3$, $(Q)&lt;em&gt;l =: q_l \in \mathbb{R}^3$分别代表在点云$P \in \mathbb{R}^{N_P \times 3}$和$Q \in \mathbb{R}^{N_Q \times 3}$中的每个点的坐标向量。而双视角配准的目标便是去恢复最优刚体变换矩阵$\hat {R&lt;/em&gt;{ij}}$和$\hat t&lt;/em&gt;{ij}$.
$$
\hat R_{ij}, \hat t_{ij} = \mathop {\arg \min }\limits_{{R_{ij}},{t_{ij}}} \sum\limits_{i = 1}^{{N_P}} {\left| {R_{ij}p_l + t_{ij} - \phi(p_l, Q)} \right|}^2     \tag 3
$$&lt;/p&gt;
&lt;p&gt;其中$\phi (p, Q)$是一个对应关系函数，用于将点$p$映射到其在点云$Q$中的对应点。等式（3）中的公式中便于求得一个可微的封闭形式的解，它受噪声分布的影响，接近于真值解。然而，最小二乘解不具有鲁棒性，因此Eq.3往往会在离群点概率高的情况下得到错误的刚体变换参数，在实践中，映射关系$\phi(p, Q)$ 的表现远非理想，错误的对应关系占据了主导地位。为了避免这种情况，等式3可以通过引入一个&lt;strong&gt;异方差的加权矩阵&lt;/strong&gt;来使得其对于离群点具有鲁棒性：
$$
\hat R_{ij}, \hat t_{ij} = \mathop {\arg \min }\limits_{{R_{ij}},{t_{ij}}} \sum\limits_{i = 1}^{{N_P}} w_l{\left| {R_{ij}p_l + t_{ij} - \phi(p_l, Q)} \right|}^2     \tag 4
$$&lt;/p&gt;
&lt;p&gt;其中$w_l := (\bold{w})_l$是假定的对应关系$\gamma &lt;em&gt;l \in \mathbb{R}^6$所对应的权重，通过一些权重函数$\mathbb{w} = \psi&lt;/em&gt;{init}(\Gamma)$计算而得，其中$\Gamma := { \gamma_l } := {P, { \phi(p_l, Q) }&lt;em&gt;l}$, 而$\psi&lt;/em&gt;{init} : \mathbb{R} ^{N_P \times 6} \to \mathbb{R}^{N_P}$. 如果假定的对应关系不是离群点，那么权重值$w_l$会接近于1，如果是离群点，那么便接近于0。Eq.4会在保留着可微分的封闭解的基础上得到正确的刚体变换参数。之后我们将这个封闭解表示为加权最小二乘刚体变换(weighted least squares transformation WLS trans).&lt;/p&gt;
&lt;p&gt;该可微分的封闭解可通过SVD分解求得。求解过程如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/LMPCR_fig2.png&#34; alt=&#34;image&#34;  /&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/LMPCR_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;32-differentiable-transformation-synchronization-可微分的刚体变换同步&#34;&gt;3.2 Differentiable transformation synchronization （可微分的刚体变换同步）&lt;/h2&gt;
&lt;p&gt;回到多视角配准的任务中来，我们重新考虑给定的初始点云集合$S$。 如果提前没有给定任何连接信息（即哪两帧点云之间有边（即配准关系）），图G可以通过指定$\left(\begin{array}{c}N_{\mathcal{S}} \ 2\end{array}\right)$组点云对（即两两之间都有边，类似于全连接），然后利用3.1中所提出的算法来估计其双视角刚体变换参数来进行初始化。全局刚体变换参数可以通过联合(即jointly， transformation synchronization)或独立的方式(将问题拆分为旋转同步(rotation synchronization)和平移同步(translation synchronization))进行估计。在此，我们选择后一种方法，它在谱关系下给出了一个可微的封闭形式的解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rotation synchronization&lt;/strong&gt;: 旋转同步的目标是通过基于其观测到的相对旋转矩阵${ \hat R_{ij} }$ 的比率来解决以下这个最小化问题，来得到全局的旋转矩阵${ R_i^*}$
$$
\mathbf{R}_{i}^{*}=\underset{\mathbf{R}_{\mathbf{i}} \in S O(3)}{\arg \min } \sum_{(i, j) \in \mathcal{E}} c_{i j}\left|\hat{\mathbf{R}}_{i j}-\mathbf{R}_{i} \mathbf{R}_{j}^{T}\right|_{F}^{2} \tag{5}
$$
其中，权重$c_{i j}:=\zeta_{\text {init }}(\boldsymbol{\Gamma})$代表相对刚体变换参数$\hat\bold{M}_{ij}$的置信度。在谱松弛条件下，Eq.5可以得到一个封闭解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Translation synchronization&lt;/strong&gt;
相似的，平移同步的目标也是恢复得到全局的平移向量${\mathbf{t}&lt;em&gt;i^*}$，其能够使得下述最小二乘问题得到最小值。
$$
\mathbf{t}&lt;/em&gt;{i}^{*}=\underset{\mathbf{t}&lt;em&gt;{\mathbf{i}}}{\arg \min } \sum&lt;/em&gt;{(i, j) \in \mathcal{E}} c_{i j}\left|\hat{\mathbf{R}}_{i j} \mathbf{t}_{i}+\hat{\mathbf{t}}_{i j}-\mathbf{t}_{j}\right|^{2} \tag{6}
$$
Eq.6也能得到一个可微分的封闭解。&lt;/p&gt;
&lt;h2 id=&#34;33-iterative-refinement-of-the-registration-配准迭代求精&#34;&gt;3.3 Iterative refinement of the registration 配准迭代求精&lt;/h2&gt;
&lt;p&gt;上述的公式(3.1和3.2)在迭代方案中实现起来十分容易，反过来可以将其视为一种IRLS算法。在第k+1次迭代前，我们先通过使用第k次迭代中得到的同步后的相对刚体变换参数$\mathbf{M}&lt;em&gt;{ij}^{*(k)} = \mathbf{M}&lt;/em&gt;{i}^{&lt;em&gt;(k)}{\mathbf{M}_{j}^{&lt;/em&gt;(k)}}^{-1}$来对点云Q进行预配准:$\mathbf{Q}^{(k+1)}:=\mathbf{M}&lt;em&gt;{i j}^{*(k)} \otimes \mathbf{Q}$ ，其中$\otimes$代表将刚体变换$\mathbf{M}&lt;/em&gt;{ij}^{*(k)}$应用于点云$\mathbf{Q}$上。此外，上一次迭代中的权重$\mathbf{w}^{(k)}$和残差$\mathbf{r}^{(k)}$会被当做边缘信息传入对应关系的权重函数(correspondence weighting function)，因此，$\psi_{\text{init}}$便可以被拓展为：
$$
\mathbf{w}^{(k+1)}:=\psi_{\text {iter }}\left(\mathbf{\Gamma}^{(k+1)}, \mathbf{w}^{(k)}, \mathbf{r}^{(k)}\right)   \tag{7}
$$
其中$\mathbf{\Gamma}^{(k+1)}:={\gamma_{l}^{(k+1)}}:=\left{\mathbf{P},\left{\phi\left(\mathbf{p}_{l}, \mathbf{Q}^{(k+1)}\right)\right}_{l}\right}$&lt;/p&gt;
&lt;p&gt;类似的，输入$\hat \mathbf{M}&lt;em&gt;{ij}^{(k)}$ （这里的输入的意思是指在一次迭代中完成了双视角配准，但尚未进行同步，此时会将上一步的结果作为输入传入同步层）和第k次迭代中同步后得到的刚体变换参数$\mathbf{M}&lt;/em&gt;{ij}^{*(k)}$的差异也可以作为估计相对刚体变换信息的置信度$c_{ij}^{(k+1)}$的额外线索。因此：$\zeta_{\text{init}}(\cdot)$可以被扩展为：
$$
c_{i j}^{(k+1)}:=\zeta_{\text {iter }}\left(\boldsymbol{\Gamma}^{(k+1)}, \hat{\mathbf{M}}_{i j}^{(k)}, \mathbf{M}_{i j}^{*(k)}\right)   \tag{8}
$$&lt;/p&gt;
&lt;h1 id=&#34;4network-architecture&#34;&gt;4.Network Architecture&lt;/h1&gt;
&lt;p&gt;我们将提出的多视角配准算法实现为了一个深度神经网络，在这一节中，我们首先描述用于近似$\phi(\cdot)$, $\psi_{\text{init}}(\cdot)$, $\psi_{\text{iter}}(\cdot)$, $\zeta_{\text{init}}(\cdot)$和$\zeta_{\text{iter}}(\cdot)$的网络架构。随后再把它们集成为一个完全可微的，端到端的，可训练算法。&lt;/p&gt;
&lt;h2 id=&#34;41-learned-correspondence-function&#34;&gt;4.1 Learned correspondence function&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Learned correspondence function&lt;/strong&gt;: 对对应关系函数$\phi(\cdot)$的近似拓展了一个最近提出的全卷积的三维特征描述子FCGF，和一个软分配层(soft assignment layer)。 FCGF在稀疏的张量上进行操作，通过一次pass，为稀疏点云中的每一个点计算得到一个32维的特征描述子。值得一提的是，我们对$\phi(\cdot)$的模拟可以用任何一个最近提出的基于学习的特征描述网络，如PPFNet等。之所以选择FCGF是由于其具有高准确性，而计算复杂度又很低。&lt;/p&gt;
&lt;p&gt;令$\mathbf{F_P},\mathbf{F_Q}$代表点云P，Q经过相同网络权重的FCGF(即shared)后得到的embedding特征。随后，每个点的对应关系${\phi(\cdot)}$可以通过在高维特征空间中的最近邻搜索(NN)而得到。 然而，这样一种hard assignment的选择规则不是可微分的。因此我们通过计算一个分类分布的概率向量$\mathbf{s}$，将最近邻选择(NN-selection)以概率的方式进行了re-form. 则点$\mathbf{p}$在点云$\mathbf{Q}$中的推测的对应关系被定义为：
$$
\phi(\mathbf{p}, \mathbf{Q}):=\mathbf{s}^{T} \mathbf{Q}, \quad(\mathbf{s})&lt;em&gt;{l}:=\frac{\exp \left(-d&lt;/em&gt;{l} / t\right)}{\sum_{l=1}^{N_{\mathbf{Q}}} \exp \left(-d_{l} / t\right)} \tag{9}
$$
其中$d_{l}:=\left|\mathbf{f}_{\mathbf{p}}-\left(\mathbf{F}_{\mathbf{Q}}\right)_{l}\right|_{2}$， $\mathbf{f_p}$是点$\mathbf{p}$的FCGF的embedding，而t表示温度参数(??? temperature parameter)， 当t趋近于0时$t \to 0$, $\phi(\mathbf{p},\mathbf{Q})$会收敛于确定的最近邻搜索。&lt;/p&gt;
&lt;p&gt;跟随FCGF中的设定，使用对应关系损失$\mathcal{L_c}$来监督$\phi(\cdot)$的学习。损失函数被定义为最严格的对比损失，并应用于FCGF的embedding 上。
$$
\begin{aligned}
\mathcal{L}&lt;em&gt;{c}=\frac{1}{N&lt;/em&gt;{\text {FCGF }}} &amp;amp; \sum_{(i, j) \in \mathcal{P}}\left{\left[d\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)-m_{p}\right]_{+}^{2} /|\mathcal{P}|\right.\&lt;br&gt;
&amp;amp;+0.5\left[m_{n}-\min _{k \in \mathcal{N}} d\left(\mathbf{f}_{i}, \mathbf{f}_{k}\right)\right]_{+}^{2} /\left|\mathcal{N}_{i}\right| \&lt;br&gt;
&amp;amp;\left.+0.5\left[m_{n}-\min _{k \in \mathcal{N}} d\left(\mathbf{f}_{j}, \mathbf{f}_{k}\right)\right]_{+}^{2} /\left|\mathcal{N}_{j}\right|\right}
\end{aligned}
$$
其中$\mathcal{P}$是FCGF的mini batch $N_{\text{FCGF}}$中所有positive pair的集合， 而$\mathcal{N}$是用于最难负例挖掘的随机采样得到的子集。$m_p = 0.1$和$m_n = 1.4$是positive pair 和 negative pair的margin.  关于这个part如果感到困惑的话，可以参考另一篇blog：&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/weixin_43977640/article/details/112642898&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;FCGF论文阅读笔记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$\phi(\cdot)$ 详细的网络结构和训练设置，参数等如下：&lt;/p&gt;
&lt;p&gt;网络架构： FCGF特征描述子是在sparse tensor上的。三维数据的稀疏张量表示为坐标$C$和相关特征$F$的集合：&lt;/p&gt;
&lt;p&gt;$$
C=\left[\begin{array}{cccc}
x_{1} &amp;amp; y_{1} &amp;amp; z_{1} &amp;amp; b_{1} \&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \&lt;br&gt;
x_{N} &amp;amp; y_{N} &amp;amp; z_{N} &amp;amp; b_{N}
\end{array}\right], F=\left[\begin{array}{c}
\mathbf{f}_{1}^{T} \&lt;br&gt;
\vdots \&lt;br&gt;
\mathbf{f}_{N}^{T}
\end{array}\right]
\tag{1}
$$&lt;/p&gt;
&lt;p&gt;其中$x_{i}, y_{i}, z_{i} \in \mathbb{Z}$是第i个三维坐标, $b_i$是batch的索引，为batch processing 提供了一个额外的维度，$\mathbf{f_i}$是与第i个点相关的特征。FCGF是使用Minkowski engine实现的，Minkowski engine是一个auto-differentiation库，它提供了对稀疏卷积的支持，并实现了所有基本的深度学习层。 我们采用了FCGF原本的全卷积网络设计，其结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/LMPCR_fig4.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;它使用了UNet架构，利用了skip connection 和 ResNet模块为每个点提取一个32维的特征描述子。为了得到独特的坐标$C$，我们使用在GPU上实现的体素网格下采样， voxel size $v := 2.5 \text{cm}$&lt;/p&gt;
&lt;h2 id=&#34;42-deep-pairwise-registration&#34;&gt;4.2 Deep pairwise registration&lt;/h2&gt;
&lt;p&gt;尽管FCGF的特征描述子的性能表现很好，还是会有一些假定的对应关系$\boldsymbol{\Gamma}^{\prime} \subset \boldsymbol{\Gamma}$ 将会是错误的。此外，inliers 和 outliers并不像噪音那样分布，而是呈现出某种规律性。因此我们的目标就是利用深度神经网络学习这种规律性。最近，人们提出了一些网络，用来表示用于过滤二维或三维特征对应关系的复杂权重函数。&lt;/p&gt;
&lt;p&gt;在这里，我们提出基于[46: Learning to find good correspondences] 用[68: Learning two-view correspondences and geometry using order-aware network]中的Order-aware block(顺序感知模块)，来扩展3D outlier 过滤网络[29: Robust point-wise correspondences for point cloud based deformation monitoring of natural scenes].  具体来说，我们创建了一个pairwise的配准模块 $f_{\theta}: \mathbb{R}^{N_{\mathbf{P}} \times 6} \mapsto \mathbb{R}^{N_{\mathbf{P}}}$， 其输入是假定的对应关系$\mathbf{\Gamma}$的坐标，输出是权重$\mathbf{w}:=\psi_{\text {init }}(\boldsymbol{\Gamma}):=\tanh \left(\operatorname{ReLU}\left(f_{\theta}(\mathbf{\Gamma})\right)\right)$。随后将权重$\mathbf{w}$与对应关系$\mathbf{\Gamma}$一起, 送入Eq.4的封闭解中去解得$\hat{\mathbf{R}}_{i j}$ 和 $\hat{\mathbf{t}}_{i j}$。 受到[53]和[68]的启发，我们在网络中添加了另一个配准模块$\psi_{\text{iter}}(\cdot)$，并将权重$\mathbf{w}$和点对之间的残差$\mathbf{r}$ append到原有的输入中去，服从于：$\mathbf{w}^{(k)} :=\psi_{i t e r}\left(\operatorname{cat}\left(\left[\boldsymbol{\Gamma}^{(k)}, \mathbf{w}^{(k-1)}, \mathbf{r}^{(k-1)}\right]\right)\right)$. 权重$\mathbf{w}^{(k)}$随后再次与初始对应关系$\mathbf{\Gamma}$一同被送入Eq.4的封闭解中，去获得refined 后的双视角刚体变换参数。 为了保证$f_{\theta}(\cdot)$的扰动不变性，在每个回归模块中均采用了一种类似PointNet的架构，其对每个单独的对应关系进行操作。因为每个branch只对单独的点对对应关系进行操作，局部的三维上下文信息便通过使用了symmetric context normalization[65] 和 order-aware filtering layers的中间层进行提取。 配准模块的详细架构在supplementary里有。 registration network的训练是通过对batch上的$N_{\mathrm{reg}}$个样例定义的配准误差进行监督的：
$$
\mathcal{L}_{\mathrm{reg}}=\alpha_{\mathrm{reg}} L_{\mathrm{class}}+\beta_{\mathrm{reg}} L_{\mathrm{trans}} \tag{10}
$$&lt;/p&gt;
&lt;p&gt;其中$\mathcal{L}&lt;em&gt;{\mathrm{class}}$是普通的二元交叉熵损失函数(&lt;!-- raw HTML omitted --&gt;此处存疑？？分明不是分类任务，为何在这里用了二元交叉熵损失函数&lt;!-- raw HTML omitted --&gt;) ， 而 $\mathcal{L}&lt;/em&gt;{\mathrm{trans}}$则定义如下：
$$
\mathcal{L}&lt;em&gt;{\text {trans }}=\frac{1}{N&lt;/em&gt;{\text {reg }}} \sum_{(i, j)} \frac{1}{N_{\mathbf{P}}} \sum_{l=1}^{N_{\mathbf{P}}}\left|\hat{\mathbf{M}}_{i j} \otimes \mathbf{p}_{l}-\mathbf{M}_{i j}^{\mathrm{GT}} \otimes \mathbf{p}_{l}\right|_{2} \tag{11}
$$&lt;/p&gt;
&lt;p&gt;$\mathcal{L}&lt;em&gt;{\mathrm{trans}}$用于惩罚与ground truth的偏移量。$\alpha&lt;/em&gt;{\mathrm{reg}}$ 和 $\beta_{\mathrm{reg}}$是用于控制contribution的权重。&lt;/p&gt;
&lt;h2 id=&#34;43-confidence-estimation-block&#34;&gt;4.3 Confidence estimation block&lt;/h2&gt;
&lt;p&gt;在我们估计得到的相对刚体变换参数$\hat \mathbf{M}&lt;em&gt;{ij}$之间，图$\mathcal{G}$中的边encode了这些估计间的置信度$c&lt;/em&gt;{ij}$。图中每个边encode的置信度都由两部分构成：(i) 两两刚体变换估计的局部置信度$c_{ij}^{\mathrm{local}}$. (ii) 从刚体变换同步(transformation synchronization)衍生得到的全局置信度$c_{ij}^{\mathrm{global}}$. 我们将$c_{ij}^{\mathrm{local}}$的估计任务视作一个分类任务，并认为其一些必要的信息包含在在registration block的倒数第二层的特征中。 令$\mathbf{X}_{i j}^{\text {conf }}=f_{\theta}^{(-2)}(\cdot)$ 表示registration block中的倒数第二层的输出。 我们提出了一个 *overlap pooling layer* 重叠池化层来通过加权平均池化提取全局特征$\mathbf{x}_{i j}^{\mathrm{conf}}$:
$$
\mathbf{x}_{i j}^{\mathrm{conf}}=\mathbf{w}_{i j}^{\mathrm{T}} \mathbf{X}_{i j}^{\mathrm{conf}} \tag{12}
$$&lt;/p&gt;
&lt;p&gt;得到的全局特征与inlier的比率$\delta_{i j}$(即权重比给定阈值高的点对对应关系的数量)进行concatenate。随后送入置信度估计网络，该网络由三个全连接层构成(129-64-32-1)，其后跟随一个ReLU激活函数。 所以local confidence局部置信度可以被表示为:
$$
c_{i j}^{\text {local }}:=\zeta_{\text {init }}(\boldsymbol{\Gamma}):=\operatorname{MLP}\left(\operatorname{cat}\left(\left[\mathbf{x}_{i j}^{\text {conf }}, \delta_{i j}\right]\right)\right) \tag{13}
$$&lt;/p&gt;
&lt;p&gt;confidence estimation block置信度估计模块的训练由置信度损失函数(confidence loss function) $\mathcal{L}&lt;em&gt;{\mathrm{conf}} = \frac{1}{N} \sum&lt;/em&gt;{(i, j)} \operatorname{BCE}\left(c_{i j}^{\text {local }}, c_{i j}^{\text {GT }}\right)$ 进行监督, 其中BCE指代二元交叉熵，ground truth confidence $c_{i j}^{\mathrm{GT}}$是在运行时，通过控制角度误差的阈值$\tau_{a} = \arccos \left(\frac{\operatorname{Tr}\left(\hat{\mathbf{R}}_{i j}^{T} \mathbf{R}_{i j}^{\mathrm{GT}}\right)-1}{2}\right)$来计算得到的。&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;在这里反思，原来作者所说的将其当做一个分类问题实质是上是一个二分类问题，$c_{ij}$的取值只有两个，要么是0， 要么是1。&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;$\zeta_{\text {init }}(\cdot)$函数计算得到了相对刚体变换参数的局部置信度。在另一方面，刚体变换同步层的输出(即全局刚体变换信息)提供了输入的相对刚体变换是如何与其他边之间保持全局一致的信息。 事实上，传统的同步算法[13,4,24]仅仅使用了其全局信息去在迭代中为每个边重新计算权重，原因在于他们并没有获得局部置信度的途径。相对刚体变换参数的全局置信度$c_{ij}^{\mathrm{global}}$可以被表示为柯西权重函数[33,4]：
$$
c_{i j}^{\text {global }}=1 /\left(1+r_{i j}^{*} / b\right) \tag{14}
$$
其中$r_{i j}^{*}=\left|\hat{\mathbf{M}}_{i j}-\mathbf{M}_{i}^{*} \mathbf{M}_{j}^{*^{T}}\right|_{F}$， 跟随[33,4]的思想，$b=1.482 \gamma \operatorname{med}\left(\left|\mathbf{r}^{*}-\operatorname{med}\left(\mathbf{r}^{*}\right)\right|\right)$, 而$\mathrm{med}(\cdot)$代表一个平均算子(median operator)。$\mathbf{r}^*$ 是残差$r_{ij}^*$的向量表达形式。&lt;!-- raw HTML omitted --&gt;此处存疑，$\gamma$ 是啥？是对应关系？那这个怎么能混到实值计算里来呢？&lt;!-- raw HTML omitted --&gt; 由于局部置信度与全局置信度提供了关于相对刚体变换参数的完整信息，我们使用其调和平均数将其结合为一个联合置信度$c_{ij}$:
$$
c_{i j}:=\zeta_{i t e r}\left(c_{i j}^{\text {local }}, c_{i j}^{\text {global }}\right):=\frac{\left(1+\beta^{2}\right) c_{i j}^{\text {global }} \cdot c_{i j}^{\text {local }}}{\beta^{2} c_{i j}^{\text {global }}+c_{i j}^{\text {local }}} \tag{15}
$$
其中$\beta$用于平衡局部置信度与全局置信度估计的贡献度，是在训练中学习得到的。&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;仍存疑，这里与上面$c_{ij}^{\mathrm{local}}$的0,1判断显然矛盾。那么$c_{ij}^{\mathrm{local}}$到底怎么计算的？ 难道是误差$\tau$与阈值的比值？？&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h2 id=&#34;44-end-to-end-multiview-3d-registration&#34;&gt;4.4 End-to-end multiview 3D registration&lt;/h2&gt;
&lt;p&gt;网络中每个独立的part 按照下图所示，连接成为一个端到端的多视角三维配准算法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/LMPCR_fig5.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;我们先对对每个子网络进行预训练(pretrain)， 然后再在3DMatch数据集上以端到端的方式，使用官方的训练测试集划分来微调整个模型&lt;!-- raw HTML omitted --&gt;。 在微调阶段我们使用$N_{\mathrm{FCGF}} = 4$来提取FCGF的特征，并从每个片段中随机采样2048个点的特征向量。这些feature被用于soft NN，来形成假定的$\left(\begin{array}{c}N_{\mathcal{S}} \ 2\end{array}\right)$组点对对应关系，随后被送入双视角配准网络。双视角配准的输出用于构建整个图，随后被输入刚体变换同步层。刚体变换参数迭代求精的过程共进行四次。我们使用联合多视角配准损失函数来对微调过程进行监督学习：
$$
\mathcal{L}=\mathcal{L}_{\mathrm{c}}+\mathcal{L}_{\mathrm{reg}}+\mathcal{L}_{\mathrm{conf}}+\mathcal{L}_{\mathrm{sync}} \tag{16}
$$&lt;/p&gt;
&lt;p&gt;其中刚体变换同步损失函数$\mathcal{L}&lt;em&gt;{\text {sync }}$记做：
$$
\mathcal{L}&lt;/em&gt;{\text {sync }}=\frac{1}{N} \sum_{(i, j)}\left(\left|\mathbf{R}_{i j}^{*}-\mathbf{R}_{i j}^{G T}\right|_{F}+\left|\mathbf{t}_{i j}^{*}-\mathbf{t}_{i j}^{G T}\right|_{2}\right) \tag{17}
$$&lt;/p&gt;
&lt;p&gt;我们对网络参数微调的过程共进行了2400次迭代，使用Adam优化器，学习率为$5 \times 10 ^{-6}$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PointConv论文阅读笔记</title>
        <link>https://codefmeister.github.io/p/pointconv%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/pointconv%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;本文发表于CVPR。 其主要内容正如标题，是提出了一个对点云进行卷积的Module，称为PointConv。由于点云的无序性和不规则性，因此应用卷积比较困难。&lt;/p&gt;
&lt;p&gt;其主要的思路是，将卷积核当做是一个由权值函数和密度函数组成的三维点的局部坐标的非线性函数。通过MLP学习权重函数，然后通过核密度估计得到密度函数。&lt;/p&gt;
&lt;p&gt;还有一个主要的贡献在于，使用了一种高效计算的方法，转换了公式的计算分时，使得PointConv的卷积操作变得memory efficient，从而加深网络的深度。&lt;/p&gt;
&lt;p&gt;This paper first published on CVPR. In this paper, author proposed a novel convolution operation which can be directly used on point cloud. As we all know, unlike image whose pixels are fixed, point cloud is irregular and unordered. So directly extend convolution operation into 3D pointcloud can be difficult.&lt;/p&gt;
&lt;p&gt;Author treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight function and density function. The wight function is learned by multi-layer perceptron networks, the density function is learned through kernel density estimation.&lt;/p&gt;
&lt;p&gt;Another main contribution of their work is that they reformulate the proposed convolution method to make it more memory efficient which significantly improves the depth of the network.&lt;/p&gt;
&lt;h1 id=&#34;about-pointconv&#34;&gt;About PointConv&lt;/h1&gt;
&lt;p&gt;作者提出了一种全新的，&lt;!-- raw HTML omitted --&gt;考虑到非均匀采样&lt;!-- raw HTML omitted --&gt;，对三维点云进行卷积的途径。 我们知道，卷积操作可以被看做是连续意义上的卷积函数的离散化近似。 在三维空间中，我们将卷积算子的权重函数视为是在以某个三维点为参考点的局部坐标系下的连续函数。该函数可以使用多层感知机进行模拟。这部分工作在之前已经有人做过，但是全部没有考虑到&lt;!-- raw HTML omitted --&gt;非均匀采样的问题&lt;!-- raw HTML omitted --&gt;。&lt;/p&gt;
&lt;p&gt;基于这个Motivation，我们进一步使用一个逆密度尺度来对MLP学到的权重函数进行重新计算，对应于原连续卷积函数的蒙特卡罗近似。我们将其称为PointConv。 PointConv以点云的位姿作为输入，通过MLP学习权重函数，然后再应用一个inverse density scale， 对权重函数进行重新计算，以弥补点云数据是由非均匀采样采得所带来的影响。&lt;/p&gt;
&lt;p&gt;PointConv最简单的实现是很消耗内存的，当输出通道的维度很大时，会给训练带来很大难度。 为了减少PointConv的内存消耗，作者通过改变加和顺序对公式进行了重写，大大提高了效率。&lt;/p&gt;
&lt;p&gt;此外，由于PointConv是卷积操作的完整近似，所以很容易就能从Conv推广到DeConv。DeConv层，也就是反卷积层，能够充分利用从粗糙层得到的信息，并将之传送到精细层去。而之前的很多卷积操作，不是full approxiamtion，所以就无法反卷积。对性能有很大影响。&lt;/p&gt;
&lt;p&gt;Author proposed a novel approach to apply convolution operation on 3D point clouds with consideration on non-uniform sampling. Note that convolution operation can be viewed as a discrete approximation of a continuous convolution operator. In 3D space, we can treat the weights of this convolution operator to be a continous function of the local 3D point coordinates with respect to a reference 3D point. The continuous function can be approximated by a multi-layer perceptron(MLP). This work has be done before, but all of them didn&amp;rsquo;t take non-uniform sampling into consideration.&lt;/p&gt;
&lt;p&gt;Based on this motivation, author used an inverse density scale to re-weight the continuous function learned by MLP, which correspond to the Monte Carlo approximation of the continuous convolution. PointConv involves taking the positions of point clouds as input and learning an MLP to approximate a weight function, as well as applying a inverse density scale on learned weights to compensate the non-uniform sampling.&lt;/p&gt;
&lt;p&gt;The naive implementation of PointConv is memory inefficient when the channel size of the output features is very large and hence hard to train and scale up to large networks.  In order to reduce the memory consumption of PointConv, author introduced an approach which is able to greatly increase the memory efficiency using a reformulation that changes the summation order.&lt;/p&gt;
&lt;p&gt;PointConv is full approximation of the convolution, it&amp;rsquo;s natural to extend PointConv to a PointDeconv, which an fully untilize the information in coarse layer and propagate to finer layers while most state of art algorithm cannot perform deconvolution because they are not the full approximation.&lt;/p&gt;
&lt;h1 id=&#34;convolution-on-3d-point-clouds&#34;&gt;Convolution on 3D Point Clouds&lt;/h1&gt;
&lt;p&gt;d维向量的卷积定义如下：
$$
(f*g)(x) = \iint\limits_{\tau  \in {\mathbb{R}^d}} {f(\tau )g(x + \tau )d\tau }
$$&lt;/p&gt;
&lt;p&gt;二维图像image的像素点是固定的。每个像素点会固定在某个体素网格。而点云不同，一个点云可以被表示为一个三维点集，其中每个点都包含着一个位置向量，同时还有其特征如颜色，表面向量等。不同于图像，点云有着更加灵活的形状，一个点的坐标(x,y,z)不会位于某个确定的网格上，而是可以取一个任意的连续值。因此，局部区域内不同点之间的相对位置是十分多变的。传统的光栅图像离散卷积滤波器不能够直接应用于3D点云上。&lt;/p&gt;
&lt;p&gt;为了得到一个能够应用于三维点云的卷积操作，首先回到连续的三维卷积：&lt;/p&gt;
&lt;p&gt;$$
Conv{(W,F)&lt;em&gt;{xyz}} = \iiint\limits&lt;/em&gt;{({\delta _x},{\delta _y},{\delta _z}) \in G} {W({\delta _x},{\delta _y},{\delta _z})F(x + {\delta _x},y + {\delta _y},z + {\delta _z})d{\delta _x}{\delta _y}{\delta _z}}
$$&lt;/p&gt;
&lt;p&gt;其中$F(x+\delta_x,y+\delta_y,z+\delta_z)$是以$(x,y,z)$为中心的局部区域$G$中点的特征。点云可以被看做是从连续空间$\mathbb{R}^3$中非均匀采样得到的。${\delta _x},{\delta _y},{\delta _z}$是局部区域内的任意位置的三维坐标。&lt;/p&gt;
&lt;p&gt;那么PointConv定义如下：&lt;/p&gt;
&lt;p&gt;$$
PointConv{(S,W,F)&lt;em&gt;{xyz}} = \sum\limits&lt;/em&gt;{({\delta _x},{\delta _y},{\delta _z}) \in G} {S({\delta _x},{\delta _y},{\delta _z})W({\delta _x},{\delta _y},{\delta _z})F(x + {\delta _x},y + {\delta _y},z + {\delta _z})} 
$$&lt;/p&gt;
&lt;p&gt;对连续函数的离散化近似从上可窥一见。 其中$S({\delta _x},{\delta _y},{\delta _z})$是点$({\delta _x},{\delta _y},{\delta _z})$处的逆密度inverse density。由于点云采样的非均匀性，$S({\delta _x},{\delta _y},{\delta _z})$是十分有必要的。直觉上讲，点云内不同局部区域内的点的个数是很不相同的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/PointConv_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;如上图，因为点$p_3,p_5,p_6,p_7,p_8,p_9,p_{10}$之间离得都很近，所以它们每一个的贡献度应该更小。&lt;!-- raw HTML omitted --&gt;inverse density&lt;!-- raw HTML omitted --&gt;的原因。就是采样密度越大，这里点的人均贡献度应该越小。&lt;/p&gt;
&lt;p&gt;PointConv的公式如上。&lt;/p&gt;
&lt;h1 id=&#34;pointconv的具体实现&#34;&gt;PointConv的具体实现&lt;/h1&gt;
&lt;p&gt;我们使用MLP来从三维坐标$({\delta _x},{\delta _y},{\delta _z})$中近似得到权重函数$W({\delta _x},{\delta _y},{\delta _z})$. 而逆密度函数$S({\delta _x},{\delta _y},{\delta _z})$ 则由一个核密度估计及紧随其后的一个由MLP实现的非线性转换实现。&lt;/p&gt;
&lt;p&gt;计算权重函数的MLP的参数是在所有点上共享的，以达到扰动不变性的目的。 而逆密度函数首先需要估计点云在某点处的密度(kernel density estimation(KDE))，然后将这个估计出来的密度送入一个MLP来进行一个一维的非线性变换。其目的是在于让网络决定，是否去使用密度估计。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/PointConv_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;首先得明确上图这个网络的输入和输出。&lt;/p&gt;
&lt;p&gt;输入是$F_{in}$，shape是$K \times C_{in}$， 也就是说要对某个点进行卷积，那么它的输入是k个邻居的feature（当前维的feature是$C_{in}$). 此外输入还有$P_{local}, Density$，一个是k近邻的局部坐标，${K \times 3}$, 一个是k近邻处的密度, $K \times 1$。&lt;/p&gt;
&lt;p&gt;输出是$F_{out}$，shape为$1 \times C_{out}$。&lt;/p&gt;
&lt;p&gt;$C_{in}, C_{out}$是输入输出feature的通道数。$k, c_{in}, c_{out}$分别指第$k$个邻接点，输入feature的第$c_{in}$个通道， 输出feature的第$c_{out}$个通道。&lt;/p&gt;
&lt;p&gt;因为每个点的权重函数不同，且其用于将$C_{in}$变换到$C_{out}$, 所以weight function的shape应该为$K \times (C_{in} \times C_{out})$， 即 $W \in \mathbb{R}^{K \times (C_{in} \times C_{out})}$. 那么第k个点处应用于第$c_{in}$个通道上的weight function应该是一个向量： $W(k,c_{in}) \in \mathbb{R}^{C_{out}}$&lt;/p&gt;
&lt;p&gt;所以有：
$$
{F_{out}} = \sum\limits_{k = 1}^K {\sum\limits_{{c_{in}} = 1}^{{C_{in}}} {S(k)W(k,{c_{in}}){F_{in}}(k,{c_{in}})} } 
$$&lt;/p&gt;
&lt;p&gt;上图中有tile的操作，相当于人为广播broadcast.&lt;/p&gt;
&lt;p&gt;PointConv通过网络学习得到权重函数W的离散化模拟，对于每个输入点，通过MLP使用其相对坐标来计算权重。下图(a)举了一个连续的权重函数用于卷积的例子，而点云是对连续输入（i.e.空间）的离散化处理（i.e.采样），我们通过图(b)类似的方式进行离散化卷积来抽象出局部特征（对于不同的采样会输出不同的W和S，所以其效果也不同）。 需要说明的是，对于光栅图像，其某个局部区域内的相对坐标都是固定的（体素化网格），所以经过PointConv时，对于整个图像，会输出相同的权重函数W和密度函数S，会变为传统的离散卷积。（所以作者还在后文专门进行了实验，将二维图像升维到三维空间中，通过PointConv，结果确实和二维一致。所以证明了PointConv确实是三维连续卷积的full approximation。&lt;/p&gt;
&lt;h1 id=&#34;pointdeconv&#34;&gt;PointDeConv&lt;/h1&gt;
&lt;h1 id=&#34;efficient-pointconv&#34;&gt;Efficient PointConv&lt;/h1&gt;
&lt;p&gt;这两个part搁置一下。因为我感觉论文的核心Part就是如何理解三维点云上的卷积操作。核心部分已经记录了。&lt;/p&gt;
&lt;p&gt;而且这篇论文我私以为存在一些问题，作者文中提到more details can be found in supplementary materials, 但是在哪都找不到他的补充材料，并且github上公开的源码并没有efficient realization of PointConv. 作者也不回答这些问题。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>torch中Dataset的构造与解读</title>
        <link>https://codefmeister.github.io/p/torch%E4%B8%ADdataset%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E8%AF%BB/</link>
        <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/torch%E4%B8%ADdataset%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E8%AF%BB/</guid>
        <description>&lt;h1 id=&#34;torch中dataset的构造与解读&#34;&gt;torch中Dataset的构造与解读&lt;/h1&gt;
&lt;h2 id=&#34;dataset的构造&#34;&gt;Dataset的构造&lt;/h2&gt;
&lt;p&gt;要自定义自己的数据集，首先需要继承&lt;code&gt;Dataset(torch.utils.data.Dataset)&lt;/code&gt;类.&lt;/p&gt;
&lt;p&gt;继承&lt;code&gt;Dataset&lt;/code&gt;类之后，必须重写三个方法:&lt;code&gt;__init__(), __getitem__(), __len__()&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;ModelNet40&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xxx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__getitem__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;__len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;解读&#34;&gt;解读&lt;/h2&gt;
&lt;p&gt;单看上面的构造结构与三个需要重写的方法可能会一头雾水。我们详细分析其作用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;__init__的作用
__init__的作用与所有构造函数都一样，初始化一个类的实例。定义类的实际属性，如点云数据集中的&lt;code&gt;unseen, guassian_noise&lt;/code&gt;等，是&lt;code&gt;True&lt;/code&gt;还是&lt;code&gt;False&lt;/code&gt;， 取出所有数据存储为成员变量等等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;__getitem__的作用
__getitem__的作用是，根据item的值取出数据。 item实际上就是索引值，会由Dataloader自动从0一直递增到__len__中取出的值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;__len__的作用
__len__的作用是，相当于返回整体数据data的shape[0]， 即给item的递增指定一个范围。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;例子&#34;&gt;例子&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;ModelNet40&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gaussian_noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unseen&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;factor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gaussian_noise&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gaussian_noise&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unseen&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unseen&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;squeeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;factor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;factor&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unseen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;######## simulate testing on first 20 categories while training on last 20 categories&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
                &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
                &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__getitem__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;pointcloud&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][:&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;          &lt;span class=&#34;c1&#34;&gt;# 核心代码，就是用item取出的数据&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gaussian_noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;pointcloud&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jitter_pointcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;anglex&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;factor&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;angley&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;factor&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;anglez&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;factor&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;cosx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anglex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cosy&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;angley&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cosz&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anglez&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;sinx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anglex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;siny&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;angley&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;sinz&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anglez&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;Rx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cosx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sinx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sinx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cosx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;Ry&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cosy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;siny&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;siny&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cosy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;Rz&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cosz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sinz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sinz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cosz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;R_ab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Rx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Ry&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Rz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;R_ba&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;R_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                   &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;translation_ba&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;R_ba&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;pointcloud1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pointcloud&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;rotation_ab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Rotation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_euler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;zyx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anglez&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;angley&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;anglex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;pointcloud2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointcloud1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;euler_ab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;asarray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anglez&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;angley&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;anglex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;euler_ba&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;euler_ab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[::&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;pointcloud1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;permutation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointcloud1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;pointcloud2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;permutation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointcloud2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointcloud1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pointcloud1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pointcloud2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;R_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; \
               &lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;R_ba&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation_ba&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; \
               &lt;span class=&#34;n&#34;&gt;euler_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;euler_ba&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__len__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;# 给item一个范围&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;进一步理解其执行逻辑&#34;&gt;进一步理解其执行逻辑&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dataset1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ModelNet40&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gaussian_noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dataloader&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_pointcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_pointcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Rotation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataloader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_pointcloud&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;首先需要说明的是，在ModelNet40中，getitem中会打印item的当前值。&lt;/p&gt;
&lt;p&gt;如果执行这段代码，在&lt;code&gt;shuffle=False&lt;/code&gt;的情况下，其结果为：&lt;/p&gt;
&lt;p&gt;item从0一直增加到&lt;code&gt;__len__&lt;/code&gt;返回的那个值-1， 也就是data的第一维（姑且称为batch维）。&lt;/p&gt;
&lt;p&gt;在getitem中取出的pointcloud的shape为(3, 1024)，只有两个axis.&lt;/p&gt;
&lt;p&gt;而最后输出的count，也就是main函数中整个for循环执行的次数，会是&lt;code&gt;__len__() / batch_size&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;比如len是9480，即self.data的shape为(9480, 2048, 3)，那么item就会从0一直增加到9479. 在batch_size为64的情况下，for循环一共执行（即count为） 9480/64 = 148.125， 那么最终会执行149次。 也就是说，每次for循环实质上调用了getitem方法64次，最后在第一维上stack，使之shape变为(64, 3, 1024).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>numpy.cross函数解析</title>
        <link>https://codefmeister.github.io/p/numpy.cross%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</link>
        <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/numpy.cross%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</guid>
        <description>&lt;h1 id=&#34;numpycross&#34;&gt;numpy.cross&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;numpy.cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;功能&#34;&gt;功能&lt;/h2&gt;
&lt;p&gt;Return the cross product of two (arrays of) vectors.
The cross product of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; in :math:&lt;code&gt;R^3&lt;/code&gt; is a vector perpendicular
to both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.  If &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are arrays of vectors, the vectors
are defined by the last axis of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; by default, and these axes
can have dimensions 2 or 3.  Where the dimension of either &lt;code&gt;a&lt;/code&gt; or &lt;code&gt;b&lt;/code&gt; is
2, the third component of the input vector is assumed to be zero and the
cross product calculated accordingly.  In cases where both input vectors
have dimension 2, the z-component of the cross product is returned.&lt;/p&gt;
&lt;p&gt;计算两个向量（向量数组）的叉乘。叉乘返回的数组既垂直于&lt;code&gt;a&lt;/code&gt;，又垂直于&lt;code&gt;b&lt;/code&gt;。 如果&lt;code&gt;a&lt;/code&gt;,&lt;code&gt;b&lt;/code&gt;是向量数组，则向量在最后一维定义。该维度可以为2，也可以为3. 为2的时候会自动将第三个分量视作0补充进去计算。&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;a : array_like
Components of the first vector(s).&lt;/li&gt;
&lt;li&gt;b : array_like
Components of the second vector(s).&lt;/li&gt;
&lt;li&gt;axisa : int, optional
Axis of &lt;code&gt;a&lt;/code&gt; that defines the vector(s).  By default, the last axis.&lt;/li&gt;
&lt;li&gt;axisb : int, optional
Axis of &lt;code&gt;b&lt;/code&gt; that defines the vector(s).  By default, the last axis.&lt;/li&gt;
&lt;li&gt;axisc : int, optional
Axis of &lt;code&gt;c&lt;/code&gt; containing the cross product vector(s).  Ignored if
both input vectors have dimension 2, as the return is scalar.
By default, the last axis.&lt;/li&gt;
&lt;li&gt;axis : int, optional
If defined, the axis of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; that defines the vector(s)
and cross product(s).  Overrides &lt;code&gt;axisa&lt;/code&gt;, &lt;code&gt;axisb&lt;/code&gt; and &lt;code&gt;axisc&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;axisa, axisb, axisc 分别指定两个输入和输出&lt;code&gt;c&lt;/code&gt;的向量所在的维度。而axis则可以覆盖前三个参数，为全局指定向量所在维度。&lt;/p&gt;
&lt;h2 id=&#34;returns&#34;&gt;Returns&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;c : ndarray
Vector cross product(s).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;raises&#34;&gt;Raises&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ValueError： 
When the dimension of the vector(s) in &lt;code&gt;a&lt;/code&gt; and/or &lt;code&gt;b&lt;/code&gt; does not
equal 2 or 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当向量所在axis的dimension不为2或者3时，raise ValueError.&lt;/p&gt;
&lt;h2 id=&#34;see-also相关函数&#34;&gt;See Also(相关函数)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;inner : Inner product 内积&lt;/li&gt;
&lt;li&gt;outer : Outer product 外积&lt;/li&gt;
&lt;li&gt;ix_ : Construct index arrays.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;p&gt;.. versionadded:: 1.9.0
Supports full broadcasting of the inputs.
支持广播。&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;product&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;One&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dimension&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;2.&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;Equivalently&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;Both&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vectors&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dimension&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;2.&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;Multiple&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;products&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Note&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;that&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;direction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;product&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;defined&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;by&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`right-hand rule`&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;orientation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`c`&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;can&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;changed&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`axisc`&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axisc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
        
&lt;span class=&#34;n&#34;&gt;Change&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;definition&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`x`&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`y`&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`axisa`&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;sb&#34;&gt;`axisb`&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;   &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;   &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;   &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axisa&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axisb&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;24&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;48&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;24&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;60&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;36&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;72&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;36&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title>np.clip作用解析</title>
        <link>https://codefmeister.github.io/p/np.clip%E4%BD%9C%E7%94%A8%E8%A7%A3%E6%9E%90/</link>
        <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/np.clip%E4%BD%9C%E7%94%A8%E8%A7%A3%E6%9E%90/</guid>
        <description>&lt;h1 id=&#34;npclip分析&#34;&gt;np.clip分析&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;np.clip(a, a_min, a_max, out=None, **kwarys)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;对数组a进行裁剪，小于a_min 的数用a_min代替， 大于a_max的数用a_max代替。&lt;/p&gt;
&lt;p&gt;需要注意的是，函数本身并不会对&lt;code&gt;a_min&lt;/code&gt;和&lt;code&gt;a_max&lt;/code&gt;之间的大小进行检查。&lt;/p&gt;
&lt;h2 id=&#34;参数&#34;&gt;参数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;a: 待裁剪的数组&lt;/li&gt;
&lt;li&gt;a_min: 下界&lt;/li&gt;
&lt;li&gt;a_max: 上界&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.arange(10)
&amp;gt;&amp;gt;&amp;gt; print(a)
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
&amp;gt;&amp;gt;&amp;gt; b = np.clip(a, 3, 7)
&amp;gt;&amp;gt;&amp;gt; print(b)
array([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;官方文档&#34;&gt;官方文档&lt;/h2&gt;
&lt;p&gt;Clip (limit) the values in an array.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Given an interval, values outside the interval are clipped to
the interval edges.  For example, if an interval of ``[0, 1]``
is specified, values smaller than 0 become 0, and values larger
than 1 become 1.
Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``.
No check is performed to ensure ``a_min &amp;lt; a_max``.
Parameters
----------
a : array_like
    Array containing elements to clip.
a_min : scalar or array_like or None
    Minimum value. If None, clipping is not performed on lower
    interval edge. Not more than one of `a_min` and `a_max` may be
    None.
a_max : scalar or array_like or None
    Maximum value. If None, clipping is not performed on upper
    interval edge. Not more than one of `a_min` and `a_max` may be
    None. If `a_min` or `a_max` are array_like, then the three
    arrays will be broadcasted to match their shapes.
out : ndarray, optional
    The results will be placed in this array. It may be the input
    array for in-place clipping.  `out` must be of the right shape
    to hold the output.  Its type is preserved.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs &amp;lt;ufuncs.kwargs&amp;gt;`.
    .. versionadded:: 1.17.0
Returns
-------
clipped_array : ndarray
    An array with the elements of `a`, but where values
    &amp;lt; `a_min` are replaced with `a_min`, and those &amp;gt; `a_max`
    with `a_max`.
See Also
--------
ufuncs-output-type
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>np.random中各函数一览</title>
        <link>https://codefmeister.github.io/p/np.random%E4%B8%AD%E5%90%84%E5%87%BD%E6%95%B0%E4%B8%80%E8%A7%88/</link>
        <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/np.random%E4%B8%AD%E5%90%84%E5%87%BD%E6%95%B0%E4%B8%80%E8%A7%88/</guid>
        <description>&lt;h1 id=&#34;nprandomuniform&#34;&gt;np.random.uniform()&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;`numpy.random.uniform(low=0.0, high=1.0, size = None)&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;返回一个均匀分布的采样结果。 左闭右开区间[low, high).&lt;/p&gt;
&lt;p&gt;返回数组的shape与size相同。&lt;/p&gt;
&lt;h2 id=&#34;参数&#34;&gt;参数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;low: 下界，默认为0.0&lt;/li&gt;
&lt;li&gt;high: 上界，默认为1.0&lt;/li&gt;
&lt;li&gt;size: 返回数组的shape， 默认为None，即返回一个单值&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;举例&#34;&gt;举例&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.random.uniform(1,2,(3,4))
&amp;gt;&amp;gt;&amp;gt; print(a)
array([[1.81297209, 1.79414559, 1.24677702, 1.44857774],
       [1.9171547 , 1.84473086, 1.33114168, 1.95953694],
       [1.66085822, 1.30895404, 1.1047299 , 1.6256421 ]])
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;nprandomrandn&#34;&gt;np.random.randn()&lt;/h1&gt;
&lt;h2 id=&#34;语法-1&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;numpy.random.randn(d0, d1, ..., dn)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;作用-1&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;返回一个&lt;code&gt;shape&lt;/code&gt;为&lt;code&gt;(d0, d1, ..., dn)&lt;/code&gt;的正态分布采样。分布的均值为0,方差为1. 如果没有提供参数，则返回单个值的采样。&lt;/p&gt;
&lt;h2 id=&#34;举例-1&#34;&gt;举例&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.random.randn(5,6)
&amp;gt;&amp;gt;&amp;gt; print(a)
array([[-0.47617937, -0.43465103,  0.14896871,  0.21132357,  0.2143598 ,
        -0.03354328],
       [-0.04106843, -1.7749601 , -0.21961397,  0.09636438, -0.96454273,
        -0.19864323],
       [ 2.23603561, -0.85890889,  0.33559106, -0.04761999, -1.67373302,
         1.02406518],
       [ 0.2637569 , -0.21446205,  0.55802706, -0.93956703,  0.71275597,
         1.13559443],
       [-0.17459087,  1.11798002, -2.17611829,  0.64592587,  0.87040789,
        -0.85487641]])
&amp;gt;&amp;gt;&amp;gt; print(a.shape)
(5, 6)
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>torch.detach()</title>
        <link>https://codefmeister.github.io/p/torch.detach/</link>
        <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/torch.detach/</guid>
        <description>&lt;h1 id=&#34;torchdetach&#34;&gt;torch.detach()&lt;/h1&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;Returns a new Tensor, detached from the current graph.&lt;/p&gt;
&lt;p&gt;返回一个新的Tensor， 从原有的图中剥离。&lt;/p&gt;
&lt;p&gt;The result will never require gradient.&lt;/p&gt;
&lt;p&gt;并且该Tensor不自动计算梯度。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>glob 函数作用分析</title>
        <link>https://codefmeister.github.io/p/glob-%E5%87%BD%E6%95%B0%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
        <pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/glob-%E5%87%BD%E6%95%B0%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
        <description>&lt;h1 id=&#34;python-中-glob&#34;&gt;Python 中 glob&lt;/h1&gt;
&lt;h2 id=&#34;作用简介&#34;&gt;作用简介&lt;/h2&gt;
&lt;p&gt;glob库中有两个函数:&lt;code&gt;glob.glob()&lt;/code&gt;， &lt;code&gt;glob.iglob()&lt;/code&gt;. 其作用是：遍历给定文件夹下所有符合条件的文件。&lt;/p&gt;
&lt;p&gt;常用的匹配符有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*&lt;/code&gt; 代表所有&lt;/li&gt;
&lt;li&gt;&lt;code&gt;?&lt;/code&gt; 代表满足单个字符&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[]&lt;/code&gt;代表满足list中指定的字符&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;globglobpath--recursivefalse&#34;&gt;&lt;code&gt;glob.glob(path, *, recursive=False)&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;在这里只介绍最基础的用法，用到的时候再深究。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;path_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DATA_DIR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;modelnet40_ply_hdf5_2048&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ply_data_&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%s&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;*.h5&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上述代码片段的意思是，寻找&amp;quot;DATADIR/modelnet40_ply_hdf5_2048/&amp;ldquo;下所有叫做&amp;quot;ply_data_train*.h5&amp;quot;的文件，并将其打包为一个**列表list**返回。&lt;/p&gt;
&lt;p&gt;如可能返回的是结果是：&lt;/p&gt;
&lt;p&gt;[&amp;ldquo;ply_data_train0.h5&amp;rdquo;, &amp;ldquo;ply_data_trainTx.h5&amp;rdquo;]&lt;/p&gt;
&lt;h2 id=&#34;globiglobpath--recursivefalse&#34;&gt;&lt;code&gt;glob.iglob(path, *, recursive=False)&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;作用与上面的函数一致。但是返回的不是&lt;code&gt;list&lt;/code&gt;,而是一个iterable的迭代器。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>torch.stack作用分析</title>
        <link>https://codefmeister.github.io/p/torch.stack%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
        <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/torch.stack%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
        <description>&lt;h1 id=&#34;torchstack作用分析&#34;&gt;torch.stack作用分析&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;torch.stack(tensors, dim=0, *, out=None)&lt;/code&gt;  &amp;ndash;&amp;gt; Tensor&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Concatenates a sequence of tensors along a new dimension.&lt;br&gt;
All tensor need to be of the same size&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;将一个序列的&lt;code&gt;tensor&lt;/code&gt;在新的一维上&lt;code&gt;concatenate&lt;/code&gt;起来，所有&lt;code&gt;tensor&lt;/code&gt;的shape需要相同。&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tensors(sequence of Tensors) &amp;ndash; sequence of tensors to concatenate&lt;/li&gt;
&lt;li&gt;dim(int) &amp;ndash; dimension to insert. Has to be between 0 and the number of dimensions of concanated tensors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keyword-arguments&#34;&gt;Keyword Arguments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;out(Tensor, optional) &amp;ndash; the output tensor.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>torch.nn.parameter.Parameter分析</title>
        <link>https://codefmeister.github.io/p/torch.nn.parameter.parameter%E5%88%86%E6%9E%90/</link>
        <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/torch.nn.parameter.parameter%E5%88%86%E6%9E%90/</guid>
        <description>&lt;h1 id=&#34;torchnnparameterparameter&#34;&gt;torch.nn.parameter.Parameter&lt;/h1&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;a kind of Tensor that is to be considered a module parameter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Parameter是一种可以作为模型参数的Tensor.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Parameters are &lt;code&gt;Tensor&lt;/code&gt; subclasses, that have a very special property when used with &lt;code&gt;Module&lt;/code&gt; S &amp;mdash;-when they&amp;rsquo;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in &lt;code&gt;parameters()&lt;/code&gt; iterator. Assigning a Tensor doesn&amp;rsquo;t have such effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Parameter是&lt;code&gt;Tensor&lt;/code&gt;的子类，同时拥有一种非常特殊的性质：当他们与Module S一起使用时，也就是说当它们作为&lt;code&gt;Module&lt;/code&gt;参数进行使用时，它们会自动添加到&lt;code&gt;Module&lt;/code&gt;的参数列表中，并且出现在&lt;code&gt;parameters()&lt;/code&gt;迭代器里。(这样就可以自动计算梯度等)&lt;/p&gt;
&lt;h2 id=&#34;构造参数&#34;&gt;构造参数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data&lt;/strong&gt;(Tensor)&amp;ndash; parameter tensor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;requires_grad&lt;/strong&gt;(bool, optional)&amp;ndash; if the parameter requires gradient. Default: True.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;定义一个网络&lt;code&gt;Module&lt;/code&gt;如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b_2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;那么，我们试着构造一个LayerNorm，来观察其参数：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layerNorm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layerNorm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;containing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requires_grad&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;containing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requires_grad&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到我们使用&lt;code&gt;nn.Parameter&lt;/code&gt;进行构造的参数，自动传入了&lt;code&gt;Module&lt;/code&gt;的参数列表。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DCP论文阅读笔记</title>
        <link>https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;dcp论文阅读笔记&#34;&gt;DCP论文阅读笔记&lt;/h1&gt;
&lt;h2 id=&#34;论文&#34;&gt;论文&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Deep Closest Point: Learning Representations for Point Cloud Registration&lt;br&gt;
Author: Wang, Yue; Solomon, Justin&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;main-attribution&#34;&gt;Main Attribution&lt;/h2&gt;
&lt;p&gt;基于ICP迭代最近点算法，提出基于深度学习的DCP算法。解决了ICP想要采用深度学习方法时遇到的一系列问题。&lt;/p&gt;
&lt;p&gt;我们先回顾一下ICP算法的基本步骤：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;each&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;iteration&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;find&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corresponding&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relations&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;points&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;between&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;two&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KNN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SVD&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;solve&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Rotation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Matrix&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Translation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;update&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cloud&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;概括起来就是： &lt;!-- raw HTML omitted --&gt;&lt;strong&gt;寻找最近点对关系，使用SVD求解刚体变换。&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;如此循环往复。&lt;/p&gt;
&lt;p&gt;结合论文，个人理解将ICP算法扩展到深度学习存在着以下的难点（可能存在各种问题，笔者深度学习的相关知识很薄弱）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先，点对关系如果是确定的话，沿着网络反向传播可能存在问题。&lt;/li&gt;
&lt;li&gt;SVD分解求解刚体变换，如何求梯度？(Confirmed by paper)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而文章克服了这些问题，主要有如下贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了能够解决传统ICP算法试图推广时存在的困难的一系列子网络架构。&lt;/li&gt;
&lt;li&gt;提出了能进行&lt;code&gt;pair-wise&lt;/code&gt;配准的网络架构&lt;/li&gt;
&lt;li&gt;评估了在采用不同设置的情况下的网络表现&lt;/li&gt;
&lt;li&gt;分析了是&lt;code&gt;global feature&lt;/code&gt;有用还是&lt;code&gt;local feature&lt;/code&gt;对配准更加有用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;网络架构&#34;&gt;网络架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;模型包含三个部分：&lt;br&gt;
(1) 一个将输入点云映射到高维空间&lt;code&gt;embedding&lt;/code&gt;的模块，具有扰动不变性（指DGCNN当点云输入时点的前后顺序发生变化，输出不会有任何改变） 或者 刚体变换不变性（指PointNet对于旋转平移具有不变的特性）。该模块的作用是&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;寻找两个输入点云之间的点的对应关系&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;. 可选的模块有&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;PointNet（Focus于全局特征）， DGCNN（结合局部特征和全局特征）&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;。&lt;/p&gt;
&lt;p&gt;(2) 一个基于注意力&lt;code&gt;attention&lt;/code&gt;的Pointer网络模块，用于预测两个点云之间的soft matching关系(&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;类似于一种基于概率的soft match，之所以soft是由于它并没有显式规定点$x_i$必须与哪个点$x_j$有对应关系，而是通过一个softmax得到的各点和某点$x_i$存在对应关系的概率乘以各点数据，得到一个类似于概率的对应点坐标&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;。 该模块采用的是&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;Transformer&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;(3) 一个&lt;strong&gt;可微&lt;/strong&gt;的&lt;code&gt;SVD&lt;/code&gt;分解层，用于输出刚体变换矩阵。&lt;/p&gt;
&lt;h2 id=&#34;问题阐述&#34;&gt;问题阐述&lt;/h2&gt;
&lt;p&gt;熟悉点云配准的同学应该知道，问题十分清晰。这里直接粘一下原文。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;值得一提的是，作者分析了一下ICP的算法步骤。和我们上面描述的一样。&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;就是用上次更新后的信息寻找最近关系，然后用寻找到的对应关系SVD求解得到$R,t$.&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt; 所以如果初始值一开始生成的是很差的&lt;code&gt;corresponding relation&lt;/code&gt;，那么一下就会陷入局部最优。&lt;/p&gt;
&lt;p&gt;而作者的思路就是：&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;使用学习的网络来得到特征，通过特征获得一个更好的对应关系$m(\cdot)$，用这个$m(\cdot)$去计算刚体变换信息。&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h2 id=&#34;代码分析与对应模块详解&#34;&gt;代码分析与对应模块详解&lt;/h2&gt;
&lt;p&gt;我们采用一种&lt;code&gt;Top-Down&lt;/code&gt;的视角来分析整个代码。先从整体入手，然后逐渐拆解模块进行分析。&lt;/p&gt;
&lt;h3 id=&#34;整体模块&#34;&gt;整体模块&lt;/h3&gt;
&lt;p&gt;DCP网络结构分为三个Part，从代码中就可以很清晰的看出来：第一个Module模块&lt;code&gt;emd_nn&lt;/code&gt;用于抽象特征，第二个Module模块&lt;code&gt;pointer&lt;/code&gt;用于match,第三个Module模块&lt;code&gt;head&lt;/code&gt;用于求解刚体变换矩阵，具体代码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;DCP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;# args 是一个存放各种参数的namespace&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DCP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embdims&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# 欲抽象到的特征维度，default为 512&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cycle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cycle&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;# ba的刚体变换关系是否重新进入网络计算&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_nn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;pointnet&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# emb_nn就是上文所说的第一个模块,若选择PointNet&lt;/span&gt;
            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_nn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PointNet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_nn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;dgcnn&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# 若选择DGCNN&lt;/span&gt;
            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_nn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DGCNN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Not implemented&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# 其他网络尚未实现&lt;/span&gt;

        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;identity&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;              &lt;span class=&#34;c1&#34;&gt;# 不使用Transformer, hard match&lt;/span&gt;
            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Identity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;transformer&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;# soft matching by tranformer&lt;/span&gt;
            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Transformer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Not implemented&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  
        
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;mlp&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;                      &lt;span class=&#34;c1&#34;&gt;# 直接用MLP预测输出矩阵&lt;/span&gt;
            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MLPHead&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svd&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;                    &lt;span class=&#34;c1&#34;&gt;# 使用可微的SVD分解层&lt;/span&gt;
            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SVDHead&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Not implemented&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_nn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_nn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# Module Part 1             (batch_size, emb_dims, num_points)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;src_embedding_p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding_p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pointer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Module Part 2&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embedding_p&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding_p&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;rotation_ab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Module Part 3&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cycle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;rotation_ba&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation_ba&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        
        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;rotation_ba&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;translation_ba&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rotation_ba&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unsqueeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;squeeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation_ab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation_ab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation_ba&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;translation_ba&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;用于抽象feature的module1emb_nn&#34;&gt;用于抽象feature的Module1：emb_nn&lt;/h3&gt;
&lt;p&gt;考虑emb_nn，我们有两个选择： 其一是PointNet， 其二是&lt;a class=&#34;link&#34; href=&#34;https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DGCNN&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;PointNet抽象的特征是&lt;code&gt;global feature&lt;/code&gt;， 而DGCNN结合了&lt;code&gt;local feature&lt;/code&gt;和&lt;code&gt;global feature&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;我们希望得到的是对&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;每一个点抽象而得的特征&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;（即每一个点都有其embedding)，并利用两个点云之间点的&lt;code&gt;embedding&lt;/code&gt;来生成映射关系（即Match关系）. 所以我们要得到的是&lt;code&gt;per-point feature&lt;/code&gt;而不是&lt;code&gt;one feature per cloud&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;出于上述原因，我们在最后一层的聚合函数&lt;code&gt;aggregation function&lt;/code&gt;之前生成每个点的&lt;code&gt;representation&lt;/code&gt;。
$$
F_X = {x_1^L,x_2^L, &amp;hellip;, x_i^L,&amp;hellip;,x_N^L}
$$
$$
F_Y = {y_1^L, y_2^L, &amp;hellip;, y_i^L, &amp;hellip;, y_N^L}
$$
上标L代表第L层的输出（假定共有L层）。&lt;/p&gt;
&lt;h4 id=&#34;pointnet&#34;&gt;PointNet&lt;/h4&gt;
&lt;p&gt;$x_i^l$是第$i$个点在第$l$层后的&lt;code&gt;embedding&lt;/code&gt;，而$h_{\theta}^l$是第$l$层的非线性映射函数。PointNet的&lt;code&gt;forward mechanism&lt;/code&gt;可以用如下公式给出：
$$
x_i^l = h_{\theta}^l(x_i^{l-1})
$$
作者&lt;code&gt;@WangYue&lt;/code&gt;在github上公布的代码，使用的&lt;code&gt;PointNet&lt;/code&gt;的网络架构如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;PointNet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PointNet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv5&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn5&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从上述代码中，可以看出，&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;作者使用的PointNet并没有&lt;code&gt;input-transform&lt;/code&gt;和&lt;code&gt;feature-transform&lt;/code&gt;这两个Module&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;，相当于只应用MLP不断对输入点云进行抽象，直到高维空间。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig4.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;em&gt;存疑&lt;/em&gt;：为什么不加&lt;code&gt;Transform-Net&lt;/code&gt;？ 如果加上效果训练效果如何? 没有cat，cat之后效果如何？&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解答&lt;/strong&gt;：记于2020-12-28日：这个问题太傻了。我们这个网络的目的就是学习出刚体变换矩阵。3x3的Transform Net相当于是硬学。而我们是通过其巧妙设计来优化模式，求解刚体变换矩阵。&lt;/p&gt;
&lt;h4 id=&#34;dgcnn&#34;&gt;DGCNN&lt;/h4&gt;
&lt;p&gt;DGCNN是作者&lt;code&gt;@WangYue&lt;/code&gt;提出的一种网络架构，其特点是&lt;code&gt;EdgeConv&lt;/code&gt;。可以结合全局特征与局部特征。
$$
x_i^l = f({{} h_{\theta}^l(x_i^{l-1},x_j^{l-1}); \forall j \in N_i {}})
$$
$f$是每一层后的聚合函数。$N_i$指的是和点$x_i$存在KNN关系的点的集合。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;get_graph_feature&lt;/code&gt;是返回&lt;code&gt;egde-feature&lt;/code&gt;的函数。这并不是我们关注的重点，代码简要粘贴一下。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;knn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;xx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pairwise_distance&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pairwise_distance&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (batch_size, num_points, k)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# x = x.squeeze()&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;knn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (batch_size, num_points, k)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;idx_base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx_base&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (batch_size, num_points, num_dims)  -&amp;gt; (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;repeat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;permute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;而网络中使用的DGCNN代码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;DGCNN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DGCNN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv5&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn5&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;            &lt;span class=&#34;c1&#34;&gt;# (batch_size, emb_dims, num_points)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以明显发现与原DGCNN不同的地方是: &lt;!-- raw HTML omitted --&gt;作者这里每次forward前传时，并没有再对抽象出来的feature寻找knn进行进一步抽象。而是单纯的不断经过MLP。&lt;!-- raw HTML omitted --&gt; 对比一下该部分原代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# (batch_size, 3, num_points) --&amp;gt; (batch_size, 3*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 3*2, num_points, k) --&amp;gt; (batch_size, 64, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points, k) --&amp;gt; (batch_size, 64, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points) --&amp;gt; (batch_size, 64*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64*2, num_points, k) --&amp;gt; (batch_size, 64, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points, k) --&amp;gt; (batch_size, 64, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points) --&amp;gt; (batch_size, 64*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64*2, num_points, k) --&amp;gt; (batch_size, 128, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# (batch_size, 128, num_points, k) --&amp;gt; (batch_size, 128, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# (batch_size, 128, num_points) --&amp;gt; (batch_size, 128*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 128*2, num_points, k) --&amp;gt; (batch_size, 256, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 256, num_points, k) --&amp;gt; (batch_size, 256, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64+64+128+256, num_points)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;差别十分明显。相当于网络结构中红框的部分消失了：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig5.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;此处同样存疑&lt;!-- raw HTML omitted --&gt;，作者在论文里未提及此细节。&lt;/p&gt;
&lt;h3 id=&#34;用于match寻找点对关系的module2&#34;&gt;用于Match（寻找点对关系）的Module2&lt;/h3&gt;
&lt;h4 id=&#34;基于attention机制的transformer&#34;&gt;基于Attention机制的Transformer&lt;/h4&gt;
&lt;p&gt;使用Attention机制的初衷在于：我们想让配准变得更加&lt;code&gt;task specify&lt;/code&gt;。也就是说，不再独立地关注两个输入点云$X$,$Y$的&lt;code&gt;embedding feature&lt;/code&gt;，而是关注$X$,$Y$之间的一些联合特性。于是乎，自然而然的想到了&lt;code&gt;Attention&lt;/code&gt;机制。基于&lt;code&gt;attention&lt;/code&gt;，设计一个可以捕捉&lt;code&gt;self-attention&lt;/code&gt;和&lt;code&gt;conditional attention&lt;/code&gt;的模块，用于学习$X,Y$点云之间的某些联合信息。&lt;/p&gt;
&lt;p&gt;我们将由上一个Module对两个点云各自独立生成的embedding特征$F_X,F_Y$作为输入，那么就有：
$$
\Phi_X = F_X + \phi(F_X,F_Y)
$$
$$
\Phi_Y = F_Y + \phi(F_Y,F_X)
$$
其中，$\phi$是Transformer学习得到的映射函数:$\phi: R^{N \times P} \times R^{N \times P} \to R^{N \times P}$.&lt;/p&gt;
&lt;p&gt;我们将$\phi$当做一个残差项，&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;基于$F_X,F_Y$的输入顺序&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;，为$F_X,F_Y$提供一个附加的改变项。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice we treat $\phi$ as a residual term, providing an additive change to $F_X$ and $F_Y$ depending on the order of its input.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;之所以采取将$F_X \to \Phi_X$的&lt;strong&gt;Motivation&lt;/strong&gt;：以一种适应$Y$中点的组织结构（个人理解即输入顺序）的方式改变$X$的&lt;code&gt;Feature embedding&lt;/code&gt;。对$F_Y \to \Phi_Y$，动机相同。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The idea here is that the map $F_X \to \Phi_X$ modifies the features associated to the points in X in a fashion that is knowledgeable about the structure of $Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;选择Transformer提供的非对称函数作为$\phi$。Transformer由一些堆叠的&lt;code&gt;encoder-decoder&lt;/code&gt;组成，是一种解决Seq2Seq问题的经典架构。关于Transformer的更多信息，移步我的另一篇博客&lt;a class=&#34;link&#34; href=&#34;https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;《图解Transformer（译）》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此Module中，&lt;code&gt;encoder&lt;/code&gt;接收$F_X$并通过&lt;code&gt;self-attention layer&lt;/code&gt;和&lt;code&gt;MLP&lt;/code&gt;把它编码到其&lt;code&gt;embedding space&lt;/code&gt;，而&lt;code&gt;decoder&lt;/code&gt;有两个部分组成，第一个部分接收另一个集合$F_Y$, 然后像&lt;code&gt;encoder&lt;/code&gt;一样将之编码到&lt;code&gt;embedding space&lt;/code&gt;。另一个部分使用&lt;code&gt;co-attention&lt;/code&gt;对两个已经映射到&lt;code&gt;embedding space&lt;/code&gt;的点云进行处理。 所以输出$\Phi_Y$,$\Phi_Y$既含有$F_X$的信息，又含有$F_Y$的信息。&lt;/p&gt;
&lt;p&gt;这里的&lt;strong&gt;Motivation&lt;/strong&gt;是：将两个点云之间的匹配关系问题(&lt;code&gt;match problem&lt;/code&gt;)类比为Sq2Sq问题。（点云是在两个输入的点的序列中寻找对应关系，而Sq2Sq问题是在输入句子中寻找单词之间的联系关系）。&lt;/p&gt;
&lt;p&gt;为了避免不可微分的&lt;code&gt;hard assignment&lt;/code&gt;，我们使用概率角度的一种方式来生成&lt;code&gt;soft map&lt;/code&gt;，将一个点云映射到另一个点云。所以，每一个$x_i \in X$都被赋予了一个概率向量：
$$
m(x_i,Y) = softmax(\Phi_y \Phi_{x_i}^T)
$$
在这里，$\Phi_Y \in R^{N \times P}$代表Y经过&lt;code&gt;Attention Module&lt;/code&gt;后生成的&lt;code&gt;embedding&lt;/code&gt;。而$\Phi_{x_i}$代表矩阵$\Phi_X$的第$i$行。 所以可以将$m(x_i, Y)$ 看做一个将每个$x_i$映射到$Y$中元素的&lt;code&gt;soft pointer&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;下面我们关注文中Transformer的实现。其架构为：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig6.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Transformer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Transformer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_blocks&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ff_dims&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ff_dims&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# Feed_forward Dims&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_heads&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_heads&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# Multihead Attention的头数&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;copy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;deepcopy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MultiHeadedAttention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_heads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;ff&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PositionwiseFeedForward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ff_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;EncoderDecoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;Decoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DecoderLayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
        
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# batch_size, emb_dims, num_points&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# batch_size, num_points, emb_dims&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt;
                
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上述代码是Transformer的实现。看起来有点绕，我们进一步关注其forward函数.&lt;/p&gt;
&lt;p&gt;流入Transformer的data： &lt;code&gt;src,tgt: (batch_size, emb_dims, num_points)&lt;/code&gt;&lt;br&gt;
经过transopose.contiguous:  &lt;code&gt;src,tgt: (batch_size, num_points, emb_dims)&lt;/code&gt;&lt;br&gt;
随后将&lt;code&gt;src, tgt&lt;/code&gt;传入&lt;code&gt;self.model&lt;/code&gt;，得到了&lt;code&gt;tgt_embedding, src_embedding&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;关注&lt;code&gt;self.model&lt;/code&gt;，在&lt;code&gt;__init__&lt;/code&gt;中定义了&lt;code&gt;self.model&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    self.model = EncoderDecoder(Encoder(EncoderLayer(self.emb_dims, c(attn), c(ff), self.dropout), self.N),
                                Decoder(DecoderLayer(self.emb_dims, c(attn), c(attn), c(ff), self.dropout),self.N),
                                nn.Sequential(),
                                nn.Sequential(),
                                nn.Sequential())
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;self.model&lt;/code&gt; 整体是一个&lt;code&gt;EncoderDecoder&lt;/code&gt;类。其传入的参数有一个&lt;code&gt;Encoder&lt;/code&gt;，一个&lt;code&gt;Decoder&lt;/code&gt;，三个&lt;code&gt;Sequential&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;Encoder&lt;/code&gt;传入的参数有两个，第一个是&lt;code&gt;EncoderLayer&lt;/code&gt;，第二个是&lt;code&gt;self.N&lt;/code&gt;。作为第一个参数的&lt;code&gt;EncoderLayer&lt;/code&gt;传入了四个参数，分别是&lt;code&gt;self.emb_dims&lt;/code&gt;, &lt;code&gt;c(attn)&lt;/code&gt;, &lt;code&gt;c(ff)&lt;/code&gt;, &lt;code&gt;self.dropout&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Decoder&lt;/code&gt;传入的参数也是两个，第一个是&lt;code&gt;DecoderLayer&lt;/code&gt;，第二个是&lt;code&gt;self.N&lt;/code&gt;。作为第一个参数的&lt;code&gt;DecoderLayer&lt;/code&gt;传入了五个参数，分别是&lt;code&gt;self.emb_dims&lt;/code&gt;, &lt;code&gt;c(attn)&lt;/code&gt;, &lt;code&gt;c(attn)&lt;/code&gt;, &lt;code&gt;c(ff)&lt;/code&gt;, &lt;code&gt;self.dropout&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;这里的&lt;code&gt;c&lt;/code&gt;是&lt;code&gt;copy.deepcopy()&lt;/code&gt;，即深拷贝。完全复制一个新的对象，所以这些网络之间参数并不共享。&lt;/p&gt;
&lt;p&gt;首先来关注EncoderDecoder类：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;EncoderDecoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;decoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;generator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;EncoderDecoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encoder&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoder&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decoder&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;decoder&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_embed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embed&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt_embed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embed&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generator&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;generator&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# Take in and process masked src and target sequences&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_embed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt_embed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从上述代码可以看出，EncoderDecoder类构造时传入的五个参数分别为： &lt;code&gt;encoder, decoder, src_embed, tgt_embed, generator&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;其前传机制&lt;code&gt;forward&lt;/code&gt;是&lt;code&gt;self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)&lt;/code&gt;。即先将&lt;code&gt;src,src_mask&lt;/code&gt;进行&lt;code&gt;encode&lt;/code&gt;,将编码后的结果同&lt;code&gt;src_mask, tgt, tgt_mask&lt;/code&gt;一同&lt;code&gt;decode&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;encode&lt;/code&gt;函数，是这样定义的: &lt;code&gt;self.encoder(self.src_embed(src), src_mask)&lt;/code&gt;, 即先将&lt;code&gt;src&lt;/code&gt;通过&lt;code&gt;src_embed&lt;/code&gt;网络，然后根据其&lt;code&gt;mask&lt;/code&gt;再通过&lt;code&gt;encoder&lt;/code&gt;网络。&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;decode&lt;/code&gt;函数，是：&lt;code&gt;self.generator(self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask))&lt;/code&gt;, 也就是说, &lt;code&gt;tgt&lt;/code&gt;先经过&lt;code&gt;tgt_embed&lt;/code&gt;网络，然后随&lt;code&gt;memory, src_mask, tgt_mask&lt;/code&gt;一同传入&lt;code&gt;decoder&lt;/code&gt;网络，&lt;code&gt;decoder&lt;/code&gt;网络的输出再流入&lt;code&gt;generator&lt;/code&gt;网络。&lt;/p&gt;
&lt;p&gt;所以我自己梳理了一下整个EncoderDecoder大概结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig7.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;进一步关注&lt;code&gt;Encoder&lt;/code&gt;与&lt;code&gt;Decoder&lt;/code&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ModuleList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;copy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;deepcopy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layers&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从上述代码，可以看出： &lt;code&gt;Encoder&lt;/code&gt;在构造时需要传入两个参数，一个为&lt;code&gt;layer&lt;/code&gt;, 一个为&lt;code&gt;N&lt;/code&gt;。而在构造函数中，通过调用&lt;code&gt;clones&lt;/code&gt;方法将传入的&lt;code&gt;layer&lt;/code&gt;深复制(&lt;code&gt;deepcopy&lt;/code&gt;)了N次，并作为一个&lt;code&gt;ModuleList&lt;/code&gt;存储在&lt;code&gt;self.layers&lt;/code&gt;成员变量中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;clones&lt;/code&gt;方法的执行效果可从下例中窥见：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;net&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;net&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;net_clones&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;net_clones&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;net_clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其执行结果是：将&lt;code&gt;net&lt;/code&gt;复制了3次，装在一个&lt;code&gt;ModuleList&lt;/code&gt;中返回。并且值得一提的是，这里是&lt;strong&gt;copy.deepcopy()&lt;/strong&gt;，深复制，所以参数之间不共享。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig8.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;而LayerNorm定义如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;# 最后一维的均值&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;           &lt;span class=&#34;c1&#34;&gt;# 最后一维的标准差&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b_2&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# 对最后一维进行Norm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可见LayerNorm的作用是对输入的数据&lt;code&gt;x&lt;/code&gt;，对其最后一维进行归一化操作。&lt;/p&gt;
&lt;p&gt;而Encoder整个的前传机制为：对于构造时生成的ModuleList,依次将&lt;code&gt;x&lt;/code&gt;通过&lt;code&gt;ModuleList&lt;/code&gt;中的每个网络&lt;code&gt;layer&lt;/code&gt;，即&lt;code&gt;x = layer(x, mask)&lt;/code&gt;, 然后再将输出通过LayerNorm对最后一维进行归一化操作返回。 Encoder的网络结构图如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig9.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;进一步关注，&lt;code&gt;Encoder&lt;/code&gt;在&lt;code&gt;EncoderDecoder&lt;/code&gt;构造时，传入的layer参数为：&lt;code&gt;EncoderLayer(self.emb_dims, c(attn), c(ff), self.dropout)&lt;/code&gt;。 解读&lt;code&gt;EncoderLayer&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SublayerConnection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;](&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;](&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feedforward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;EncoderLayer&lt;/code&gt;在构造时需要传入的参数有：&lt;code&gt;size, self_attn, feed_forward, dropout&lt;/code&gt;。 其中&lt;code&gt;self_attn, feed_forward&lt;/code&gt;两个网络，以及&lt;code&gt;size&lt;/code&gt;作为成员变量存储。而另一个成员变量&lt;code&gt;sublayer&lt;/code&gt;通过&lt;code&gt;clones&lt;/code&gt;方法将&lt;code&gt;SublayerConnection(size, dropout)&lt;/code&gt;复制两遍，存着一个ModuleList.&lt;/p&gt;
&lt;p&gt;观察&lt;code&gt;SublayerConnection&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;SublayerConnection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SublayerConnection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;SublayerConnection&lt;/code&gt;在构造时，只传入了&lt;code&gt;size, dropout&lt;/code&gt;两个参数，用于构造LayerNorm。而前传&lt;code&gt;forward&lt;/code&gt;的时候，返回的是&lt;code&gt;x + sublayer(self.norm(x))&lt;/code&gt;， 即将&lt;code&gt;x&lt;/code&gt;通过了&lt;code&gt;LayerNorm&lt;/code&gt;后，再通过作为参数传入的网络&lt;code&gt;sublayer&lt;/code&gt;，最后与&lt;code&gt;x&lt;/code&gt;相加，返回。&lt;/p&gt;
&lt;p&gt;搞清楚&lt;code&gt;SublayerConnection&lt;/code&gt;的机制后，我们回看&lt;code&gt;EncoderLayer&lt;/code&gt;的&lt;code&gt;forward&lt;/code&gt;机制：&lt;code&gt;x&lt;/code&gt;先通过一个传入网络为&lt;code&gt;attn&lt;/code&gt;的&lt;code&gt;sublayer&lt;/code&gt;，然后再通过一个传入网络为&lt;code&gt;feedforward&lt;/code&gt;的&lt;code&gt;sublayer&lt;/code&gt;. &lt;code&gt;EncoderLayer&lt;/code&gt;网络结构如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig10.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;接下来关注&lt;code&gt;attn&lt;/code&gt;，即&lt;code&gt;MultiHeadedAttention&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;math&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# (nbatches, h, num_points, num_points)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;masked_fill&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;p_attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;softmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (nbatches, h, num_points, num_points)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p_attn&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;MultiHeadedAttention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# h: number of heads ;  d_model: dims of model(emb_dims)&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MultiHeadedAttention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# we assume d_v always equals d_k&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;# d_k 是每个head的dim&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linears&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# Same mask applied to all h heads&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unsqueeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;nbatches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;c1&#34;&gt;# 1) Do all the linear projections in batch from d_model =&amp;gt; h x d_k&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nbatches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linears&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))]&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# (nbatches, h, num_points, d_k)&lt;/span&gt;

        &lt;span class=&#34;c1&#34;&gt;# 2) Apply attention on all the projected vectors in batch.&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;c1&#34;&gt;# 3) &amp;#34;Concat&amp;#34; using a view and apply a final linear.&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nbatches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linears&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;](&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;MultiHeadAttention其构造函数有两个参数：&lt;code&gt;h, d_model&lt;/code&gt;. 其中&lt;code&gt;h&lt;/code&gt;是&lt;code&gt;head&lt;/code&gt;的个数，而&lt;code&gt;d_model&lt;/code&gt;实际上就是&lt;code&gt;emb_dims&lt;/code&gt;。我们总是规定&lt;code&gt;emb_dims&lt;/code&gt;是可以整除&lt;code&gt;h&lt;/code&gt;的，否则每个&lt;code&gt;attention&lt;/code&gt;的维度不是整数。&lt;code&gt;d_k = d_model // h&lt;/code&gt;即是每个&lt;code&gt;head&lt;/code&gt;的维度。同时在构造时，在self.linears中存储了一个&lt;code&gt;ModuleList&lt;/code&gt;,&lt;code&gt;ModuleList&lt;/code&gt;中有四个&lt;code&gt;Linear&lt;/code&gt;线性映射&lt;code&gt;d_model --&amp;gt; d_model&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;MultiHeadAttention的前传机制：&lt;/p&gt;
&lt;p&gt;(1) 通过线性映射&lt;code&gt;linear projection&lt;/code&gt;，生成&lt;code&gt;query, key, value&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;用&lt;code&gt;zip&lt;/code&gt;方法将三个线性映射绑定到&lt;code&gt;query, key, value&lt;/code&gt;上，相当于指定了其生成的矩阵。（这里的&lt;code&gt;query, key, value&lt;/code&gt;只是用于生成&lt;code&gt;query, key, value&lt;/code&gt;的原始数据，事实上都是&lt;code&gt;x&lt;/code&gt;）。 将&lt;code&gt;query, key, value&lt;/code&gt;分别通过对应的&lt;code&gt;Linear Projection&lt;/code&gt;投影生成真正的&lt;code&gt;query, key, value&lt;/code&gt;。 输入的&lt;code&gt;x&lt;/code&gt;的&lt;code&gt;shape&lt;/code&gt;是&lt;code&gt;nbatches, num_points, emb_dims&lt;/code&gt; (&lt;code&gt;emb_dim&lt;/code&gt; 即 &lt;code&gt;d_model&lt;/code&gt;). 经过对应的&lt;code&gt;Linear Projection&lt;/code&gt;后，生成的&lt;code&gt;shape&lt;/code&gt;是&lt;code&gt;nbatches, num_points, emb_dims&lt;/code&gt;, 通过&lt;code&gt;view()&lt;/code&gt;变为&lt;code&gt;nbatches, num_points, self.h, self.d_k&lt;/code&gt;。 随后又进行了&lt;code&gt;transpose(1,2).contiguous()&lt;/code&gt;, 那么最后生成的&lt;code&gt;query, key, value&lt;/code&gt;的&lt;code&gt;shape&lt;/code&gt;是&lt;code&gt;nbatches, self.h, num_points, self.d_k&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;需要说明的是，按照Transformer的理论，MultiHeadAttention的生成矩阵(即我们在上面用的投影应该是每个head有一个单独的projection)，但是因为这个projection是学习得到的，所以我们只用一个&lt;code&gt;projection&lt;/code&gt;然后再进行&lt;code&gt;view()&lt;/code&gt;分割得到&lt;code&gt;MultiHead&lt;/code&gt;，在理论上应该能得到相同的效果。&lt;/p&gt;
&lt;p&gt;(2) 根据得到的&lt;code&gt;query, key, value&lt;/code&gt;计算Self-Attention.&lt;/p&gt;
&lt;p&gt;Self-Attention的计算： $softmax(\frac{Q \times K^T}{\sqrt{d_k}}) V$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig12.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;返回的&lt;code&gt;z&lt;/code&gt;的shape为：&lt;code&gt;nbatches, self.h, num_points, self.d_k&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(3) 通过&lt;code&gt;view&lt;/code&gt;进行所谓的&lt;code&gt;Concatenate&lt;/code&gt;，将之应用于&lt;code&gt;Linear&lt;/code&gt;网络，输出。&lt;/p&gt;
&lt;p&gt;首先进行一个&lt;code&gt;transpose(1,2)&lt;/code&gt;,随后改变其内存分布&lt;code&gt;contiguous&lt;/code&gt;，然后再通过&lt;code&gt;view()&lt;/code&gt;，相当于把多个头的&lt;code&gt;attention&lt;/code&gt;拼接起来。此时的shape为：&lt;code&gt;nbatches, num_points, h * d_k&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;再应用于第四个&lt;code&gt;Linear&lt;/code&gt;上，输出的shape: (&lt;code&gt;nbatches, num_points, d_model&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;整个&lt;code&gt;Attention&lt;/code&gt;的网络结构如下：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig11.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;下面关注&lt;code&gt;PositionwiseFeedForward&lt;/code&gt;网络结构：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;PositionwiseFeedForward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_ff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PositionwiseFeedForward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_ff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_ff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w_1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w_2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;PositionwistFeedForward&lt;/code&gt;的网络结构比较简单，不再单独分析。
整个&lt;code&gt;Encoder&lt;/code&gt;的结构分析完毕。 下面再关注一下&lt;code&gt;Decoder&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Decoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Decoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layers&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LayerNorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;layers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;DecoderLayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# Decoder is made of self-attn, src-attn, and feed forward(defined below)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DecoderLayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_attn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_attn&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SublayerConnection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;](&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;self_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;](&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sublayer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;](&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feed_forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Decoder的网络结构如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig13.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;而DecoderLayer，接收的变量有&lt;code&gt;x&lt;/code&gt;,&lt;code&gt;memory&lt;/code&gt;,&lt;code&gt;src_mask&lt;/code&gt;,&lt;code&gt;tgt_mask&lt;/code&gt;，相当于先计算&lt;code&gt;self-attention&lt;/code&gt;,再计算&lt;code&gt;co-attention&lt;/code&gt;，最后&lt;code&gt;feed-forward&lt;/code&gt;，其网络结构为：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig14.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;至此Transformer应该已经分析清楚了。&lt;/p&gt;
&lt;h3 id=&#34;用于svd求解的模块module3&#34;&gt;用于SVD求解的模块Module3&lt;/h3&gt;
&lt;p&gt;我们的最终目的是求出刚体变换矩阵。使用上一个&lt;code&gt;Module&lt;/code&gt;中计算出的&lt;code&gt;soft pointer&lt;/code&gt;，可以生成一个平均意义上的&lt;code&gt;match point&lt;/code&gt;。
$$
\hat{y_i} = (Y_m)^T m(x_i,Y)
$$
这里，$Y_m$是指一个$R^{N \times 3}$的矩阵，包含着$Y$中所有点的信息。 根据这样一种&lt;code&gt;match&lt;/code&gt;关系，便可以通过&lt;code&gt;SVD&lt;/code&gt;求解出刚体变换矩阵。&lt;/p&gt;
&lt;p&gt;为了能够使得梯度反向传播，我们必须对SVD进行求导，由于我们使用SVD求解的只是3x3，可以使用其导数的近似形式。SVD求导的详细请参考论文:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimating the jacobian of the singular value decomposition: Theory and applications&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;SVDHead&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SVDHead&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reflect&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eye&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requires_grad&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reflect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_embedding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tgt_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;math&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;softmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;src_corr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tgt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;src_centered&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;src_corr_centered&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_corr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_corr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;H&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src_centered&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_corr_centered&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;S&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;V&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[],[],[]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;R&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;

        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;svd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;r&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;r_det&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;det&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;r_det&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;svd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reflect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;r&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;

            &lt;span class=&#34;n&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

            &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;S&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;V&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;U&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;V&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;V&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;S&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;S&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;R&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src_corr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;R&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;!-- raw HTML omitted --&gt;疑惑&lt;!-- raw HTML omitted --&gt;： 没在源码中看到有针对SVD求导的优化。&lt;/p&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数&lt;/h3&gt;
&lt;p&gt;$$
Loss = ||R_{XY}^TR_{XY}^g - I||^2 + ||t_{XY} - t_{XY}^g||^2 + \lambda||\theta||^2
$$&lt;/p&gt;
&lt;h3 id=&#34;关于dcp_v1和dcp_v2&#34;&gt;关于DCP_v1和DCP_v2&lt;/h3&gt;
&lt;p&gt;DCP_v1 没有应用&lt;code&gt;Attention&lt;/code&gt;机制。&lt;/p&gt;
&lt;p&gt;DCP_v2 应用了&lt;code&gt;Attention&lt;/code&gt;机制。&lt;/p&gt;
&lt;h2 id=&#34;深入探究&#34;&gt;深入探究&lt;/h2&gt;
&lt;h3 id=&#34;关于特征提取选择pointnet还是dgcnn&#34;&gt;关于特征提取：选择PointNet还是DGCNN?&lt;/h3&gt;
&lt;p&gt;PointNet 学习的是全局特征， 而DGCNN通过构建&lt;code&gt;k-NN&lt;/code&gt; Graph学习到的是局部集合特征。&lt;/p&gt;
&lt;p&gt;从文中的实验结果可以看出，使用DGCNN作为emb_net，比PointNet的性能始终要好。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig15.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;关于计算刚体变换选择mlp还是svd&#34;&gt;关于计算刚体变换：选择MLP还是SVD？&lt;/h3&gt;
&lt;p&gt;MLP在理论上可以模拟任何非线性映射。 而SVD是针对任务进行有目的性设计的网络。&lt;/p&gt;
&lt;p&gt;从文中的实验结果可以看出，使用SVD计算&lt;code&gt;rigid transformation&lt;/code&gt;总是更优。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig16.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h1 id=&#34;结语&#34;&gt;结语&lt;/h1&gt;
&lt;p&gt;以上就是我个人对DCP的笔记记录以及一些解读。&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;疑惑: LayerNorm的作用机制如何？&lt;/p&gt;
&lt;h1 id=&#34;dcp的缺点&#34;&gt;DCP的缺点&lt;/h1&gt;
&lt;p&gt;DCP 的文章主要有如下问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实验进行的数据集是ModelNet40， ModelNet40严格上来讲适用于分类任务，所有点基本都是均匀采样的，没有离群点， 即使有噪声也是人为添加的高斯噪声，这就使得文章的结论很缺乏说服力。&lt;/li&gt;
&lt;li&gt;DCP没有涉及裁剪率的问题，只能对默认两个点云之间是满射，这是不对的。在应用中也会有很多问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果觉得文章对您有帮助，欢迎点赞留言交流。给作者买杯咖啡就更感激不过了！
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/PayCode.jpg&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>torch.unsqueeze()解读</title>
        <link>https://codefmeister.github.io/p/torch.unsqueeze%E8%A7%A3%E8%AF%BB/</link>
        <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/torch.unsqueeze%E8%A7%A3%E8%AF%BB/</guid>
        <description>&lt;h1 id=&#34;torchunsqueeze函数解读&#34;&gt;torch.unsqueeze()函数解读&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;torch.unsqueeze(input, dim) --&amp;gt; Tensor&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;input(Tensor) &amp;ndash; the input tensor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;dim(int) &amp;ndash; the index at which to insert the singleton dimension&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;功能&#34;&gt;功能&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Return a new tensor with a dimension of size one inserted at the specified position.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;返回一个新的tensor，在指定的位置插入维度为1的一维。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The returned tensor shares the same underlaying data with this tensor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;返回的这一Tensor在内存中是和原Tensor共享一个内存数据的。(可以用&lt;code&gt;contiguous&lt;/code&gt;来重新分配)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A &lt;code&gt;dim&lt;/code&gt; value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at &lt;code&gt;dim = dim + input.dim() + 1&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;dim&lt;/code&gt;的取值范围是[-input.dim() - 1, input.dim() + 1). 负的&lt;code&gt;dim&lt;/code&gt;值将被映射到&lt;code&gt;dim + input.dim() + 1&lt;/code&gt;这一位置去。就相当于$-n$就是从最右往左数第$n$个位置。(-1就是最右插入的位置)&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unsqueeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unsqueeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title>DGCNN论文解读</title>
        <link>https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</link>
        <pubDate>Mon, 21 Dec 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</guid>
        <description>&lt;h1 id=&#34;dgcnn&#34;&gt;DGCNN&lt;/h1&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;因为关心的领域主要是配准，对于分类等网络的架构设计分析并没有侧重太多，主要侧重的是EdgeConv的思想。&lt;/p&gt;
&lt;h2 id=&#34;论文&#34;&gt;论文&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Dynamic Graph CNN for Learning on Point Clouds, Wang, Yue and Sun, Yongbin.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;核心思想关于edgeconv&#34;&gt;核心思想:关于&lt;code&gt;EdgeConv&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;将点云表征为一个图，${\rm{G}}(V,\xi )$ ,点云的每一个点对应图中的一个结点，而图中的每一条边对应的是点之间的特征&lt;code&gt;feature&lt;/code&gt;，称为&lt;code&gt;Edge-feature&lt;/code&gt;。举个例子，最简单的情景，可以通过KNN来构建图。&lt;code&gt;Edge Feature&lt;/code&gt;用$e_{ij}$来表示，定义为：
$$
e_{ij} = h_{\Theta}(x_i,x_j)
$$
$$
h_{\Theta}: {R^F} \times {R^F} \to {R^{F&#39;}}
$$
$h_{\Theta}$是一个非线性的映射，拥有一系列可学习的参数。&lt;/p&gt;
&lt;p&gt;提出了一个名为&lt;code&gt;EdgeConv&lt;/code&gt;的神经网络模块&lt;code&gt;Module&lt;/code&gt;，该模块基于卷积神经网络，可以适应在点云上的高阶任务。&lt;code&gt;EdgeConv&lt;/code&gt;的对于第i个顶点的输出为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/DGCNN_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;其中$□$代表的是一个对称聚合函数，如$\Sigma, max$。&lt;/p&gt;
&lt;p&gt;可以将上述描述类比为在图像上的卷积操作。我们把$x_i$看作是中心像素点，而$x_j:(i,j) \in \xi$可以看做是围绕在点$x_i$周围的像素($x_j$事实上就是和$x_i$之间存在着&lt;code&gt;feature edge&lt;/code&gt;的点）。所以类比这样的卷积操作，&lt;code&gt;Edge-Conv&lt;/code&gt;可以将n个点的$F$维点云通过“卷积”转换为具有n个点的$F&#39;$维的点云。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/DGCNN_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;所以选择$h$和$□$就变得十分关键。它会直接影响&lt;code&gt;EdgeConv&lt;/code&gt;的性能特性。&lt;/p&gt;
&lt;p&gt;一些其他的选择在下一个小part中讨论。在本文中，作者采用的：
$$
h_{\Theta}(x_i,x_j) = {\bar h}_{\Theta}(x_i, x_j - x_i)
$$&lt;/p&gt;
&lt;p&gt;从这个表达式可以非常明显的看出，既结合了全局形状结构，也结合了局部的结构信息。&lt;code&gt;Global shape structure&lt;/code&gt;通过$x_i$捕捉，&lt;code&gt;local neighborhood information&lt;/code&gt;通过$x_j - x_i$来捕捉。&lt;/p&gt;
&lt;p&gt;更具体一点的说，通过如下两个公式来计算edge_feature以及x&#39;：
$$
e_{ijm}&#39; = ReLU(\theta_m \cdot(x_j - x_i) + \phi_m \cdot x_i)
$$
$$
x_{im}&#39; = \mathop {\max }\limits_{j:(i,j) \in \xi }e_{ijm}&#39;
$$&lt;/p&gt;
&lt;p&gt;可以通过shared MLP实现。$\Theta = (\theta_1, &amp;hellip;, \theta_M,\phi_1, &amp;hellip;, \phi_M)$&lt;/p&gt;
&lt;p&gt;文中采用了&lt;strong&gt;Dynamic Graph Update&lt;/strong&gt;，即动态图更新。在每一层计算结束得到新的$x&#39;$后，会根据在特征空间上的最近邻(其实就是x&amp;rsquo;间的欧式距离)关系，动态更新图。这也是该文章命名的由来。动态更新可以使得EdgeConv的感受野变得越来越大，与点云的直径一样大，同时还很稀疏。&lt;/p&gt;
&lt;p&gt;在每一层之后，根据新的特征点云$x&#39;$，在特征空间上的距离，对于每一个点，使用KNN寻找其k个最近点，重新构建&lt;code&gt;Feature Edge&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;网络结构&#34;&gt;网络结构&lt;/h2&gt;
&lt;p&gt;网络结构的整体结构并不是我关注的重点。作者给出的网络结构有两个：一个用于&lt;code&gt;classification&lt;/code&gt;,一个用于&lt;code&gt;segmentation&lt;/code&gt;。&lt;br&gt;
同时，需要说明的是，作者在Sec.4中描述的网络结构与贴的图不一致。这里采用的是修正后的网络结构图。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/DGCNN_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;部分关键代码&#34;&gt;部分关键代码&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;寻找KNN，获得feature_edge的代码&lt;/strong&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;knn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;xx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pairwise_distance&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pairwise_distance&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;        &lt;span class=&#34;c1&#34;&gt;#(batch_size, num_points, k)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;
    

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim9&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# x: (batch_size, 3, num_points)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim9&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;knn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;knn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;idx_base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx_base&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# (batch_size, num_points, num_dims) --&amp;gt; (batch_size*num_points, num_dims)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# KNN&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;repeat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;permute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contiguous&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# (batch_size, 2*num_dims, num_points, k)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;下面这部分代码是分类网络，从中可以窥到EdgeConv以及动态更新的机制:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;DGCNN_cls&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_channels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;40&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DGCNN_cls&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;
        
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn5&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv5&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
                                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                    &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_dims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn6&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dp1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn7&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dp2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;256&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# (batch_size, 3, num_points) --&amp;gt; (batch_size, 3*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 3*2, num_points, k) --&amp;gt; (batch_size, 64, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points, k) --&amp;gt; (batch_size, 64, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points) --&amp;gt; (batch_size, 64*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64*2, num_points, k) --&amp;gt; (batch_size, 64, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points, k) --&amp;gt; (batch_size, 64, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64, num_points) --&amp;gt; (batch_size, 64*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64*2, num_points, k) --&amp;gt; (batch_size, 128, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# (batch_size, 128, num_points, k) --&amp;gt; (batch_size, 128, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_graph_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# (batch_size, 128, num_points) --&amp;gt; (batch_size, 128*2, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 128*2, num_points, k) --&amp;gt; (batch_size, 256, num_points, k)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;# (batch_size, 256, num_points, k) --&amp;gt; (batch_size, 256, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (batch_size, 64+64+128+256, num_points)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                       &lt;span class=&#34;c1&#34;&gt;#(batch_size, 64+64+128+256, num_points) --&amp;gt; (batch_size, emb_dims, num_points)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;adaptive_max_pool1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;           &lt;span class=&#34;c1&#34;&gt;# (batch_size, emb_dims, num_points) --&amp;gt; (batch_size, emb_dims)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;adaptive_avg_pool1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;           &lt;span class=&#34;c1&#34;&gt;# (batch_size, emb_dims, num_points) --&amp;gt; (batch_size, emb_dims)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                                      &lt;span class=&#34;c1&#34;&gt;# (batch_size, emb_dims*2)&lt;/span&gt;
        
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;leaky_relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# (batch_size, emb_dims*2) --&amp;gt; (batch_size, 512)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dp1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;leaky_relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;negative_slope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# (batch_size, 512) --&amp;gt; (batch_size, 256)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dp2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                                             &lt;span class=&#34;c1&#34;&gt;#(batch_size, 256) --&amp;gt; (batch_size, output_channels)&lt;/span&gt;
        
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;特性&#34;&gt;特性&lt;/h2&gt;
&lt;h3 id=&#34;置换不变性&#34;&gt;置换不变性&lt;/h3&gt;
&lt;p&gt;考虑每一层的输出：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/DGCNN_fig6.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;每一层的输出$x_i&#39;$是不会随着输入$x_j$的输入顺序改变而变化的。其原因在于max是一个对称的函数。&lt;/p&gt;
&lt;h3 id=&#34;平移不变性&#34;&gt;平移不变性&lt;/h3&gt;
&lt;p&gt;EdgeConv有着部分额平移不变性。因为：
$$
h_{\Theta}(x_i,x_j) = {\bar h}_{\Theta}(x_i, x_j - x_i)
$$
从这个公式就可以看出，函数的一部分是&lt;code&gt;Translation-dependent&lt;/code&gt;的，而另一部分是具有平移不变性的。&lt;/p&gt;
&lt;h2 id=&#34;用edgeconv的理论重新审视其他网络&#34;&gt;用EdgeConv的理论重新审视其他网络&lt;/h2&gt;
&lt;h3 id=&#34;pointnet&#34;&gt;PointNet&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/DGCNN_fig4.png&#34; alt=&#34;image&#34;  /&gt;
PointNet的特点是将每一个点进行单独处理，不断抽象到高维空间（这也正是其缺乏局部结构信息的缺陷来源）。
所以PointNet实际是本文理论的一种特殊情形。$k = 1, \xi  = \emptyset$,采用的&lt;code&gt;Edge Function&lt;/code&gt;实际上是：$h_{\Theta}(x_i,x_j) = h_{\Theta}(x_i)$.只考虑到&lt;code&gt;Global feature&lt;/code&gt;而忽略了&lt;code&gt;local geometry&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;pointnet-1&#34;&gt;PointNet++&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/DGCNN_fig5.png&#34; alt=&#34;image&#34;  /&gt;
PointNet++意识到了PointNet中存在的问题，所以它通过FPS采样，然后通过&lt;code&gt;ball query&lt;/code&gt;构造局部点集，再通过PointNet抽象出局部特征。&lt;/p&gt;
&lt;p&gt;PointNet++ 首先通过欧氏距离来构建图Graph，然后每经过一个layer对图进行一次粗糙化。在每一层，首先通过FPS选取一些点，只有这些被选取的点将保留，其他的点都将被抛弃。通过这种方法，图会在每经过一层网络后变得越来越小。同时，PointNet++使用输入数据的欧式距离来计算点对关系，所以就导致了他们的图在整个训练过程中是固定的，而不是动态调整的（DGCNN一大特点就是动态图）。&lt;code&gt;Edge Function&lt;/code&gt;是$h_{\Theta}(x_i,x_j)=h_{\Theta}(x_j)$&lt;/p&gt;
&lt;h3 id=&#34;monet-与-pcnn&#34;&gt;MoNet 与 PCNN&lt;/h3&gt;
&lt;p&gt;笔者还未阅读这两篇论文。看的时候没什么概念。先留个坑。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>图解Transformer</title>
        <link>https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/</link>
        <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/</guid>
        <description>&lt;h1 id=&#34;图解transformer&#34;&gt;图解Transformer&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a class=&#34;link&#34; href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Illustrated Transformer&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文自译用于加深理解与印象。&lt;/p&gt;
&lt;p&gt;关于注意力机制，可以参考先前的&lt;code&gt;Seq2Seq Model with Attention&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Transformer是论文&lt;strong&gt;Attention is All You Need&lt;/strong&gt;提出的。在这篇文章中，我们将尝试把事情弄得简单一点，逐个介绍概念，以便更好理解。&lt;/p&gt;
&lt;h2 id=&#34;a-high-level-look&#34;&gt;A High-Level Look&lt;/h2&gt;
&lt;p&gt;我们首先把模型看作是一个黑箱。在机器翻译领域的应用中，输入一种语言的一个句子，会输出另外其翻译结果。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;揭开盖子，我们能够看到一个编码组件&lt;code&gt;encoding component&lt;/code&gt;，一个解码组件&lt;code&gt;decoding component&lt;/code&gt;，还有其之间的连接关系&lt;code&gt;connections&lt;/code&gt;。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;编码组件是一堆编码器构成的（Paper中堆叠了六个编码器，&lt;em&gt;六个&lt;/em&gt;并没有什么说法，你也可以尝试其他数字）。解码组件也是由一堆解码器构成的（数量与编码器相同）。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;所有编码器在结构上都是相同的，&lt;strong&gt;然而他们并不共享参数（或权重）。&lt;/strong&gt; 每一个都可以被拆分为两个子层&lt;code&gt;sub-layers&lt;/code&gt;。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig4.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;编码器的输入首先流过&lt;code&gt;self-attention&lt;/code&gt;层，&lt;code&gt;self-attention&lt;/code&gt;层可以帮助我们在对某个特定的词进行编码的时候同时关注到句子中其他位置单词的影响。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;self-attention&lt;/code&gt;层的输出被送往&lt;code&gt;feed-foward neural network&lt;/code&gt;，即前馈神经网络层。完全相同的前馈网络，&lt;strong&gt;独立地&lt;/strong&gt;作用于每一个位置&lt;code&gt;position&lt;/code&gt;上。&lt;/p&gt;
&lt;p&gt;解码器也有上述这两个层，但除此以外，在这两层之间，还有一个&lt;code&gt;attention layer&lt;/code&gt;，帮助解码器更加关注输入句子中相关的部分。（作用类似于Seq2Seq中的注意力机制的作用。）
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig5.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;bringing-the-tensor-into-the-picture&#34;&gt;Bringing The Tensor Into The Picture&lt;/h2&gt;
&lt;p&gt;现在，我们已经了解了模型的主要组件，下面让我们开始研究各种矢量/张量以及它们如何在这些组件之间流动，以将经过训练的模型的输入转换为输出。&lt;/p&gt;
&lt;p&gt;首先我们将每一个输入单词通过&lt;code&gt;embedding algorithm&lt;/code&gt;转换为一个词向量。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig6.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;嵌入过程只发生在最底部的encoder。对于所有的编码器&lt;code&gt;Encoder&lt;/code&gt;，他们都接受一个size为512的向量列表作为输入。只不过对于最底部的Encoder，其输入为单词经过嵌入后得到的词向量，而其他的Encoder的输入，是其下方一层Encoder的输出。列表的size是一个我们可以设定的超参数——通常来讲它会是我们训练集中最长的一个句子的长度。&lt;/p&gt;
&lt;p&gt;在将输入序列中的单词进行&lt;code&gt;Embedding&lt;/code&gt;之后，他们中的每一个都会流过编码器的两层。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig7.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;从这里我们可以看到一个Transformer非常重要的特性，那便是每一个位置上的单词在Encoder中自己的路径上各自流动。在&lt;code&gt;self-attention&lt;/code&gt;层中，这些路径之间存在相互依赖。而前馈层&lt;code&gt;feed-forward&lt;/code&gt;中彼此间并无依赖。所以在流经前馈层的时候，可以进行并行化处理。&lt;/p&gt;
&lt;p&gt;下面我们将举一个短句的例子，然后观察&lt;code&gt;sub-layer&lt;/code&gt;上发生了什么。&lt;/p&gt;
&lt;h2 id=&#34;now-were-encoding&#34;&gt;Now We&amp;rsquo;re Encoding&lt;/h2&gt;
&lt;p&gt;像我们先前提到的，一个编码器接收一个向量列表作为输入。这个向量列表首先被送往&lt;code&gt;self-attention&lt;/code&gt;层，然后再送往&lt;code&gt;feed-forward&lt;/code&gt;前馈层。处理结束后将其&lt;code&gt;output&lt;/code&gt;送往下一个&lt;code&gt;Encoder&lt;/code&gt;。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig8.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;每个位置的单词都被送往一个&lt;code&gt;self attention&lt;/code&gt;层，然后再穿过一个前馈神经网络——每个向量独立穿过这个完全相同的网络。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;self-attention-at-a-high-level&#34;&gt;Self-Attention at a High Level&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;self-attention&lt;/code&gt;是Paper中提出的一个全新概念，不要被其简单的命名给迷惑。&lt;/p&gt;
&lt;p&gt;假设我们输入了如下一个句子，并试图进行翻译：&lt;br&gt;
&lt;strong&gt;&amp;ldquo;The animal didn&amp;rsquo;t cross the street because it was too tired&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那么句子中的&lt;code&gt;it&lt;/code&gt;指代谁呢？是指街道&lt;code&gt;street&lt;/code&gt;还是指动物&lt;code&gt;animal&lt;/code&gt;。这个问题对人类来说再简单不过，不过对算法来说却不是这样。&lt;/p&gt;
&lt;p&gt;当处理单词&lt;code&gt;it&lt;/code&gt;时，&lt;code&gt;self-attention&lt;/code&gt;机制就可以让&lt;code&gt;it&lt;/code&gt;与&lt;code&gt;animal&lt;/code&gt;联系在一起。&lt;/p&gt;
&lt;p&gt;当模型处理每个单词（即输入序列中的每个position）时，&lt;code&gt;self-attention&lt;/code&gt;可以在输入序列中的其他位置中寻找线索，来帮助这个单词获得更好的编码效果。&lt;/p&gt;
&lt;p&gt;如果你熟悉RNN的话，想一下RNN是通过维持一个隐藏状态，来结合先前的处理过的向量与当前的输入向量。 而&lt;code&gt;Self-attention&lt;/code&gt;层是结合所有其他相关单词的&amp;quot;理解&amp;quot;，将这些理解“融入”对当前处理单词的编码中。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig9.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当我们在最顶端的编码器#5对单词&lt;code&gt;it&lt;/code&gt;进行编码时，注意力机制的一部分就会集中在&lt;code&gt;The Animal&lt;/code&gt;上，并将它的一部分编码表示（&lt;code&gt;representation&lt;/code&gt;，我个人理解为是编码表示）融入到对&lt;code&gt;it&lt;/code&gt;的编码中去。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;self-attention-in-detail&#34;&gt;Self-Attention in Detail&lt;/h2&gt;
&lt;p&gt;我们先看一下如何使用向量来计算&lt;code&gt;self-attention&lt;/code&gt;，然后再看一次它真正的实现方式——通过矩阵计算。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Self-Attention&lt;/code&gt;的&lt;strong&gt;第一步&lt;/strong&gt;：为输入的向量列表中的每一个向量(在这里的例子，由于是最底层，所以输入的是单词的Embedding)都创建三个向量。所以对于每个单词而言，我们创建了一个&lt;code&gt;Query vector&lt;/code&gt;，一个&lt;code&gt;Key vector&lt;/code&gt;，一个&lt;code&gt;Value vector&lt;/code&gt;。这三个向量是通过将输入向量乘以三个矩阵（矩阵是在训练中得到的）而得到的。&lt;/p&gt;
&lt;p&gt;值得关注的是，这些新的向量在维度上比输入向量更小。他们的维度是&lt;code&gt;64&lt;/code&gt;，而&lt;code&gt;embedding&lt;/code&gt;和编码器的&lt;code&gt;input/output&lt;/code&gt;向量的维度是512。这些向量并不是&lt;strong&gt;必须&lt;/strong&gt;要比原来的维度更小，这仅仅是一种架构上的选择，为的是&lt;code&gt;multiheaded attention&lt;/code&gt;的计算恒定。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig10.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;用$x_1$乘以权重矩阵$W^Q$就得到了$x_1$的&lt;code&gt;query vector&lt;/code&gt;-$q1$。类似这样，我们最终为输入序列中的每个单词都创造一个&lt;code&gt;query&lt;/code&gt;，一个&lt;code&gt;key&lt;/code&gt;，以及一个&lt;code&gt;value&lt;/code&gt;投影。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;第二步&lt;/strong&gt;：计算&lt;code&gt;score&lt;/code&gt;。例如当我们计算下例中的第一个单词&amp;quot;Thinking&amp;quot;时，我们需要基于当前单词，为输入句子中其他位置的每个单词打分（有一点条件概率的意思）。这个分数决定了在我们对这个单词进行编码的时候，对输入序列的单词需要给予多少注意力。&lt;/p&gt;
&lt;p&gt;该分数通过将&lt;code&gt;query vector&lt;/code&gt;与&lt;code&gt;key vector&lt;/code&gt;进行点积操作&lt;code&gt;dot product&lt;/code&gt;来得到。所以当我们对位置1的单词计算其&lt;code&gt;self-attention&lt;/code&gt;时，第一个分数会是&lt;code&gt;q1&lt;/code&gt;和&lt;code&gt;k1&lt;/code&gt;的点积。第二个分数是&lt;code&gt;q1&lt;/code&gt;和&lt;code&gt;k2&lt;/code&gt;的点积。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig11.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;第三步是将这些分数除以8(8是论文中&lt;code&gt;key vector&lt;/code&gt;维度&lt;code&gt;64&lt;/code&gt;的平方根，这可以使得梯度更加稳定，同时你可以选取其他的值)。&lt;/p&gt;
&lt;p&gt;第四步是经过一个&lt;code&gt;softmax&lt;/code&gt;操作，&lt;code&gt;softmax&lt;/code&gt;可以保证score经过处理后全部为正，而且加和为1.
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig12.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;softmax score&lt;/code&gt;决定了每个单词将对这个&lt;code&gt;position&lt;/code&gt;的下一次编码起到多大作用。显然，当前位置的单词会有最大的&lt;code&gt;softmax score&lt;/code&gt;，但是无疑，关注与当前单词有关的单词是很有用。&lt;/p&gt;
&lt;p&gt;第五步：是将每个值向量&lt;code&gt;value vector&lt;/code&gt;乘以其对应的&lt;code&gt;softmax&lt;/code&gt;分数（要准备将他们加在一起了），这一步的动机就是放大那些我们&lt;code&gt;focus&lt;/code&gt;的单词，而淡化那些不那么重要的单词。&lt;/p&gt;
&lt;p&gt;第六步：将所有加权后的值向量&lt;code&gt;weighted value vector&lt;/code&gt;（上一步得到的）相加，得到self-attention层对于该位置（例子中是第一个单词）的输出。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig13.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;以上就是&lt;code&gt;self-attention&lt;/code&gt;的计算过程。得到的向量是一个我们可以直接送往前馈神经网络的向量。 在实际的实现中，我们通过矩阵运算来更快的处理。&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculation-of-self-attention&#34;&gt;Matrix Calculation of Self-Attention&lt;/h2&gt;
&lt;p&gt;第一步： 计算&lt;code&gt;Query&lt;/code&gt;,&lt;code&gt;Key&lt;/code&gt;and &lt;code&gt;Value&lt;/code&gt;矩阵。我们通过把输入的向量打包为一个矩阵，然后乘以我们训练得到的权重矩阵计算之。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig14.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;X中的每一行对应输入序列中的一个单词。我们从图中又一次可以看到&lt;code&gt;embdding vector&lt;/code&gt;和&lt;code&gt;q/k/v&lt;/code&gt;向量的在维度上是不同的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后：由于我们使用矩阵进行数据处理，我们可以把2-6步浓缩到一步，直接计算出&lt;code&gt;self-attention&lt;/code&gt;层的输出。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig15.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-beast-with-many-heads&#34;&gt;The Beast With Many Heads&lt;/h2&gt;
&lt;p&gt;论文随后进一步细化了&lt;code&gt;self-attention&lt;/code&gt;层，为其增加了一种叫做&lt;code&gt;multi-headed attention&lt;/code&gt;的机制。它通过两种方式提升&lt;code&gt;attention-layer&lt;/code&gt;的表现。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Multi-head&amp;quot;扩展了模型focus于不同位置的能力。就像上面那个例子一样，&lt;code&gt;z1&lt;/code&gt;包含有其他每个字母编码的一小部分，但其主要还是被其本来位置上的单词所主宰。在我们翻译类似&amp;quot;The animal didn&amp;rsquo;t cross the street because it was too tired&amp;quot;这种句子时，Multi-Head Attention是非常有用的，因为我们想要知道&lt;code&gt;it&lt;/code&gt;指代的是谁。&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Multi-head&amp;quot;给了&lt;code&gt;Attention Layer&lt;/code&gt;一些代表子空间&lt;code&gt;representation subspaces&lt;/code&gt;。&lt;code&gt;Multi-Headed Attention&lt;/code&gt;机制下，我们不再是只有一组权重矩阵，而是有多组权重矩阵(Transformer中使用了八个&lt;code&gt;attention head&lt;/code&gt;，所以我们最后会得到八组&lt;code&gt;Query/Key/Value&lt;/code&gt;的权重矩阵。每一组都是随机初始化， 然后经过循环后，每一组都用于将输入向量投影到不同的子空间中&lt;code&gt;representation subspace&lt;/code&gt;。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig16.png&#34; alt=&#34;image&#34;  /&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Multi-headed Attention&lt;/code&gt;机制下，我们分别保存着每个子空间(&lt;code&gt;each head&lt;/code&gt;)的&lt;code&gt;Q/K/V&lt;/code&gt;矩阵。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那么为上述八组&lt;code&gt;Q/K/V&lt;/code&gt;矩阵，分别做完上述的&lt;code&gt;self-attention&lt;/code&gt;计算后，我们会得到八个不同的&lt;code&gt;Z&lt;/code&gt;矩阵。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig17.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;然后迎面而来的就有一个小问题，前馈神经网络并不需要8个矩阵，它期待的输入是一个矩阵，其中每一个向量代表一个单词。所以我们需要一种方法来将八个矩阵浓缩为一个。&lt;/p&gt;
&lt;p&gt;怎么做呢？我们&lt;code&gt;concatenate&lt;/code&gt;这些矩阵，然后将拼接后的矩阵乘以一个额外的&lt;code&gt;weights matrix&lt;/code&gt;&amp;ndash;$WO$。所得到的结果&lt;code&gt;Z&lt;/code&gt;就会捕捉到所有&lt;code&gt;attention head&lt;/code&gt;中包含的信息，然后这个&lt;code&gt;Z&lt;/code&gt;被送往FFNN。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig18.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;以上就是&lt;code&gt;Multi-head self-attention&lt;/code&gt;的全部，下面我们将所有的信息集合在一张图中：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig19.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;说完了&lt;code&gt;attention-head&lt;/code&gt;，我们回顾一下先前的例子，看一下不同的&lt;code&gt;attention head&lt;/code&gt;在我们对&lt;code&gt;it&lt;/code&gt;进行编码的是如何focus的。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig20.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当我们编码单词&lt;code&gt;it&lt;/code&gt;时，一个&lt;code&gt;attention head&lt;/code&gt;更集中于&lt;code&gt;the animal&lt;/code&gt;上，另一个更集中于&lt;code&gt;tired&lt;/code&gt;上。从语义上分析，it在编码时融入了&lt;code&gt;animal&lt;/code&gt;和&lt;code&gt;tired&lt;/code&gt;的&lt;code&gt;representation&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所有&lt;code&gt;attention head&lt;/code&gt;全部加进来的情况如下，语义上有点难解释。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig29.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;representing-the-order-of-the-sequence-using-positional-encoding&#34;&gt;Representing The Order of the Sequence Using Positional Encoding&lt;/h2&gt;
&lt;p&gt;以上我们描述的模型中，缺失了单词在输入序列中的输入顺序。（没有时序信息）&lt;/p&gt;
&lt;p&gt;为了记录这一点，Transformer为每个&lt;code&gt;input embedding&lt;/code&gt;加上了一个向量。这些向量遵循模型学习的一种特定模式&lt;code&gt;specific pattern&lt;/code&gt;，有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是：将这些值添加到&lt;code&gt;embedding&lt;/code&gt;中，当其被投影到&lt;code&gt;Q/K/V&lt;/code&gt;向量中或者在进行点积操作时，就会提供有意义的距离信息。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig21.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;假设&lt;code&gt;embedding&lt;/code&gt;是四维的话，那么实际的&lt;code&gt;positional encoding&lt;/code&gt;看起来可能是这个样子:
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig22.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;pattern&lt;/code&gt;看起来应该是什么样子？&lt;/p&gt;
&lt;p&gt;在下图中，每一行对应一个向量的位置信息的编码结果。所以第一行就是我们将要加到输入序列中第一个单词的&lt;code&gt;embedding&lt;/code&gt;结果上的向量。每一行含有512个值，每个值的取值范围都是[-1,1],下图进行了可视化。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig23.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;positional encoding&lt;/code&gt;的公式在论文中详细描述了，而且这也不是唯一的对位置信息进行编码的方法。但是论文中的编码方式有一个优势：可以对未训练过的序列长度进行很好的缩放。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig24.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-residuals&#34;&gt;The Residuals&lt;/h2&gt;
&lt;p&gt;一个encoder架构的细节是： 在每个&lt;code&gt;sub-layer&lt;/code&gt;之后，都有一个&lt;code&gt;residual connection&lt;/code&gt;，随后是一个&lt;code&gt;layer-normalization&lt;/code&gt;归一化操作。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig25.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;如果我们将向量和&lt;code&gt;layer-norm&lt;/code&gt;与&lt;code&gt;self attention&lt;/code&gt;的操作形象化，它看起来会是这样：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig26.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;对于解码器的子层，也是这样。如果我们把它想象成一个由两个堆叠的编码器和解码器（Paper中是六个），它看起来应该是这样的。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_fig27.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-decoder-side&#34;&gt;The Decoder Side&lt;/h2&gt;
&lt;p&gt;我们已经讨论了编码器方面的绝大部分概念，也知道了其是如何工作的。下面让我们看一下他们是怎么样一起工作的。&lt;/p&gt;
&lt;p&gt;编码器从处理输入序列开始，最顶端的编码器的输出随后被转换为&lt;code&gt;attention vector&lt;/code&gt; $K,V$的集合。这些向量在每一个解码器&lt;code&gt;decoder&lt;/code&gt;的&lt;code&gt;encoder-decoder&lt;/code&gt;层使用，用于帮助解码器focus在&lt;code&gt;input sequence&lt;/code&gt;的恰当位置。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_gif1.gif&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;下面的步骤重复这个过程，直到遇到终止符（表明decoder已经完成输出）。每一步的输出都会在下一个时间步反馈给最底层的解码器，解码器会将其输出一层一层向上&lt;code&gt;bubble up&lt;/code&gt;，就像编码器所做的一样。同时，我们也为解码器的输入加上了位置信息的编码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Transformer_gif2.gif&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;解码器中的&lt;code&gt;self-attention layer&lt;/code&gt;与编码器中有微微的一些不同。&lt;/p&gt;
&lt;p&gt;在解码器中，&lt;code&gt;self-attention layer&lt;/code&gt;只允许将时序上在前的位置信息“融合”进输出序列，这是通过在softmax步前进行mask来完成的。&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;Encoder-Decoder Attention&lt;/code&gt;层的工作原理，类似于&lt;code&gt;Multiheaded self-attention&lt;/code&gt;，不同的是，它从其下一层创建&lt;code&gt;Query Matrix&lt;/code&gt;， 而&lt;code&gt;Key&lt;/code&gt;和&lt;code&gt;Value&lt;/code&gt;矩阵都来自编码器最上层的输出。&lt;/p&gt;
&lt;h2 id=&#34;the-final-linear-and-softmax-layer&#34;&gt;The Final Linear and Softmax Layer&lt;/h2&gt;
&lt;p&gt;解码器输出的是一个浮点数的向量，我们应该如何将其转换为单词呢？这就是最后一个线性层的工作，其后还跟着一个Softmax层。&lt;/p&gt;
&lt;p&gt;Linear Layer是一个简单的全连接神经网络，将解码器产生的向量投影到一个维度大的多的向量&lt;code&gt;logits vector&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;假设我们的模型从训练集中学到了10000个不同的英文单词，那么我们的&lt;code&gt;logits vector&lt;/code&gt;就有10000个元素，每个都对应着一个单词的score。这就是我们将其转换成单词的方法。&lt;/p&gt;
&lt;p&gt;Softmax 层随后将这些score转换为概率，有最高概率的单词被选中，然后输出。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Conv1d与Conv2d</title>
        <link>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</link>
        <pubDate>Wed, 16 Dec 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</guid>
        <description>&lt;h1 id=&#34;conv1d与conv2d&#34;&gt;Conv1d与Conv2d&lt;/h1&gt;
&lt;p&gt;本文分为几个部分来详解Conv2d与Conv1d。主要侧重于Conv2d&lt;/p&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;本文记于2020年12月15日，起因是DGCNN中部分卷积使用了二维卷积，部分卷积使用了一维卷积。加之之前对Conv2d与Conv1d属于一种迷迷糊糊的状态，趁着这个机会弄清楚。&lt;/p&gt;
&lt;h2 id=&#34;conv2d原理二维卷积层&#34;&gt;Conv2d原理（二维卷积层）&lt;/h2&gt;
&lt;h3 id=&#34;二维互相关运算&#34;&gt;二维互相关运算&lt;/h3&gt;
&lt;h4 id=&#34;互相关运算与卷积运算&#34;&gt;互相关运算与卷积运算&lt;/h4&gt;
&lt;p&gt;虽然卷积层得名于卷积(convolution)运算，但所有框架在实现卷积层的底层，都采用的是互相关运算。实际上，卷积运算与互相关运算类似，&lt;strong&gt;为了得到卷积运算的输出，我们只需要将核数组左右翻转并上下翻转，然后再与输入数组做互相关运算&lt;/strong&gt;。所以这两种运算虽然类似，但是输出并不相同。&lt;/p&gt;
&lt;p&gt;但是由于深度学习中核数组都是学习得到的，所以卷积层无论使用互相关运算还是卷积运算，都不影响模型预测时的输出。也就是说我们用卷积运算学出的核数组与用互相关运算学出的核数组两者之间可以通过上下翻转，左右翻转来相互转换。所以在框架乃至于绝大部分深度学习文献中，都使用互相关运算来代替了卷积运算。&lt;/p&gt;
&lt;h4 id=&#34;互相关运算&#34;&gt;互相关运算&lt;/h4&gt;
&lt;p&gt;在二维卷积层中，一个二维输入数组和一个二维核(kernel)数组通过互相关运算输出一个二维数组。举个例子来解释二维互相关运算：&lt;/p&gt;
&lt;p&gt;假设输入数组的高和宽均为3， 核数组的高和宽均为2，该数组在卷积运算中又称为卷积核或者过滤器(filter)。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Conv1d_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;19是这样得出的： $19 = 0\times0 + 1\times1 + 3\times2 + 4\times3$ 。&lt;/p&gt;
&lt;p&gt;卷积窗口从输入数组的最左上方开始，按照从左往右，从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和。得到输出数组中对应位置的元素。&lt;/p&gt;
&lt;h3 id=&#34;二维卷积层&#34;&gt;二维卷积层&lt;/h3&gt;
&lt;p&gt;二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包含了卷积核和标量偏差。我们在训练模型的时候，通常先对卷积层进行随机的初始化，然后不断迭代卷积核和偏差。&lt;/p&gt;
&lt;p&gt;卷积窗口形状为$p \times q$的卷积层称为$p \times q$卷积层。&lt;/p&gt;
&lt;h3 id=&#34;特征图与感受野&#34;&gt;特征图与感受野&lt;/h3&gt;
&lt;p&gt;二维卷积层输出的二维数组可以看做是输入在空间维度上(宽和高)上某一级的表征，也叫特征图(&lt;code&gt;feature map&lt;/code&gt;)。影响元素$x$的前向计算的所有可能输入区域(甚至可能大于输入的实际尺寸)叫做$x$的感受野(&lt;code&gt;receptive field&lt;/code&gt;)。以上图为例，图中输入的阴影部分的四个元素就是输出数组中阴影部分元素的感受野。如果我们将该输出再和一个$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么$z$在输入上的感受野包含全部的9个元素。&lt;/p&gt;
&lt;p&gt;可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。&lt;/p&gt;
&lt;h3 id=&#34;填充与步幅&#34;&gt;填充与步幅&lt;/h3&gt;
&lt;p&gt;卷积层的输出形状由输入形状和卷积核窗口形状决定，通过填充与步幅，我们可以改变给定形状的输入和卷积层下的输出形状。&lt;/p&gt;
&lt;h4 id=&#34;填充&#34;&gt;填充&lt;/h4&gt;
&lt;p&gt;填充&lt;code&gt;padding&lt;/code&gt;是指在输入高和宽的两侧填充元素(通常是0元素)。如下图：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Conv1d_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;假设输入形状为$n_h \times n_w$， 卷积核窗口形状是$k_h \times k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是：
$$
(n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)
$$
很多情况下我们会设置$p_h = k_h -1$和$p_w = k_w - 1$来使得输入输出具有相同的高和宽。&lt;/p&gt;
&lt;p&gt;对于任意的二维数组&lt;code&gt;X&lt;/code&gt;，当两端的填充个数相同，并使得输入和输出具有相同的高和宽时，我们就知道输出&lt;code&gt;Y[i,j]&lt;/code&gt;是输入以&lt;code&gt;X[i,j]&lt;/code&gt;为中心的窗口同卷积核进行互相关运算而得到的。&lt;/p&gt;
&lt;h4 id=&#34;步幅&#34;&gt;步幅&lt;/h4&gt;
&lt;p&gt;卷积窗口从输入数组的最左上方开始，按照从左向右，从上向下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅(&lt;code&gt;stride&lt;/code&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Conv1d_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：
$$
[(n_h - k_h + p_h + s_h)/s_h] \times [(n_w - k_w +p_w +s_w) / s_w]
$$&lt;/p&gt;
&lt;p&gt;为了表述简洁，当输入的高和宽两侧的填充数分别为$p_h$和$p_w$时，我们称填充为$(p_h, p_w)$。特别地，当$p_h = p_w = p$时，填充为$p$。当在高和宽上的步幅分别为$s_h$和$s_w$时，我们称步幅为($s_h,s_w$)。特别地，当$s_h = s_w = s$时，步幅为$s$。在默认情况下，填充为0，步幅为1.&lt;/p&gt;
&lt;h3 id=&#34;多输入通道和多输出通道&#34;&gt;多输入通道和多输出通道&lt;/h3&gt;
&lt;h4 id=&#34;多输入通道&#34;&gt;多输入通道&lt;/h4&gt;
&lt;p&gt;当输入数据含多个通道时，我们需要构造一个输入通道数和输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。&lt;/p&gt;
&lt;p&gt;假设输入数据的通道数为$c_i$，那么卷积核的输入通道数同样为$c_i$。设卷积核的窗口形状为$k_h \times k_w$。当$c_i = 1$，我们知道卷积核只包含一个形状为$k_h \times k_w$的二维数组。当$c_i &amp;gt; 1$时，我们将会为每个输入通道各分配一个形状为$k_h \times k_w$的核数组，这$c_i$个数组在通道维上连结，即得到一个形状为$c_i \times k_h \times k_w$的卷积核。&lt;/p&gt;
&lt;p&gt;由于输入和卷积核各有$c_i$个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这$c_i$个互相关运算的二维输出按照通道相加，得到&lt;strong&gt;一个&lt;/strong&gt;二维数组。 这就是多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Conv1d_fig4.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h4 id=&#34;多输出通道&#34;&gt;多输出通道&lt;/h4&gt;
&lt;p&gt;当输入通道有多个时，因为我们对各个通道的结果进行了累加，所以不论输入通道数是多少，输出通道数总是为1。 设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，宽和高分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i \times k_h \times k_w$的核数组。将它们在输出通道维连结，卷积核的形状即：$c_o \times c_i \times k_h \times k_w$。 在做互相关运算时，每个&lt;strong&gt;输出通道&lt;/strong&gt;上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。&lt;/p&gt;
&lt;h3 id=&#34;1-times-1卷积层-与-全连接层&#34;&gt;$1 \times 1$卷积层 与 全连接层&lt;/h3&gt;
&lt;p&gt;最后我们讨论卷积窗口形状为$1 \times 1 （k_h = k_w =1)$的多通道卷积层。我们通常称之为$1\times1$卷积层，并将其中的卷积运算称为$1\times1$卷积。因为使用了最小窗口，$1\times1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\times1$卷积的主要计算发生在通道维上。$1\times1$卷积的输入和输出具有相同的高和宽。输出中的每个元素来自于输入中在高和宽上相同位置的元素在不同通道之间按权重累加。假设我们将通道维当做特征维，而将高和宽维度上的元素当成数据样本，&lt;strong&gt;那么$1\times1$卷积层的作用于全连接层等价&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;用$1\times1$卷积替代全连接层的时候，一定需要注意Tensor的维度顺序。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/Conv1d_fig5.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;conv1d&#34;&gt;Conv1d&lt;/h2&gt;
&lt;p&gt;Conv1d的输入是三维数据:&lt;code&gt;(Batch_size, channels, width)&lt;/code&gt;
卷积操作沿着通道维对&lt;code&gt;width&lt;/code&gt;维进行。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LeakyReLU函数解析</title>
        <link>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</link>
        <pubDate>Mon, 14 Dec 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</guid>
        <description>&lt;h1 id=&#34;leakyrelu&#34;&gt;LeakyReLU&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CLASS&lt;/strong&gt;  &lt;code&gt;torch.nn.LeakyReLU(negative_slope: float = 0.01, inplace: bool = False)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Element-wise&lt;/strong&gt;&lt;br&gt;
对于每个x，应用函数如图：
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/LeakyRelu_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;函数图像&#34;&gt;函数图像&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/LeakyRelu_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MATrICP论文解读</title>
        <link>https://codefmeister.github.io/p/matricp%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</link>
        <pubDate>Thu, 03 Dec 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/matricp%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</guid>
        <description>&lt;h1 id=&#34;matricp&#34;&gt;MATrICP&lt;/h1&gt;
&lt;h2 id=&#34;论文&#34;&gt;论文&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Improved techniques for multi-view registration with motion averaging&lt;/code&gt;&lt;br&gt;
&lt;code&gt;Li, Zhongyu Zhu, Jihua Lan, Ke Li, Chen Fang, Chaowei&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;将&lt;code&gt;Trimmed ICP&lt;/code&gt;与&lt;strong&gt;运动平均算法&lt;/strong&gt;结合起来，应用到多视角聚类上。&lt;/p&gt;
&lt;h2 id=&#34;算法步骤&#34;&gt;算法步骤&lt;/h2&gt;
&lt;h3 id=&#34;1-估算各帧之间的重叠百分比xi_ij&#34;&gt;1. 估算各帧之间的重叠百分比$\xi_{i,j}$&lt;/h3&gt;
&lt;p&gt;总的来说，估算各帧之间的重叠百分比主要分为两步：&lt;br&gt;
(1) 对于每一帧，计算其$d_{threshold}$&lt;br&gt;
(2) 计算出每一帧的$d_{threshold}$之后，使用该参数计算该帧与其他帧的重叠百分比。&lt;/p&gt;
&lt;h4 id=&#34;10-背景知识&#34;&gt;1.0 背景知识&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;ObjFunc&lt;/code&gt;：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h4 id=&#34;11-计算d_threshold&#34;&gt;1.1 计算$d_{threshold}$&lt;/h4&gt;
&lt;p&gt;对于第i帧的每一个点，可以在其他所有帧中寻找到N-1个对应点（通过NN），假设第i帧有$N_i$个点，那么一共会有$N_i * (N-1)$个点对与距离。因为我们是要进行多视角配准的，相当于把当前帧作为源scan，其他所有帧组成的模型作为目标模板进行配准。所以将这些所有距离按照从小到大进行排序，然后依次对于每一个距离，计算该距离以及之前所有距离对应的ObjectFunction值。可以使用&lt;code&gt;cumsum&lt;/code&gt;操作。结果是得到同样长度的ObjectFunction值的数组，取其中的最小值，该目标函数最小值对应着一个距离$d_i$，这个距离$d_i$就可以作为第i帧的$d_{threshold}$，用于第i帧与其他帧（第j帧）的重叠率估算。&lt;/p&gt;
&lt;h4 id=&#34;12-计算第i帧与其他帧的重叠百分比&#34;&gt;1.2 计算第i帧与其他帧的重叠百分比&lt;/h4&gt;
&lt;p&gt;对于第$i$帧，我们现在有其$d_{threshold}$。那么求$\xi_{i,j}$，即为：使用NN寻找点对pair$(P_i,P_j)$，然后从小到大排列，取$d &amp;lt; d_{threshold}$的部分。假设有$N_j^{&#39;}$个点对满足要求。那么重叠百分比$\xi_{i,j}  = N_j^{&#39;} / N_j$，$N_j$为第j帧的点。&lt;/p&gt;
&lt;h3 id=&#34;2-根据估算得到的xi_ij选择重叠率高的scan-pair应用tricp算法求解其relative-motion-m_ij&#34;&gt;2. 根据估算得到的${\xi_{i,j}}$，选择重叠率高的&lt;code&gt;scan pair&lt;/code&gt;，应用TrICP算法求解其&lt;code&gt;relative Motion&lt;/code&gt; $M_{i,j}$&lt;/h3&gt;
&lt;h3 id=&#34;2-应用运动平均算法&#34;&gt;2. 应用运动平均算法&lt;/h3&gt;
&lt;p&gt;在应用&lt;code&gt;Motion Average&lt;/code&gt;前，我们已经有了初始的&lt;code&gt;Global Motion&lt;/code&gt;以及一系列&lt;code&gt;Relative Motion&lt;/code&gt;。&lt;br&gt;
运动平均的主要思想是，将&lt;code&gt;relative Motion&lt;/code&gt;看作是&lt;code&gt;global Motion&lt;/code&gt;的某种组合。先求出$\Delta M_{i,j}$，将其转换为李代数对应的&lt;code&gt;6x1&lt;/code&gt;的向量。然后通过Average的思想，求出&lt;code&gt;global Motion&lt;/code&gt;的变化值。&lt;/p&gt;
&lt;h4 id=&#34;21-计算relative-motion-m_ij的变化值&#34;&gt;2.1 计算&lt;code&gt;relative motion&lt;/code&gt; $M_{i,j}$的变化值&lt;/h4&gt;
&lt;p&gt;通过&lt;code&gt;global motion&lt;/code&gt;，可以求出$\Delta M_{i,j}$
$$
\Delta M_{i,j} = M_i^0 M_{i,j} {(M_j^0)}^{-1}
$$&lt;/p&gt;
&lt;h4 id=&#34;22-转换为李代数从se4-到-se4&#34;&gt;2.2 转换为李代数，从SE(4) 到 se(4)&lt;/h4&gt;
&lt;p&gt;$$
\Delta m_{i,j} = logm(\Delta M_{i,j})
$$&lt;/p&gt;
&lt;h4 id=&#34;23-求李代数上变化量的均值&#34;&gt;2.3 求李代数上变化量的均值&lt;/h4&gt;
&lt;p&gt;首先先使用映射将其李代数映射为其对应的六元向量：
$$
\Delta v_{i,j} = vec(\Delta m_{i,j})
$$&lt;/p&gt;
&lt;p&gt;然后构造一个矩阵D，应用如下公式即可求出&lt;code&gt;global motion&lt;/code&gt;的Average值。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig5.png&#34; alt=&#34;image&#34;  /&gt;&lt;br&gt;
$D_{i,j}$在第i列是$-I$，在第j列为$I$。&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;D的含义可能比较难理解，我们有：&lt;br&gt;
$$
\Delta \zeta = D^{\dag} \Delta V_{i,j}&lt;br&gt;
$$
$$
D \Delta \zeta = \Delta V_{i,j}
$$
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig3.png&#34; alt=&#34;image&#34;  /&gt;
该符号打不出来，我们用$\Delta \zeta$代替了。&lt;/p&gt;
&lt;p&gt;举个例子：
一共3个scan。relativeMotion有两个，假设为$\Delta v_{1,2}, \Delta v_{1,3}$。&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig4.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;可以看出来，通过D对$\Delta \zeta$的线性变换，的确得到了$\Delta v_{i,j}$的另一种表现形式，所以对D求伪逆，左乘在$V_{i,j}$旁，即可得到包含有&lt;code&gt;global motion&lt;/code&gt;的结果。&lt;/p&gt;
&lt;h4 id=&#34;24-根据求得的global-motion-difference变换回se4对现有motion进行更新&#34;&gt;2.4 根据求得的&lt;code&gt;global motion difference&lt;/code&gt;，变换回SE(4)，对现有Motion进行更新。&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig6.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h4 id=&#34;25-判断是否满足条件若不满足重新回到21-若满足退出&#34;&gt;2.5 判断是否满足条件，若不满足重新回到2.1， 若满足退出。&lt;/h4&gt;
&lt;p&gt;条件：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MATrICP_fig7.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>np.transpose()详解</title>
        <link>https://codefmeister.github.io/p/np.transpose%E8%AF%A6%E8%A7%A3/</link>
        <pubDate>Sat, 21 Nov 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/np.transpose%E8%AF%A6%E8%A7%A3/</guid>
        <description>&lt;h2 id=&#34;ndarray的转置transpose&#34;&gt;ndarray的转置(transpose)&lt;/h2&gt;
&lt;p&gt;对于A是由&lt;code&gt;np.ndarray&lt;/code&gt;表示的情况：&lt;br&gt;
可以直接使用命令&lt;code&gt;A.T&lt;/code&gt;。&lt;br&gt;
也可以使用命令&lt;code&gt;A.transpose()&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;at-与-atranspose对比&#34;&gt;A.T 与 A.transpose()对比&lt;/h3&gt;
&lt;h4 id=&#34;结论&#34;&gt;&lt;strong&gt;结论&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;在默认情况下，两者效果相同，但&lt;code&gt;transpose()&lt;/code&gt;可以指定交换的&lt;code&gt;axis&lt;/code&gt;维度。&lt;br&gt;
对于一维数组，两者均不改变，返回原数组。&lt;br&gt;
对于二维数组，默认进行标准的转置操作。&lt;br&gt;
对于多维数组&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;A.shape&lt;/code&gt;为&lt;code&gt;(a,b,c,d,...,n)&lt;/code&gt;，则转置后的&lt;code&gt;shape&lt;/code&gt;为&lt;code&gt;(n,...,d,c,b,a)&lt;/code&gt;。&lt;br&gt;
对于&lt;code&gt;.transpose()&lt;/code&gt;，可以指定转置后的维度。语法：&lt;code&gt;A.transpose((axisOrder1,...,axisOrderN))&lt;/code&gt;，其效果等同于&lt;code&gt;np.transpose(A,(axisOrder1,...,axisOrderN))&lt;/code&gt;,&lt;code&gt;(axisOrder)&lt;/code&gt;中是想要得到的索引下标顺序。效果详见例子。&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example：&lt;/h4&gt;
&lt;h5 id=&#34;二维默认情况下&#34;&gt;二维默认情况下：&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;A = np.array([[1,2],[3,4]])
print(A)
print(A.T)
print(A.transpose())
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如下：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/transpose_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h5 id=&#34;多维默认情况下&#34;&gt;多维默认情况下：&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]])
print(a.shape)
print(a.T.shape)
print(a.transpose().shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如下：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/transpose_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h5 id=&#34;指定维度情况&#34;&gt;指定维度情况：&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]])
print(a.shape)
print(a.transpose(1,2,0).shape)
A = np.transpose(a,(1,2,0))
print(A.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如下：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/transpose_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;从截图中可以看出，&lt;code&gt;a.transpose(1,2,0)&lt;/code&gt;与&lt;code&gt;np.transpose(a,(1,2,0))&lt;/code&gt;效果相同。代码段中给出的&lt;code&gt;axes&lt;/code&gt;是&lt;code&gt;(1,2,0)&lt;/code&gt;，这决定了&lt;code&gt;transpose&lt;/code&gt;后的数组，其&lt;code&gt;shape&lt;/code&gt;在第一个维度即&lt;code&gt;shape[0]&lt;/code&gt;上是原来的&lt;code&gt;shape[1]&lt;/code&gt;，第二维&lt;code&gt;shape[1]&lt;/code&gt;是原来的&lt;code&gt;shape[2]&lt;/code&gt;，第三维&lt;code&gt;shape[2]&lt;/code&gt;是原来的&lt;code&gt;shape[0]&lt;/code&gt;。所以原&lt;code&gt;shape&lt;/code&gt;为&lt;code&gt;(3,2,4)&lt;/code&gt;。新的shape为&lt;code&gt;(2,4,3)&lt;/code&gt;。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>np.unique()解读</title>
        <link>https://codefmeister.github.io/p/np.unique%E8%A7%A3%E8%AF%BB/</link>
        <pubDate>Sat, 21 Nov 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/np.unique%E8%A7%A3%E8%AF%BB/</guid>
        <description>&lt;h1 id=&#34;npunique官方文档分析以及举例&#34;&gt;np.unique()官方文档分析以及举例&lt;/h1&gt;
&lt;h2 id=&#34;11-官方文档及解读&#34;&gt;1.1 官方文档及解读&lt;/h2&gt;
&lt;h3 id=&#34;numpyunique&#34;&gt;&lt;code&gt;numpy.unique&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;A003ED8BB5294B16AF990EB1F9B1BF60&#34; alt=&#34;image&#34;  /&gt;
&lt;strong&gt;语法&lt;/strong&gt;：&lt;code&gt;numpy.unique(ar, return_index=False, return_inverse=False, return_counts=False, axis=None)&lt;/code&gt;&lt;br&gt;
&lt;strong&gt;作用&lt;/strong&gt;：找到array中不重复（独一无二）的元素&lt;br&gt;
&lt;strong&gt;返回值&lt;/strong&gt;：默认返回不重复元素的&lt;code&gt;sorted&lt;/code&gt;排好序的从小到大的数组。可选的返回值有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入数组提供不重复值(unique)元素的索引下标(如果有多个返回第一个)&lt;/li&gt;
&lt;li&gt;利用unique数组重构原有的input数组所需要的的索引下标&lt;/li&gt;
&lt;li&gt;该unique元素在input数组中的出现次数，相当于count&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;E8161EFDE0B546778DF12C0EC6DFB3FE&#34; alt=&#34;image&#34;  /&gt;
&lt;strong&gt;Parameter&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ar&lt;/code&gt;：array like&lt;br&gt;
输入的数组，除非特别指定&lt;code&gt;axis&lt;/code&gt;，数组将被展平为1-D形式进行处理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;return_index&lt;/code&gt;: bool, optional&lt;br&gt;
如果为True，返回输入数组提供不重复值(unique)元素的索引下标(如果有多个返回第一个)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;return_inverse&lt;/code&gt;: bool, optional&lt;br&gt;
如果为True，返回利用unique数组重构原有的input数组所需要的的索引下标&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;axis&lt;/code&gt;: int or None, optional&lt;br&gt;
进行操作的维度。如果为None，数组将被展平作为一维数组处理，如果指定了axis，则以该维索引构成的子数组作为元素，将整个数组视为一维数组进行处理。如果&lt;code&gt;axis&lt;/code&gt;被使用，则不支持&lt;code&gt;Object Array&lt;/code&gt;以及&lt;code&gt;structured arrays&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;4CE1F37640064C9383FA56F3254EF153&#34; alt=&#34;image&#34;  /&gt;
&lt;strong&gt;Returns&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;unique&lt;/code&gt;: ndarray&lt;br&gt;
排好序(从小到大)的unique值&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;unique_indices&lt;/code&gt;: ndarray, optional&lt;br&gt;
unique数组中对应位置的value值第一次在input数组中出现的下标值。当&lt;code&gt;return_index = True&lt;/code&gt;时返回。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;unique_inverse&lt;/code&gt;: ndarray, optional&lt;br&gt;
利用unique数组重构源输入input数组所需要的索引下标。当&lt;code&gt;return_inverse = True&lt;/code&gt;的时候返回。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;unique_count&lt;/code&gt;: ndarray, optional
每个&lt;code&gt;unique values&lt;/code&gt;在原数组中出现的次数，当&lt;code&gt;return_counts=True&lt;/code&gt;时返回。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;F7C2B3A94376444EA1511ECA7521E9B6&#34; alt=&#34;image&#34;  /&gt;
&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当axis被指定时，&lt;code&gt;subarrays&lt;/code&gt;按照指定轴的维度索引。该操作通过将&lt;code&gt;specified axis&lt;/code&gt;挪到数组的第一维，然后再将&lt;code&gt;subarrays&lt;/code&gt;展开。被展平的&lt;code&gt;subarrays&lt;/code&gt;被视为一个结构化类型，我们就可以把以这个结构化类型将原数组视为一个1维数组。结果排序是按结构化元素的第一个元素（类似字典序）排列的。&lt;/p&gt;
&lt;h2 id=&#34;12-examples&#34;&gt;1.2 Examples&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;D0A339CD11C549EA8CF24995F7B14DED&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;补一个return_counts:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    &amp;gt;&amp;gt;&amp;gt; a = np.array([1,1,2,2,2,3,8,5,4])
    &amp;gt;&amp;gt;&amp;gt; x,x2 = np.unique(a,return_counts = True)
    x: [1,2,3,4,5,8]
    x2:[2,3,1,1,1,1,dtype=int64]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;E53F32B7636B466C9AC5C0EE24AC6805&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>numpy求解范数--numpy.linalg.norm</title>
        <link>https://codefmeister.github.io/p/numpy%E6%B1%82%E8%A7%A3%E8%8C%83%E6%95%B0-numpy.linalg.norm/</link>
        <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/numpy%E6%B1%82%E8%A7%A3%E8%8C%83%E6%95%B0-numpy.linalg.norm/</guid>
        <description>&lt;h1 id=&#34;numpylinalgnorm&#34;&gt;numpy.linalg.norm&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;numpy.linalg.norm(x,ord=None,axis=None,keepdims=False)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;x: array_like&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Input array. If &lt;code&gt;axis&lt;/code&gt; is None, x must be 1-D or 2-D, unless &lt;code&gt;ord&lt;/code&gt; is None. If both &lt;code&gt;axis&lt;/code&gt; and &lt;code&gt;ord&lt;/code&gt; are None, the 2-norm of &lt;code&gt;x.ravel&lt;/code&gt; will be returned.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;X是输入的array, array的情况必须是以下三种情况之一:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;axis&lt;/code&gt;未指定，&lt;code&gt;ord&lt;/code&gt;指定。此时x必须是一维或二维数组&lt;/li&gt;
&lt;li&gt;&lt;code&gt;axis&lt;/code&gt;指定，&lt;code&gt;x&lt;/code&gt;任意&lt;/li&gt;
&lt;li&gt;&lt;code&gt;axis&lt;/code&gt;未指定，&lt;code&gt;ord&lt;/code&gt;未指定，此时&lt;code&gt;x&lt;/code&gt;任意，返回值为x被展平后的一维向量&lt;code&gt;x.ravel&lt;/code&gt;的二范数。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ord：{non-zero int, inf, -inf, &amp;lsquo;fro&amp;rsquo;, &amp;lsquo;nuc&amp;rsquo;}, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Order of the norm (see table under Notes). inf means numpy&amp;rsquo;s inf object. The default is None.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;范数的阶数，可以不指定。默认为None。inf代表无穷大，-inf为无穷小。&lt;br&gt;
可选的阶数见下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/NumpyNorm_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;axis:{None, int, 2-tuple of ints},optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If &lt;code&gt;axis&lt;/code&gt; is an integer, it specifies the &lt;code&gt;axis&lt;/code&gt; of x along which to compute the vector norms. If &lt;code&gt;axis&lt;/code&gt; is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果&lt;code&gt;axis&lt;/code&gt;是整数，指定了一个维度，在该维度上按照向量进行范数计算。如果是一个二元整数组，指定了两个维度，在指定的这两个维度上可以构成矩阵。对这些矩阵进行计算。如果没有指定&lt;code&gt;axis&lt;/code&gt;,那么对于一维输入返回其向量形式的范数计算值，对于二维输入返回其矩阵形式的范数。默认值为None&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;keepdims: bool, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果&lt;code&gt;keepdims=True&lt;/code&gt;，被指定计算范数的维度将在返回结果中保留，其size为1。计算结果会在该维度上进行&lt;code&gt;broadcast&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;各范数详析&#34;&gt;各范数详析&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: 对于&lt;code&gt;ord&amp;lt;1&lt;/code&gt;的各个范数，结果在严格意义不等于数学意义上的范数。但在数值计算层面仍然有效。
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/NumpyNorm_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;默认情况&#34;&gt;默认情况&lt;/h3&gt;
&lt;p&gt;当不指定ord时，即&lt;code&gt;ord = None&lt;/code&gt;，对于矩阵，计算其&lt;code&gt;Frobenius norm&lt;/code&gt;，对于向量，计算其&lt;code&gt;2-norm&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;frobenius范数&#34;&gt;Frobenius范数&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ord = &#39;fro&#39;&lt;/code&gt;&lt;br&gt;
其公式为：&lt;br&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/NumpyNorm_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;br&gt;
F范数只对矩阵存在。其值为对所有元素的绝对值的平方求和后开平方。&lt;/p&gt;
&lt;h3 id=&#34;nuclear范数核范数&#34;&gt;Nuclear范数(核范数)&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ord = &#39;nuc&#39;&lt;/code&gt;
只对矩阵存在，矩阵的核范数等于其所有奇异值的和。&lt;/p&gt;
&lt;h3 id=&#34;无穷大范数&#34;&gt;无穷大范数&lt;/h3&gt;
&lt;p&gt;对于矩阵：&lt;code&gt;max(sum(abs(x), axis=1))&lt;/code&gt; ,每一行最终得到一个数，返回最大的数。&lt;br&gt;
对于向量：&lt;code&gt;max(abs(x)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;无穷小范数&#34;&gt;无穷小范数&lt;/h3&gt;
&lt;p&gt;对于矩阵: &lt;code&gt;min(sum(abs(x),axis=1))&lt;/code&gt;,每一行得到一个数，返回最小的数。&lt;br&gt;
对于向量: &lt;code&gt;min(abs(x))&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;0-范数&#34;&gt;0 范数&lt;/h3&gt;
&lt;p&gt;对于矩阵：不存在&lt;br&gt;
对于向量：&lt;code&gt;sum(x!=0)&lt;/code&gt;  所有非零元素的和&lt;/p&gt;
&lt;h3 id=&#34;1-范数&#34;&gt;1 范数&lt;/h3&gt;
&lt;p&gt;对于矩阵：&lt;code&gt;max(sum(abs(x)),axis=0&lt;/code&gt;，每一列得到一个数，返回最大值。&lt;br&gt;
对于向量：&lt;code&gt;sum(abs(x)**ord)**(1./ord)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;-1-范数&#34;&gt;-1 范数&lt;/h3&gt;
&lt;p&gt;对于矩阵：&lt;code&gt;min(sum(abs(x)),axis=0&lt;/code&gt;，每一列得到一个数，返回最小值。&lt;br&gt;
对于向量：&lt;code&gt;sum(abs(x)**ord)**(1./ord)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-范数&#34;&gt;2 范数&lt;/h3&gt;
&lt;p&gt;对于矩阵：最大的奇异值&lt;br&gt;
对于向量：&lt;code&gt;sum(abs(x)**ord)**(1./ord)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;-2范数&#34;&gt;-2范数&lt;/h3&gt;
&lt;p&gt;对于矩阵：最小的奇异值&lt;br&gt;
对于向量：&lt;code&gt;sum(abs(x)**ord)**(1./ord)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;其余int值对应的范数&#34;&gt;其余int值对应的范数&lt;/h3&gt;
&lt;p&gt;对于矩阵： Undefined&lt;br&gt;
对于向量：&lt;code&gt;sum(abs(x)**ord)**(1./ord)&lt;/code&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>parse_args传参</title>
        <link>https://codefmeister.github.io/p/parse_args%E4%BC%A0%E5%8F%82/</link>
        <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/parse_args%E4%BC%A0%E5%8F%82/</guid>
        <description>&lt;h1 id=&#34;python中parse_args以及namespace&#34;&gt;&lt;code&gt;python&lt;/code&gt;中&lt;code&gt;parse_args&lt;/code&gt;以及&lt;code&gt;namespace&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;声明&lt;/strong&gt;：本笔记记录的是使用parse_args在函数内部进行传参，并非在命令行进行输入。所有操作均需先进行&lt;code&gt;import argparse&lt;/code&gt;。&lt;br&gt;
通过以下操作，可以在传参时直接传入&lt;code&gt;args&lt;/code&gt;这个&lt;code&gt;namespace&lt;/code&gt;，而不是具体的某个参数。&lt;/p&gt;
&lt;h2 id=&#34;创建argparse对象设置参数以及默认值&#34;&gt;创建&lt;code&gt;argparse&lt;/code&gt;对象，设置参数以及默认值&lt;/h2&gt;
&lt;p&gt;使用&lt;code&gt;argparse.ArgumentParser()&lt;/code&gt;创建对象，使用&lt;code&gt;argparser.add_argument()&lt;/code&gt;操作设置参数以及默认值。&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;parser = argparse.ArgumentParser(&#39;Exampe&#39;)
parser.add_argument(&#39;--NDArray&#39;,type=np.ndarray,default= NDArray)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;argument&lt;/code&gt;中的参数类型丰富多样，可以是任何数据类型。使用&lt;code&gt;--name&lt;/code&gt;来设置参数名，使用&lt;code&gt;type = &lt;/code&gt;设置类型，使用&lt;code&gt;default&lt;/code&gt;设置初始化后的默认值。&lt;/p&gt;
&lt;h2 id=&#34;parser转换为name_space&#34;&gt;parser转换为name_space&lt;/h2&gt;
&lt;p&gt;使用命令&lt;code&gt;parser.parse_args()&lt;/code&gt;，即可将一个&lt;code&gt;ArgumentParser&lt;/code&gt;转换为&lt;code&gt;name_space&lt;/code&gt;.转换为&lt;code&gt;namespace&lt;/code&gt;后，可以对先前设置的&lt;code&gt;argument&lt;/code&gt;通过&lt;code&gt;.name&lt;/code&gt;的方式类似属性一样进行访问，同样可以进行&lt;strong&gt;赋值&lt;/strong&gt;，&lt;strong&gt;存取&lt;/strong&gt;等操作。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;args = parser.parse_args()
print(args.NDArray)
a = np.array([[1,2],[3,4]])
args.NDArray = a
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;namespace-的一些操作&#34;&gt;namespace 的一些操作&lt;/h2&gt;
&lt;p&gt;在初始化ArgumentParser时，我们可能忘记添加某些&lt;code&gt;argument&lt;/code&gt;，这就导致在转换为&lt;code&gt;namespace&lt;/code&gt;后缺少某些&lt;code&gt;attribute&lt;/code&gt;.&lt;br&gt;
我们可以对&lt;code&gt;args&lt;/code&gt;使用&lt;code&gt;.__setattr(name,value)&lt;/code&gt;设置新的属性值。开辟之后就可以使用&lt;code&gt;.attr&lt;/code&gt;的方式进行赋值存取。
同样，我们可以使用&lt;code&gt;.__contains__(attribute_name)&lt;/code&gt;判断&lt;code&gt;args&lt;/code&gt;这个&lt;code&gt;namespace&lt;/code&gt;是否含有该属性。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;args.__setattr__(&#39;cloudArray&#39;,[])
args.cloudArray.append(1024)
if args.__contains__(&#39;cloudArray&#39;):
    print(&#39;namespace args contains attr&#39;)
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>torch.optim解读</title>
        <link>https://codefmeister.github.io/p/torch.optim%E8%A7%A3%E8%AF%BB/</link>
        <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/torch.optim%E8%A7%A3%E8%AF%BB/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;本文参考
&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qyhaill/article/details/103043637&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;lr_scheduler介绍&lt;/a&gt; 以及
&lt;a class=&#34;link&#34; href=&#34;https://pytorch.org/docs/stable/optim.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PyTorch optim文档&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;1-概述&#34;&gt;1 概述&lt;/h1&gt;
&lt;h2 id=&#34;11-pytorch文档torchoptim解读&#34;&gt;1.1 PyTorch文档：torch.optim解读&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;下图是optim的文档&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;torchoptim&#34;&gt;TORCH.OPTIM&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;torch.optim&lt;/code&gt; is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;torch.optim简介&lt;/strong&gt;&lt;br&gt;
torch.optim是PyTorch实现的一个包，里面有各种各样的优化算法，大部分常用的优化算法都已经被支持，接口也十分通用，所以可以用来集成实现更加复杂的系统。&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;how-to-use-an-optimizer&#34;&gt;How to use an optimizer&lt;/h4&gt;
&lt;p&gt;To use &lt;code&gt;torch.optim&lt;/code&gt; you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;如何使用PyTorch提供的optimizer&lt;/strong&gt;&lt;br&gt;
通过&lt;code&gt;torch.optim&lt;/code&gt;来创建一个&lt;code&gt;Optimizer&lt;/code&gt;对象，这个对象中会保存当前的状态，并且会根据计算的梯度值更新参数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;constructing-it&#34;&gt;Constructing it&lt;/h4&gt;
&lt;p&gt;To construct an &lt;code&gt;Optimizer&lt;/code&gt; you have to give it an iterable containing the parameters (all should be &lt;code&gt;Variables&lt;/code&gt;) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h5 id=&#34;note&#34;&gt;NOTE&lt;/h5&gt;
&lt;p&gt;If you need to move a model to GPU via &lt;code&gt;.cuda()&lt;/code&gt;, please do so before constructing optimizers for it. Parameters of a model after &lt;code&gt;.cuda()&lt;/code&gt; will be different objects with those before the call.     &lt;br&gt;
In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;构造Optimizer&lt;/strong&gt;&lt;br&gt;
构造&lt;code&gt;Optimizer&lt;/code&gt;时，需要传入一个包含需要进行优化的所有参数的&lt;code&gt;iterable&lt;/code&gt;对象，所有参数都必须是&lt;code&gt;Variables&lt;/code&gt;类型。随后可以进一步设置optimizer的其他具体参数，如learning rate, weight decay, etc.&lt;br&gt;
&lt;strong&gt;注意&lt;/strong&gt;:&lt;br&gt;
如果需要将模型移到cuda上(通过&lt;code&gt;.cuda&lt;/code&gt;命令)，那么必须先移动模型，再对模型构造&lt;code&gt;optimizer&lt;/code&gt;。因为调用&lt;code&gt;.cuda&lt;/code&gt;前的模型参数与调用&lt;code&gt;.cuda&lt;/code&gt;后的模型参数不同。&lt;br&gt;
通常来讲，在使用&lt;code&gt;Optimizer&lt;/code&gt;对参数进行优化时，需要保证构造和使用时，被优化的参数保存在同一位置。&lt;/p&gt;
&lt;p&gt;以下是实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;per-parameter-options&#34;&gt;Per-parameter options&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Optimizer&lt;/code&gt;s also support specifying per-parameter options. To do this, instead of passing an iterable of &lt;code&gt;Variable&lt;/code&gt;s, pass in an iterable of &lt;code&gt;dict&lt;/code&gt;s. Each of them will define a separate parameter group, and should contain a params key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h5 id=&#34;note-1&#34;&gt;NOTE&lt;/h5&gt;
&lt;p&gt;You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn&amp;rsquo;t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Per-parameter option&lt;/strong&gt;&lt;br&gt;
我个人翻译为逐参数选项。&lt;code&gt;Optimizer&lt;/code&gt;在构造的时候同样支持对每个参数进行指定。要实现这种功能，我们不再传入一个含有&lt;code&gt;Variable&lt;/code&gt;类型参数的&lt;code&gt;iterable&lt;/code&gt;对象，而是传入一个&lt;code&gt;dict&lt;/code&gt;字典类型的&lt;code&gt;iterable&lt;/code&gt;对象。每个字典都定义了一个参数组，该参数组的key值是&amp;quot;params&amp;quot;，而对应的值为一个包含参数的列表。同样的可以利用字典的键值对&lt;code&gt;Optimizer&lt;/code&gt;的其他参数进行指定，但是key必须与&lt;code&gt;Optimizer&lt;/code&gt;构造器传参时的关键字一致。这些指定的&lt;code&gt;Optimizer&lt;/code&gt;的参数会被单独应用于该字典中的&lt;code&gt;params&lt;/code&gt;这些参数。&lt;br&gt;
&lt;strong&gt;注意&lt;/strong&gt;：你仍然可以在构造器中以关键字方式传入参数，这些参数将被当做默认值使用，如果一组参数没有override这个参数，那么就将自动使用默认的参数值。&lt;br&gt;
以下是实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, this is very useful when one wants to specify per-layer learning rates:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;optim.SGD([
               {&#39;params&#39;: model.base.parameters()},
               {&#39;params&#39;: model.classifier.parameters(), &#39;lr&#39;: 1e-3}
           ], lr=1e-2, momentum=0.9)
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This means that model.base&amp;rsquo;s parameters will use the default learning rate of 1e-2, model.classifier&amp;rsquo;s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于上面这个例子，首先我们可以看到，传入了一个dict的列表。列表中有两个dict，第一个的&lt;code&gt;params&lt;/code&gt;key 对应的是&lt;code&gt;model.base.parameters()&lt;/code&gt;，而没有对&lt;code&gt;Optimizer&lt;/code&gt;的其他参数进行具体指定。第二个dict的&lt;code&gt;params&lt;/code&gt;key对应的是&lt;code&gt;model.classifier.parameters()&lt;/code&gt;，此外还有一个键值对，说明了&lt;code&gt;lr&lt;/code&gt;的值为1e-3。而在列表之外同时又传入了&lt;code&gt;lr=1e-2&lt;/code&gt;,&lt;code&gt;momentum=0.9&lt;/code&gt;，这两个值将作为默认值来使用。所以整个&lt;code&gt;Optimizer&lt;/code&gt;中，&lt;code&gt;base&#39;s parameters&lt;/code&gt;将使用默认的学习率&lt;code&gt;1e-2&lt;/code&gt;,默认的动量超参数&lt;code&gt;0.9&lt;/code&gt;;而&lt;code&gt;classifier.parameters()&lt;/code&gt;将使用其dict中提供的学习率&lt;code&gt;1e-3&lt;/code&gt;,&lt;code&gt;momentum&lt;/code&gt;仍然使用默认值。&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;taking-an-optimization-step&#34;&gt;Taking an optimization step&lt;/h4&gt;
&lt;p&gt;All optimizers implement a &lt;code&gt;step()&lt;/code&gt; method, that updates the parameters. It can be used in two ways:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;optimizerstep&#34;&gt;&lt;strong&gt;&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. backward().&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;采取优化步骤&lt;/strong&gt;&lt;br&gt;
所有的Optimizer都实现了&lt;code&gt;step()&lt;/code&gt;方法，该方法可以用于更新参数。可以通过两种方式使用&lt;code&gt;.step()&lt;/code&gt;进行优化：&lt;/p&gt;
&lt;p&gt;第一种方式：&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;br&gt;
该方法是一个简化后的版本，被大多数optimizer所支持。该函数一般在所有梯度值被更新（或者被计算）后进行调用,如在&lt;code&gt;.backward()&lt;/code&gt;后进行调用。&lt;/p&gt;
&lt;p&gt;以下是例子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for input, target in dataset:
   optimizer.zero_grad()
   output = model(input)
   loss = loss_fn(output, target)
   loss.backward()
   optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;optimizerstepclosure&#34;&gt;&lt;code&gt;optimizer.step(closure)&lt;/code&gt;&lt;/h4&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第二种方式：&lt;code&gt;optimizer.step(closure)&lt;/code&gt;&lt;br&gt;
有一些优化算法例如&lt;code&gt;Conjugate Gradient&lt;/code&gt;，&lt;code&gt;LBFGS&lt;/code&gt;等需要多次重新计算函数，所以需要传入一个闭包&lt;code&gt;closure&lt;/code&gt;，闭包中应该实现的操作有：清零梯度，计算损失并返回。&lt;br&gt;
以下是例子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for input, target in dataset:
   def closure():
       optimizer.zero_grad()
       output = model(input)
       loss = loss_fn(output, target)
       loss.backward()
       return loss
   optimizer.step(closure)
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;具体的各个优化算法的数学原理在此不表，详参手写的笔记本。&lt;/p&gt;
&lt;h1 id=&#34;2-如何调整学习率&#34;&gt;2 如何调整学习率&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;torch.optim.lr_scheduler&lt;/code&gt;模块，提供了一些根据训练次数来调整学习率(learning rate)的方法，一般情况下我们会设置随着epoch的增大而逐渐减小学习率，从而达到更好的训练效果。
而&lt;code&gt;torch.optim.lr_scheduler.ReduceLROnPlateau&lt;/code&gt;提供了一些基于训练中某些测量值使得学习率动态下降的办法。&lt;/p&gt;
&lt;p&gt;学习率的调整应该放在optimizer更新之后，参考模板：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;define scheduler
for epoch in range(1000):
    train(...)
    validate(...)
    scheduler.step()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;： 在PyTorch 1.1.0之前的版本，学习率的调整应该被放在optimizer更新之前，如果我们1.1.0之后仍然将学习率的调整(即&lt;code&gt;scheduler.step()&lt;/code&gt;)放在optimizer&amp;rsquo;s update(即&lt;code&gt;optimizer.step&lt;/code&gt;)之前，那么learning rate schedule的第一个值将被跳过。所以如果某个代码是在1.1.0之前的版本开发，移植到高版本进行运行，发现效果变差，可以检查是否将&lt;code&gt;scheduler.step()&lt;/code&gt;放在了&lt;code&gt;optimizer.step()&lt;/code&gt;之前。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：以上部分参考官方文档批示。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;21-torchoptimlr_schedulersteplr&#34;&gt;2.1 &lt;code&gt;torch.optim.lr_scheduler.StepLR&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;首先贴上官方文档：&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&#34;torchoptimlr_schedulersteplroptimizer-step_size-gamma01-last_epoch-1-verbosefalse&#34;&gt;&lt;code&gt;torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;StepLR&lt;/code&gt;可以根据超参数&lt;code&gt;gamma&lt;/code&gt;每隔固定的&lt;code&gt;step_size&lt;/code&gt;就衰减&lt;code&gt;learning_rate&lt;/code&gt;一次。需要说明的是，这种对&lt;code&gt;learning_rate&lt;/code&gt;的更新可以与外界的其他变化同时进行。当&lt;code&gt;last_epoch = -1&lt;/code&gt;时，将lr置为初始值。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Parameters&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;optimizer (Optimizer) – Wrapped optimizer.&lt;/li&gt;
&lt;li&gt;step_size (int) – Period of learning rate decay.&lt;/li&gt;
&lt;li&gt;gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.&lt;/li&gt;
&lt;li&gt;last_epoch (int) – The index of last epoch. Default: -1.&lt;/li&gt;
&lt;li&gt;verbose (bool) – If True, prints a message to stdout for each update. Default: False.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;参数说明&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;optimizer(Optimizer) &amp;mdash;&amp;ndash;用于指定scheduler的应用对象。&lt;/li&gt;
&lt;li&gt;step_size(int)&amp;mdash;&amp;ndash;用于指定步长，即几次迭代之后进行一次decay&lt;/li&gt;
&lt;li&gt;gamma(float)&amp;mdash;&amp;ndash;学习率衰减的乘法因子，默认值为0.1&lt;/li&gt;
&lt;li&gt;last_epoch(int)&amp;mdash;&amp;ndash;更新的边界index，当等于这个值的时候，重置lr，默认为-1&lt;/li&gt;
&lt;li&gt;verbose(bool)&amp;mdash;&amp;ndash;如果为True，每次decay会向stdout输出一条信息。默认为false.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Assuming optimizer uses lr = 0.05 for all groups
# lr = 0.05     if epoch &amp;lt; 30
# lr = 0.005    if 30 &amp;lt;= epoch &amp;lt; 60
# lr = 0.0005   if 60 &amp;lt;= epoch &amp;lt; 90
# ...
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;可见:每经过一个&lt;code&gt;step_size&lt;/code&gt;，&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;lr = lr*gamma 
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>MatrixCookBook Chapter1:矩阵的基础知识</title>
        <link>https://codefmeister.github.io/p/matrixcookbook-chapter1%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link>
        <pubDate>Sun, 15 Nov 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/matrixcookbook-chapter1%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid>
        <description>&lt;h1 id=&#34;矩阵的基础知识转置逆迹行列式&#34;&gt;矩阵的基础知识（转置，逆，迹，行列式）&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;References: &lt;code&gt;MatrixCookBook(Version 2012)  Chapter1&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;chapter1-basics&#34;&gt;Chapter1: Basics&lt;/h2&gt;
&lt;h3 id=&#34;1-basics&#34;&gt;1 Basics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MatrixCookBookChapter1_fig1.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：${A^H}$是A的&lt;code&gt;Transposed and complex conjugated matrix (Hermitian)&lt;/code&gt;，即转置复共轭矩阵。&lt;/p&gt;
&lt;h3 id=&#34;11-矩阵的迹trace&#34;&gt;1.1 矩阵的迹(&lt;code&gt;Trace&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MatrixCookBookChapter1_fig2.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;式子&lt;code&gt;(11)&lt;/code&gt;表明矩阵的迹是主对角线元素的和。&lt;br&gt;
式子&lt;code&gt;(12)&lt;/code&gt;表明矩阵的迹是矩阵的特征值的和。&lt;br&gt;
式子&lt;code&gt;(13)&lt;/code&gt;表明矩阵的迹等于其转置矩阵的迹。&lt;br&gt;
式子&lt;code&gt;(14)&lt;/code&gt;表明&lt;code&gt;AB&lt;/code&gt;的迹等于&lt;code&gt;BA&lt;/code&gt;的迹。&lt;br&gt;
式子&lt;code&gt;(15)&lt;/code&gt;表明&lt;code&gt;A+B&lt;/code&gt;的迹等于&lt;code&gt;A&lt;/code&gt;的迹加&lt;code&gt;B&lt;/code&gt;的迹。
式子&lt;code&gt;(16)&lt;/code&gt;表明&lt;code&gt;ABC&lt;/code&gt;的迹等于&lt;code&gt;BCA&lt;/code&gt;的迹等于&lt;code&gt;CAB&lt;/code&gt;的迹。&lt;br&gt;
式子&lt;code&gt;(17)&lt;/code&gt;表明一个&lt;code&gt;nx1&lt;/code&gt;的向量&lt;code&gt;a&lt;/code&gt;，&lt;code&gt;a&lt;/code&gt;的转置乘以&lt;code&gt;a&lt;/code&gt;所得的常数等于&lt;code&gt;a&lt;/code&gt;乘以&lt;code&gt;a&lt;/code&gt;的转置所得矩阵的迹。&lt;/p&gt;
&lt;h3 id=&#34;12-行列式determinant&#34;&gt;1.2 行列式(&lt;code&gt;Determinant&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MatrixCookBookChapter1_fig3.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前提&lt;/strong&gt;：此处的A是&lt;code&gt;nxn&lt;/code&gt;矩阵。&lt;br&gt;
式子&lt;code&gt;(18)&lt;/code&gt;表明矩阵的行列式等于特征值的连乘积。&lt;br&gt;
式子&lt;code&gt;(19)&lt;/code&gt;表明&lt;code&gt;cA&lt;/code&gt;的行列式等于&lt;code&gt;A&lt;/code&gt;的行列式的${c^n}$倍。&lt;br&gt;
式子&lt;code&gt;(20)&lt;/code&gt;表明矩阵的行列式等于其转置矩阵的行列式。&lt;br&gt;
式子&lt;code&gt;(21)&lt;/code&gt;表明矩阵&lt;code&gt;AB&lt;/code&gt;的行列式等于矩阵&lt;code&gt;A&lt;/code&gt;的行列式乘以矩阵&lt;code&gt;B&lt;/code&gt;的行列式。&lt;br&gt;
式子&lt;code&gt;(22)&lt;/code&gt;表明矩阵${A^{-1}}$的行列式等于矩阵&lt;code&gt;A&lt;/code&gt;的倒数。&lt;br&gt;
式子&lt;code&gt;(23)&lt;/code&gt;表明矩阵${A^n}$的行列式等于矩阵&lt;code&gt;A&lt;/code&gt;的行列式的n次幂。&lt;br&gt;
式子&lt;code&gt;(24)&lt;/code&gt;表明如果&lt;code&gt;u&lt;/code&gt;和&lt;code&gt;v&lt;/code&gt;是&lt;code&gt;nx1&lt;/code&gt;向量，那么${I+uv^T}$的行列式等于${1+u^Tv}$的值。&lt;br&gt;
式子&lt;code&gt;(25)&lt;/code&gt;表明如果&lt;code&gt;A&lt;/code&gt;是&lt;code&gt;2x2&lt;/code&gt;矩阵，&lt;code&gt;I+A&lt;/code&gt;的行列式等于${1+det(A)+Tr(A)}$,即1+A的行列式+A的迹。&lt;br&gt;
式子&lt;code&gt;(26)&lt;/code&gt;表明如果&lt;code&gt;A&lt;/code&gt;是&lt;code&gt;3x3&lt;/code&gt;矩阵，&lt;code&gt;I+A&lt;/code&gt;的行列式等于${1+det(A)+Tr(A)+\frac{1}{2}Tr(A)^2-\frac{1}{2}Tr(A^2)}$。&lt;br&gt;
式子&lt;code&gt;(27)&lt;/code&gt;不表。&lt;br&gt;
式子&lt;code&gt;(28)&lt;/code&gt;表示对于微小扰动$\varepsilon$，可以将$\varepsilon A$近似作为2x2形式处理：&lt;/p&gt;
&lt;h3 id=&#34;13-特例2x2矩阵&#34;&gt;1.3 特例：2x2矩阵&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/MatrixCookBookChapter1_fig4.png&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;2x2&lt;/code&gt;矩阵有着以上的性质与结论。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>model.eval()作用分析</title>
        <link>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
        <pubDate>Sat, 14 Nov 2020 09:40:49 +0800</pubDate>
        
        <guid>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
        <description>&lt;h2 id=&#34;modeleval&#34;&gt;model.eval()&lt;/h2&gt;
&lt;p&gt;model.eval() 作用等同于 self.train(False)&lt;br&gt;
简而言之，就是评估模式。而非训练模式。&lt;br&gt;
在评估模式下，&lt;code&gt;batchNorm&lt;/code&gt;层，&lt;code&gt;dropout&lt;/code&gt;层等用于优化训练而添加的网络层会被关闭，从而使得评估时不会发生偏移。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在对模型进行评估时，应该配合使用&lt;code&gt;with torch.no_grad()&lt;/code&gt; 与 &lt;code&gt;model.eval()&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    loop:
        model.train()    # 切换至训练模式
        train……
        model.eval()
        with torch.no_grad():
            Evaluation
    end loop
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;numpyarange&#34;&gt;numpy.arange()&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;x = numpy.arange(start,end,step,dtype=None)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;parameters说明&#34;&gt;Parameters说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;start&lt;/code&gt;: Optional，起始值，默认值为0。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end&lt;/code&gt;: 结束值(不含)。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step&lt;/code&gt;: Optional，步长，默认值为1。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dtype&lt;/code&gt;：Optional，默认为None，从其他输入值中推测。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;功能&#34;&gt;功能&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;[start,end)&lt;/code&gt;的&lt;strong&gt;左闭右开区间&lt;/strong&gt;内，每隔一个&lt;code&gt;step&lt;/code&gt;取一次值。return值是&lt;code&gt;ndarray&lt;/code&gt;。
对于浮点数来说，&lt;code&gt;length = ceil((end - start)/step)&lt;/code&gt;，由于浮点数的上溢，此条规则可能会导致在浮点数情况下，最后一个&lt;code&gt;element&lt;/code&gt;比&lt;code&gt;end&lt;/code&gt;长。&lt;/p&gt;
&lt;h2 id=&#34;note&#34;&gt;Note&lt;/h2&gt;
&lt;p&gt;如果使用非整数步长（譬如0.1），结果往往不一致（原因见上），所以在这种情况下推荐使用&lt;code&gt;numpy.linspace&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.arange(3)
array([0, 1, 2])

&amp;gt;&amp;gt;&amp;gt;np.arange(3.0)
array([ 0.,  1.,  2.])

&amp;gt;&amp;gt;&amp;gt;np.arange(3,7)
array([3, 4, 5, 6])

&amp;gt;&amp;gt;&amp;gt;np.arange(3,7,2)
array([3, 5])
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;numpy&#34;&gt;numpy&lt;/h1&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;numpyrepeat&#34;&gt;numpy.repeat()&lt;/h1&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;可以用于重复数组中的元素&lt;/p&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;numpy.repeat(a, repeats, axis=None)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;参数解读&#34;&gt;参数解读&lt;/h3&gt;
&lt;h4 id=&#34;parameters&#34;&gt;Parameters&lt;/h4&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;a : array_like&lt;br&gt;
Input array.&lt;/li&gt;
&lt;li&gt;repeats : int or array of ints&lt;br&gt;
The number of repetitions for each element.  &lt;code&gt;repeats&lt;/code&gt; is broadcasted
to fit the shape of the given axis.&lt;/li&gt;
&lt;li&gt;axis : int, optional&lt;br&gt;
The axis along which to repeat values.  By default, use the
flattened input array, and return a flat output array.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;a: array_like&lt;br&gt;
输入的想要进行repeat的数组&lt;/li&gt;
&lt;li&gt;repeats：int or array of ints&lt;br&gt;
&lt;code&gt;repeats&lt;/code&gt;参数应该是int类型或者是一个int数组。是对每一个元素repeat的次数。&lt;code&gt;repeats&lt;/code&gt;将被广播去适应给定&lt;code&gt;axis&lt;/code&gt;的shape。&lt;/li&gt;
&lt;li&gt;axis: int, optional&lt;br&gt;
repeat操作进行的维度，可选，int值。如果未指定，默认情况下，会将数组展平(flattened)，然后返回一个扁平的重复后的数组。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;returns&#34;&gt;Returns&lt;/h4&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;repeated_array : ndarray&lt;br&gt;
Output array which has the same shape as &lt;code&gt;a&lt;/code&gt;, except along
the given axis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;repeated_array : ndarray&lt;br&gt;
返回的repeat后的数组，除了在指定的&lt;code&gt;axis&lt;/code&gt;维度以外，其余各维度的shape与原数组&lt;code&gt;a&lt;/code&gt;一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.repeat(3, 4)
array([3, 3, 3, 3])

# 下面这个例子中，x被展平(flattened，返回的数组也是一个扁平数组)
&amp;gt;&amp;gt;&amp;gt; x = np.array([[1,2],[3,4]])
&amp;gt;&amp;gt;&amp;gt; np.repeat(x, 2)
array([1, 1, 2, 2, 3, 3, 4, 4])

# 下面这个例子指定了axis=1，axis=1的维度上的shape值为2，
# 而只给定了3一个数字，所以进行了广播，即进行的操作实际为（3，3）
&amp;gt;&amp;gt;&amp;gt; np.repeat(x, 3, axis=1)
array([[1, 1, 1, 2, 2, 2],
       [3, 3, 3, 4, 4, 4]])

# 下面这个例子指定了axis=0，同时给定了repeats数组
# 其长度等于axis=0的shape值。对第一行重复1次，对第二行重复两次。       
&amp;gt;&amp;gt;&amp;gt; np.repeat(x, [1, 2], axis=0)
array([[1, 2],
       [3, 4],
       [3, 4]])
       
# 如果给定的repeats数组长度与axis不一致，会报错，can not broadcast
&amp;gt;&amp;gt;&amp;gt; y = np.array([[1,2],[3,4],[5,6],[7,8]])
&amp;gt;&amp;gt;&amp;gt; np.repeat(y,(3,1),axis = 0)
ValueError: operands could not be broadcast together with shape (4,) (2,)

# 如果未指定axis，默认展平，但是repeats是数组，同样会报错。
&amp;gt;&amp;gt;&amp;gt; np.repeat(y,(3,1))
ValueError: operands could not be broadcast together with shape (8,) (2,)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;类似的函数&#34;&gt;类似的函数&lt;/h2&gt;
&lt;p&gt;如果repeat的功能和你想象中的不一致，那么你可能寻找的是这个函数：&lt;br&gt;
&lt;code&gt; numpy.tile(A,reps)&lt;/code&gt;&lt;br&gt;
它的功能是：用A，按照reps指定的次数拼成一个新的数组。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;numpytile&#34;&gt;numpy.tile&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;numpy.tile(A,reps)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Construct an array by repeating A the number of times given by reps.&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;reps&lt;/code&gt; has length &lt;code&gt;d&lt;/code&gt;, the result will have dimension of
&lt;code&gt;max(d, A.ndim)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;A.ndim &amp;lt; d&lt;/code&gt;, &lt;code&gt;A&lt;/code&gt; is promoted to be d-dimensional by prepending new
axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication,
or shape (1, 1, 3) for 3-D replication. If this is not the desired
behavior, promote &lt;code&gt;A&lt;/code&gt; to d-dimensions manually before calling this
function.&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;A.ndim &amp;gt; d&lt;/code&gt;, &lt;code&gt;reps&lt;/code&gt; is promoted to &lt;code&gt;A&lt;/code&gt;.ndim by pre-pending 1&amp;rsquo;s to it.
Thus for an &lt;code&gt;A&lt;/code&gt; of shape (2, 3, 4, 5), a &lt;code&gt;reps&lt;/code&gt; of (2, 2) is treated as
(1, 1, 2, 2).&lt;/p&gt;
&lt;p&gt;Note : Although tile may be used for broadcasting, it is strongly
recommended to use numpy&amp;rsquo;s broadcasting operations and functions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过按照&lt;code&gt;reps&lt;/code&gt;给定的次数，repeat数组&lt;code&gt;A&lt;/code&gt;，构造一个新的数组。&lt;br&gt;
新的数组的维度数取决于&lt;code&gt;A.ndim&lt;/code&gt;和reps的长度&lt;code&gt;d&lt;/code&gt;中最大的一个，即&lt;code&gt;max(d,A.ndim)&lt;/code&gt;&lt;br&gt;
如果&lt;code&gt;A.ndim&amp;lt;d&lt;/code&gt;，数组A会被升维到d维，通过在其前面增加new axes。如(3,)可以被升维到二维(1,3)，三维(1,1,3)。如果不希望这样的行为，那么需要在调用tile前手动升维。&lt;br&gt;
如果&lt;code&gt;A.ndim&amp;gt;d&lt;/code&gt;，reps会通过前置1的方式被“升维”到A.ndim，如A.shape为(2,3,4,5)，给定的reps为(2,2)。那么reps会被当做(1,1,2,2)处理。&lt;/p&gt;
&lt;h2 id=&#34;参数与返回值&#34;&gt;参数与返回值&lt;/h2&gt;
&lt;h3 id=&#34;parameters&#34;&gt;Parameters&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A : array_like&lt;br&gt;
The input array.&lt;/li&gt;
&lt;li&gt;reps : array_like&lt;br&gt;
The number of repetitions of &lt;code&gt;A&lt;/code&gt; along each axis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A: array_like&lt;br&gt;
输入数组A&lt;/li&gt;
&lt;li&gt;reps: array_like&lt;br&gt;
每一维度A的重复次数&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;returns&#34;&gt;Returns&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;c : ndarray&lt;br&gt;
The tiled output array.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;c : ndarray&lt;br&gt;
输出数组&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;仔细观察下面的例子，你会发现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先，返回数组的shape十分有规律，就是升维后的shape与reps的相乘。&lt;/li&gt;
&lt;li&gt;其次，返回数组十分有规律。就像贴瓷砖一样，将原数组当做瓷砖，一块一块的进行贴合。（搭积木一般）&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# a.ndim == len(reps)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# a.ndim &amp;lt; len(reps), a被升维为(1,3),返回结果为(2,6)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# a.ndim &amp;lt; len(reps), a被升维为(1,1,3)，返回结果为(2,1,6)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# b.ndim &amp;gt; len(reps),reps被作为(1,2)处理，返回结果为(2,4)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# b.ndim == len(reps), 返回结果为(4,2)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# c.ndim &amp;lt; len(reps)，c被升维为(1,4),返回结果为(4,4)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;类似的函数&#34;&gt;类似的函数&lt;/h3&gt;
&lt;p&gt;repeat : Repeat elements of an array.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://numpy.org/doc/stable/reference/generated/numpy.broadcast_to.html?highlight=broadcast_to#numpy.broadcast_to&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;broadcast_to : Broadcast an array to a new shape&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;numpyconcatenate&#34;&gt;numpy.concatenate&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;numpy.concatenate((a1,a2,...), axis=0, out=None, dtype=None, casting=&amp;quot;same_kind&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;将一个数组序列在指定的维度上进行连接&lt;code&gt;join&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;parameter&#34;&gt;Parameter&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a1,a2,&amp;hellip; : sequence of array_like&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The arrays must have the same shape, except in the dimension corresponding to &lt;code&gt;axis&lt;/code&gt; (the first, by default)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;数组序列的&lt;code&gt;shape&lt;/code&gt;在除了&lt;code&gt;axis&lt;/code&gt;指定维度以外的所有维度上都应该相同。axis默认为第一个维度，即&lt;code&gt;axis=0&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;axis : int, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The axis along which the arrays will be joined. If axis is None, arraysare flattened before use. Default is 0.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;axis&lt;/code&gt;指定了数组进行join操作的维度。默认为0，即第一维。如果&lt;code&gt;axis=None&lt;/code&gt;，那么数组将会先展平，再进行join。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;out : ndarray, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果指定了&lt;code&gt;out&lt;/code&gt;，则将join后的结果输出到&lt;code&gt;out&lt;/code&gt;指定的数组中，但&lt;code&gt;shape&lt;/code&gt;必须正确。如果没有指定&lt;code&gt;out&lt;/code&gt;，则会返回一个匹配大小的数组。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;dtype : str or dtype,optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If provided, the destination array will have this dtype. Cannot be provided together with &lt;code&gt;out&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果提供了&lt;code&gt;dtype&lt;/code&gt;，则输出的数组的数据类型会与&lt;code&gt;dtype&lt;/code&gt;中指定的一致。不可以同时与&lt;code&gt;out&lt;/code&gt;一起指定。1.20.0中新增，以前的numpy无法使用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;casting : { &amp;lsquo;no&amp;rsquo;, &amp;lsquo;equiv&amp;rsquo;, &amp;lsquo;safe&amp;rsquo;, &amp;lsquo;same_kind&amp;rsquo;, &amp;lsquo;unsafe&amp;rsquo; }, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Controls what kind of data casting may occur.Defaults to &amp;lsquo;same_kind&amp;rsquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;‘no’&lt;/code&gt; means the data types should not be cast at all.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;‘equiv’&lt;/code&gt; means only byte-order changes are allowed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;‘safe’&lt;/code&gt; means only casts which can preserve values are allowed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;‘same_kind’&lt;/code&gt; means only safe casts or casts within a kind, like float64 to float32, are allowed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;‘unsafe’&lt;/code&gt; means any data conversions may be done.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;控制数据类型的&lt;code&gt;cast&lt;/code&gt;，共有五种类型。不表。1.20.0中新增，以前的numpy无法使用。&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]])
&amp;gt;&amp;gt;&amp;gt; print(a.shape)
&amp;gt;&amp;gt;&amp;gt; b = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]]])
&amp;gt;&amp;gt;&amp;gt; print(b.shape)
&amp;gt;&amp;gt;&amp;gt; x = np.concatenate((a,b),axis = 0)
&amp;gt;&amp;gt;&amp;gt; print(x.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如图：
&lt;img src=&#34;D5A3A72E0AA547A5B9E546E02A468298&#34; alt=&#34;image&#34;  /&gt;
可见，在指定维度上，第0维进行了join。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; x1 = np.concatenate((a,b),axis = 1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如图：
&lt;img src=&#34;B40F1D20B6974E6684D3DE860B5942E1&#34; alt=&#34;image&#34;  /&gt;
可以看出，如果指定&lt;code&gt;axis=1&lt;/code&gt;，则会报错，因为除了&lt;code&gt;axis=1&lt;/code&gt;的其他维度上，shape并不相等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; x2 = np.concatenate((a,b),axis=None)
&amp;gt;&amp;gt;&amp;gt; print(x2.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;F2FAA06B2296493B9192F49B7BBF8C04&#34; alt=&#34;image&#34;  /&gt;
可以看出，数组被展平了。&lt;/p&gt;
&lt;h2 id=&#34;注意事项&#34;&gt;注意事项&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;When one or more of the arrays to be concatenated is a  &lt;code&gt;MaskedArray&lt;/code&gt;, this function will return a &lt;code&gt;MaskedArray&lt;/code&gt; object instead of an &lt;code&gt;ndarray&lt;/code&gt;, but the &lt;code&gt;input masks&lt;/code&gt; are not preserved. In cases where a &lt;code&gt;MaskedArray&lt;/code&gt; is expected as input, use the &lt;code&gt;ma.concatenate&lt;/code&gt; function from the masked array module instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当有一个或者多个&lt;code&gt;MaskedArray&lt;/code&gt;作为输入进行&lt;code&gt;concatenate&lt;/code&gt;时，不再会返回一个&lt;code&gt;ndarray&lt;/code&gt;，而是会返回&lt;code&gt;MaskedArray&lt;/code&gt;,但是其&lt;code&gt;mask&lt;/code&gt;不会保留，所以在输入中有&lt;code&gt;MaskedArray&lt;/code&gt;的情况下，应该尽量使用&lt;code&gt;ma.concatenate&lt;/code&gt;，而不是&lt;code&gt;np.concatenate()&lt;/code&gt;。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;numpy求矩阵的特征值与特征向量nplinalgeig&#34;&gt;numpy求矩阵的特征值与特征向量(np.linalg.eig)&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;np.linalg.eig(a)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;功能&#34;&gt;功能&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the eigenvalues and right eigenvectors of a square array.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;求方阵(&lt;code&gt;n x n&lt;/code&gt;)的特征值与右特征向量&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;a : (&amp;hellip;, M, M) array&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Matrices for which the eigenvalues and right eigenvectors will
be computed&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;a是一个矩阵&lt;code&gt;Matrix&lt;/code&gt;的数组。每个矩阵&lt;code&gt;M&lt;/code&gt;都会被计算其特征值与特征向量。&lt;/p&gt;
&lt;h2 id=&#34;returns&#34;&gt;Returns&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;w : (&amp;hellip;, M) array&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The eigenvalues, each repeated according to its multiplicity.
The eigenvalues are not necessarily ordered. The resulting
array will be of complex type, unless the imaginary part is
zero in which case it will be cast to a real type. When &lt;code&gt;a&lt;/code&gt;
is real the resulting eigenvalues will be real (0 imaginary
part) or occur in conjugate pairs&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;返回的&lt;code&gt;w&lt;/code&gt;是其特征值。特征值不会特意进行排序。返回的array一般都是复数形式，除非虚部为0，会被cast为实数。当&lt;code&gt;a&lt;/code&gt;是实数类型时，返回的就是实数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v : (&amp;hellip;, M, M) array&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The normalized (unit &amp;ldquo;length&amp;rdquo;) eigenvectors, such that the
column &lt;code&gt;v[:,i]&lt;/code&gt; is the eigenvector corresponding to the
eigenvalue &lt;code&gt;w[i]&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;返回的&lt;code&gt;v&lt;/code&gt;是归一化后的特征向量（&lt;code&gt;length&lt;/code&gt;为1）。特征向量&lt;code&gt;v[:,i]&lt;/code&gt;对应特征值&lt;code&gt;w[i]&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;raises&#34;&gt;Raises&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LinAlgError&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If the eigenvalue computation does not converge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ralated-function&#34;&gt;Ralated Function:&lt;/h2&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;see-also&#34;&gt;See Also&lt;/h2&gt;
&lt;p&gt;eigvals : eigenvalues of a non-symmetric array.
eigh : eigenvalues and eigenvectors of a real symmetric or complex
Hermitian (conjugate symmetric) array.
eigvalsh : eigenvalues of a real symmetric or complex Hermitian
(conjugate symmetric) array.
scipy.linalg.eig : Similar function in SciPy that also solves the
generalized eigenvalue problem.
scipy.linalg.schur : Best choice for unitary and other non-Hermitian
normal matrices.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;相关的函数有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;eigvals&lt;/code&gt;：计算非对称矩阵的特征值&lt;/li&gt;
&lt;li&gt;&lt;code&gt;eigh&lt;/code&gt;：实对称矩阵或者复共轭对称矩阵(&lt;code&gt;Hermitian&lt;/code&gt;)的特征值与特征向量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;eigvalsh&lt;/code&gt;: 实对称矩阵或者复共轭对称矩阵(&lt;code&gt;Hermitian&lt;/code&gt;)的特征值与特征向量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scipy.linalg.eig&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scipy.linalg.schur&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;.. versionadded:: 1.8.0&lt;/p&gt;
&lt;p&gt;Broadcasting rules apply, see the &lt;code&gt;numpy.linalg&lt;/code&gt; documentation for
details.&lt;/p&gt;
&lt;p&gt;This is implemented using the &lt;code&gt;_geev&lt;/code&gt; LAPACK routines which compute
the eigenvalues and eigenvectors of general square arrays.&lt;/p&gt;
&lt;p&gt;The number &lt;code&gt;w&lt;/code&gt; is an eigenvalue of &lt;code&gt;a&lt;/code&gt; if there exists a vector
&lt;code&gt;v&lt;/code&gt; such that &lt;code&gt;a @ v = w * v&lt;/code&gt;. Thus, the arrays &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;w&lt;/code&gt;, and
&lt;code&gt;v&lt;/code&gt; satisfy the equations &lt;code&gt;a @ v[:,i] = w[i] * v[:,i]&lt;/code&gt;
for &lt;code&gt;$i \in \{0,...,M-1\}$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The array &lt;code&gt;v&lt;/code&gt; of eigenvectors may not be of maximum rank, that is, some
of the columns may be linearly dependent, although round-off error may
obscure that fact. If the eigenvalues are all different, then theoretically
the eigenvectors are linearly independent and &lt;code&gt;a&lt;/code&gt; can be diagonalized by
a similarity transformation using &lt;code&gt;v&lt;/code&gt;, i.e, &lt;code&gt;inv(v) @ a @ v&lt;/code&gt; is diagonal.&lt;/p&gt;
&lt;p&gt;For non-Hermitian normal matrices the SciPy function &lt;code&gt;scipy.linalg.schur&lt;/code&gt;
is preferred because the matrix &lt;code&gt;v&lt;/code&gt; is guaranteed to be unitary, which is
not the case when using &lt;code&gt;eig&lt;/code&gt;. The Schur factorization produces an
upper triangular matrix rather than a diagonal matrix, but for normal
matrices only the diagonal of the upper triangular matrix is needed, the
rest is roundoff error.&lt;/p&gt;
&lt;p&gt;Finally, it is emphasized that &lt;code&gt;v&lt;/code&gt; consists of the &lt;em&gt;right&lt;/em&gt; (as in
right-hand side) eigenvectors of &lt;code&gt;a&lt;/code&gt;.  A vector &lt;code&gt;y&lt;/code&gt; satisfying
&lt;code&gt;y.T @ a = z * y.T&lt;/code&gt; for some number &lt;code&gt;z&lt;/code&gt; is called a &lt;em&gt;left&lt;/em&gt;
eigenvector of &lt;code&gt;a&lt;/code&gt;, and, in general, the left and right eigenvectors
of a matrix are not necessarily the (perhaps conjugate) transposes
of each other.&lt;/p&gt;
&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;
&lt;p&gt;G. Strang, &lt;em&gt;Linear Algebra and Its Applications&lt;/em&gt;, 2nd Ed., Orlando, FL,
Academic Press, Inc., 1980, Various pp.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;需要说明的是，特征向量之间可能存在线性相关关系，即返回的v可能不是满秩的。但如果特征值都不同的话，理论上来说，所有特征向量都是线性无关的。此时可以利用&lt;code&gt;inv(v)@ a @ v&lt;/code&gt;来计算特征值的对角矩阵（对角线上的元素是特征值，其余元素为0),同时可以用&lt;code&gt;v @ diag(w) @ inv(v)&lt;/code&gt;来恢复a。&lt;br&gt;
同时需要说明的是，这里得到的特征向量都是右特征向量。即&lt;code&gt;$Ax = {\lambda}x$&lt;/code&gt;.所以，数组&lt;code&gt;a&lt;/code&gt;,&lt;code&gt;w&lt;/code&gt;,&lt;code&gt;v&lt;/code&gt;满足:&lt;code&gt;a @ v[:,i] = w[i] * v[:,i]&lt;/code&gt;
for &lt;code&gt;$i \in \{0,...,M-1\}$&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from numpy import linalg as LA

(Almost) trivial example with real e-values and e-vectors.

&amp;gt;&amp;gt;&amp;gt; w, v = LA.eig(np.diag((1, 2, 3)))
&amp;gt;&amp;gt;&amp;gt; w; v
array([1., 2., 3.])
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])

Real matrix possessing complex e-values and e-vectors; note that the
e-values are complex conjugates of each other.

&amp;gt;&amp;gt;&amp;gt; w, v = LA.eig(np.array([[1, -1], [1, 1]]))
&amp;gt;&amp;gt;&amp;gt; w; v
array([1.+1.j, 1.-1.j])
array([[0.70710678+0.j        , 0.70710678-0.j        ],
       [0.        -0.70710678j, 0.        +0.70710678j]])

Complex-valued matrix with real e-values (but complex-valued e-vectors);
note that ``a.conj().T == a``, i.e., `a` is Hermitian.

&amp;gt;&amp;gt;&amp;gt; a = np.array([[1, 1j], [-1j, 1]])
&amp;gt;&amp;gt;&amp;gt; w, v = LA.eig(a)
&amp;gt;&amp;gt;&amp;gt; w; v
array([2.+0.j, 0.+0.j])
array([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary
       [ 0.70710678+0.j        , -0.        +0.70710678j]])

Be careful about round-off error!

&amp;gt;&amp;gt;&amp;gt; a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])
&amp;gt;&amp;gt;&amp;gt; # Theor. e-values are 1 +/- 1e-9
&amp;gt;&amp;gt;&amp;gt; w, v = LA.eig(a)
&amp;gt;&amp;gt;&amp;gt; w; v
array([1., 1.])
array([[1., 0.],
       [0., 1.]])

&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h2 id=&#34;numpy中求矩阵的逆与伪逆&#34;&gt;numpy中求矩阵的逆与伪逆&lt;/h2&gt;
&lt;p&gt;numpy中求矩阵的逆：&lt;code&gt;numpy.linalg.inv()&lt;/code&gt;&lt;br&gt;
numpy中求矩阵的伪逆: &lt;code&gt;numpy.linalg.pinv()&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;numpy中求矩阵的逆numpylinalginv&#34;&gt;numpy中求矩阵的逆（numpy.linalg.inv)&lt;/h3&gt;
&lt;p&gt;使用命令&lt;code&gt;numpy.linalg.inv(Matrix)&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;功能&#34;&gt;功能&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the (multiplicative) inverse of a matrix.&lt;br&gt;
Given a square matrix &lt;code&gt;a&lt;/code&gt;, return the matrix &lt;code&gt;ainv&lt;/code&gt; satisfying
&lt;code&gt;dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;计算一个方阵的逆，使之满足&lt;code&gt;$AA^{-1}=A^{-1}A=I$&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;parameters&#34;&gt;Parameters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a : (&amp;hellip;, M, M) array_like&lt;/strong&gt;&lt;br&gt;
Matrix to be inverted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;a是输入的要计算逆的矩阵数组。&lt;/p&gt;
&lt;h4 id=&#34;returns&#34;&gt;Returns&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ainv : (&amp;hellip;, M, M)&lt;/strong&gt;&lt;br&gt;
ndarray or matrix (Multiplicative) inverse of the matrix &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;返回的是对应的逆矩阵的数组。&lt;/p&gt;
&lt;h4 id=&#34;raises&#34;&gt;Raises&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LinAlgError&lt;/strong&gt;&lt;br&gt;
If &lt;code&gt;a&lt;/code&gt; is not square or inversion fails.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果a不是方阵或者不可逆，则&lt;code&gt;Raise LinAlgError&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;examples&#34;&gt;Examples&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from numpy.linalg import inv
&amp;gt;&amp;gt;&amp;gt; a = np.array([[1., 2.], [3., 4.]])
&amp;gt;&amp;gt;&amp;gt; ainv = inv(a)
&amp;gt;&amp;gt;&amp;gt; np.allclose(np.dot(a, ainv), np.eye(2))
True
&amp;gt;&amp;gt;&amp;gt; np.allclose(np.dot(ainv, a), np.eye(2))
True

If a is a matrix object, then the return value is a matrix as well:

&amp;gt;&amp;gt;&amp;gt; ainv = inv(np.matrix(a))
&amp;gt;&amp;gt;&amp;gt; ainv
matrix([[-2. ,  1. ],
        [ 1.5, -0.5]])

Inverses of several matrices can be computed at once:

&amp;gt;&amp;gt;&amp;gt; a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])
&amp;gt;&amp;gt;&amp;gt; inv(a)
array([[[-2.  ,  1.  ],
        [ 1.5 , -0.5 ]],
       [[-1.25,  0.75],
        [ 0.75, -0.25]]])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;numpy中求矩阵的伪逆numpylinalginv&#34;&gt;numpy中求矩阵的伪逆(numpy.linalg.inv)&lt;/h3&gt;
&lt;h4 id=&#34;伪逆的定义以及意义&#34;&gt;伪逆的定义以及意义&lt;/h4&gt;
&lt;p&gt;伪逆在某些情况下特指摩尔彭若斯广义逆。
广义逆矩阵：
&lt;img src=&#34;BEE48849E60D4A24BE040A2F3AC12FDC&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$A$&lt;/code&gt;的摩尔彭若斯矩阵记为&lt;code&gt;$A^+$&lt;/code&gt;.那么有两个基本性质：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;AA^+A=A  

A^+AA^+=A^+
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;功能-1&#34;&gt;功能&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the (Moore-Penrose) pseudo-inverse of a matrix.
Calculate the generalized inverse of a matrix using its
singular-value decomposition (SVD) and including all
&lt;em&gt;large&lt;/em&gt; singular values.&lt;br&gt;
.. versionchanged:: 1.14&lt;br&gt;
Can now operate on stacks of matrices&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;parameters-1&#34;&gt;Parameters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a : (&amp;hellip;, M, N) array_like&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Matrix or stack of matrices to be pseudo-inverted.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;要做伪逆运算的矩阵或者矩阵的栈&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;rcond : (&amp;hellip;) array_like of float&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Cutoff for small singular values.
Singular values less than or equal to
&lt;code&gt;rcond * largest_singular_value&lt;/code&gt; are set to zero.
Broadcasts against the stack of matrices.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对很小的奇异值做裁剪。当某个奇异值小于等于最大的奇异值乘以&lt;code&gt;rcond&lt;/code&gt;，就将它设置为0.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hermitian : bool, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If True, &lt;code&gt;a&lt;/code&gt; is assumed to be Hermitian (symmetric if real-valued),
enabling a more efficient method for finding singular values.
Defaults to False.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果为真，相当于告诉Numpy输入矩阵是Hermitian矩阵。会采用更加有效的方法去寻找奇异值。默认为False.&lt;/p&gt;
&lt;h4 id=&#34;returns-1&#34;&gt;Returns&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;B : (&amp;hellip;, N, M) ndarray&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The pseudo-inverse of &lt;code&gt;a&lt;/code&gt;. If &lt;code&gt;a&lt;/code&gt; is a &lt;code&gt;matrix&lt;/code&gt; instance, then so
is &lt;code&gt;B&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;返回&lt;code&gt;a&lt;/code&gt;的伪逆&lt;/p&gt;
&lt;h4 id=&#34;raises-1&#34;&gt;Raises&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LinAlgError&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If the SVD computation does not converge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果矩阵不能进行奇异值分解(SVD)，则报错。&lt;/p&gt;
&lt;h4 id=&#34;see-also&#34;&gt;See Also&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;--------
scipy.linalg.pinv : Similar function in SciPy.
scipy.linalg.pinv2 : Similar function in SciPy (SVD-based).
scipy.linalg.pinvh : Compute the (Moore-Penrose) pseudo-inverse of a
                     Hermitian matrix.
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;The pseudo-inverse of a matrix A, denoted&lt;code&gt;$A^+$&lt;/code&gt;, is
defined as: &amp;ldquo;the matrix that &amp;lsquo;solves&amp;rsquo; [the least-squares problem]
&lt;code&gt;$Ax = b$&lt;/code&gt;,&amp;rdquo; i.e., if &lt;code&gt;${\bar x}$&lt;/code&gt; is said solution, then
&lt;code&gt;$A^+$&lt;/code&gt; is that matrix such that&lt;code&gt;${\bar x} = A^+b$&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;It can be shown that if :  &lt;code&gt;$Q_1 \Sigma Q_2^T = A$&lt;/code&gt; is the singular
value decomposition of A, then
:&lt;code&gt;$A^+ = Q_2\Sigma^+ Q_1^T$&lt;/code&gt;, where :&lt;code&gt;$Q_{1,2}$&lt;/code&gt; are
orthogonal matrices, :&lt;code&gt;$\Sigma$&lt;/code&gt; is a diagonal matrix consisting
of A&amp;rsquo;s so-called singular values, (followed, typically, by
zeros), and then :&lt;code&gt;$\Sigma^+$&lt;/code&gt; is simply the diagonal matrix
consisting of the reciprocals of A&amp;rsquo;s singular values
(again, followed by zeros). [1]_&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;References:&lt;br&gt;
.. [1] G. Strang, &lt;em&gt;Linear Algebra and Its Applications&lt;/em&gt;, 2nd Ed., Orlando,
FL, Academic Press, Inc., 1980, pp. 139-142.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于矩阵A，有方程&lt;code&gt;$Ax=b$&lt;/code&gt;,那么它的摩尔彭若斯伪逆&lt;code&gt;$A^+$&lt;/code&gt;满足：对于&lt;code&gt;$\bar x = A^+b$&lt;/code&gt;,&lt;code&gt;$\bar x$&lt;/code&gt;能使得&lt;code&gt;Ax&lt;/code&gt;与&lt;code&gt;b&lt;/code&gt;的二范数距离最小。&lt;br&gt;
矩阵A的摩尔彭若斯伪逆可以通过奇异值分解来求得。A的奇异值分解为：&lt;code&gt;$Q_1 \Sigma Q_2^T = A$&lt;/code&gt;。那么其伪逆&lt;code&gt;$A^+ = Q_2\Sigma^+ Q_1^T$&lt;/code&gt;。其中&lt;code&gt;$\Sigma^+$&lt;/code&gt;就是&lt;code&gt;$\Sigma$&lt;/code&gt;的对角元素的倒数构成的对角矩阵。其实因为&lt;code&gt;$\Sigma$&lt;/code&gt;本身就是对角矩阵，&lt;code&gt;$\Sigma^+$&lt;/code&gt;就是&lt;code&gt;$\Sigma^{-1}$&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;examples-1&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;The following example checks that &lt;code&gt;a * a+ * a == a&lt;/code&gt; and
&lt;code&gt;a+ * a * a+ == a+&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = np.random.randn(9, 6)
&amp;gt;&amp;gt;&amp;gt; B = np.linalg.pinv(a)
&amp;gt;&amp;gt;&amp;gt; np.allclose(a, np.dot(a, np.dot(B, a)))
True
&amp;gt;&amp;gt;&amp;gt; np.allclose(B, np.dot(B, np.dot(a, B)))
True
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description></description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;python如何使得list中的元素是ndarray&#34;&gt;python如何使得&lt;code&gt;list&lt;/code&gt;中的元素是&lt;code&gt;ndarray&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;在一个代码实现时，会想要类似MATLAB一样，拥有一个&lt;code&gt;struct&lt;/code&gt;类型的数组，数组中的每个&lt;code&gt;cell&lt;/code&gt;都是一个&lt;code&gt;ndarray&lt;/code&gt;。因为在某些维度上&lt;code&gt;shape&lt;/code&gt;不同，所以不能整合为一个大的&lt;code&gt;ndarray&lt;/code&gt;。&lt;br&gt;
要实现上述需求，可以进行如下操作:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;targetList = []
targetList.append(ndarray)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样即可获得由&lt;code&gt;ndarray&lt;/code&gt;构成的&lt;code&gt;list&lt;/code&gt;，可以进一步对&lt;code&gt;list&lt;/code&gt;进行&lt;code&gt;concatenate&lt;/code&gt;等操作再次整合。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;python中李群so3与李代数so3之间指数映射与实现源码&#34;&gt;Python中李群&lt;code&gt;SO(3)&lt;/code&gt;与李代数&lt;code&gt;so(3)&lt;/code&gt;之间指数映射与实现源码&lt;/h1&gt;
&lt;h2 id=&#34;调用scipylinalgexpm&#34;&gt;调用&lt;code&gt;scipy.linalg.expm()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;对于李群&lt;code&gt;SE(3)&lt;/code&gt;、&lt;code&gt;SO(3)&lt;/code&gt;，和与其对应的李代数&lt;code&gt;se(3)&lt;/code&gt;,&lt;code&gt;so(3)&lt;/code&gt;。指数映射是十分重要的。&lt;br&gt;
在Python中我们可以调用：&lt;code&gt;scipy.linalg.expm()&lt;/code&gt;来将李代数&lt;code&gt;$\xi $&lt;/code&gt;对应的反对称矩阵&lt;code&gt;${\hat \xi }$&lt;/code&gt;映射到其对应的旋转矩阵&lt;code&gt;$R$&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;源码示例&#34;&gt;源码示例&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import scipy.linalg.expm as expm

def SkewFun(a):
    &amp;quot;&amp;quot;&amp;quot;
    got the corresponded antiSymmetric Matrix of the Lie algebra
    :param a:   Lie algebra
    :return:    antiSymmetric Matrix
    &amp;quot;&amp;quot;&amp;quot;
    if len(a) == 3:
        A = np.array([[0, -a[2], a[1]],
                      [a[2], 0, -a[0]],
                      [-a[1], a[0], 0]
                      ])
        return A
    if len(a) == 2:
        A = np.array([a[1], -a[0]])
        return A
    exit(-1)
    
def so3ToSO3(xi):
    return expm(SkewFun(xi)

    &lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;python判断不可变对象字符串整数浮点数数组相等的办法以及其底层实现原理&#34;&gt;Python判断不可变对象（字符串，整数，浮点数，数组）相等的办法以及其底层实现原理&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;中，判断不可变对象是否相等与&lt;code&gt;Java&lt;/code&gt;十分不同。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;中判断两个字符串相等：既可以使用&lt;code&gt;&amp;quot;==&amp;quot;&lt;/code&gt;，又可以使用&lt;code&gt;&amp;quot;is&amp;quot;&lt;/code&gt;。&lt;br&gt;
判断整数，浮点数以及&lt;code&gt;Tuple&lt;/code&gt;时，最好使用&lt;code&gt;&amp;quot;==&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = &#39;abc&#39;
b = &#39;abc&#39;
print(a is b)
print(a == b)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;A11A3DC092674644A4C0CF98AFC0A63C&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;这个问题可以进一步引申至&lt;code&gt;Python&lt;/code&gt;的底层实现原理上。&lt;/p&gt;
&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;
&lt;h3 id=&#34;整数&#34;&gt;整数&lt;/h3&gt;
&lt;p&gt;Python在底层实现中，一切都是对象。包括整数等也是对象。这些基本的不可变对象在python里会被频繁的引用,创建,如果不能重用的话，极易导致效率瓶颈,所以python引入了整数对象池的机制。&lt;br&gt;
Python中，对于[-5,256]的整数，创建了整数对象池，创建范围内的小整数会自动引用对象池中的整数对象。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = -5
b = -5
print(a is b)
print(a == b)
a1 = -6
b1 = -6
print(a1 is b1)
print(a1 == b1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;DD52A334BA4243128BE38A38179499EB&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;从运行结果图上可以看出，&lt;code&gt;-5&lt;/code&gt;由于是小整数，引用整数池中的对象，所以是同一个对象,&lt;code&gt;a is b&lt;/code&gt;为&lt;code&gt;True&lt;/code&gt;。而&lt;code&gt;-6&lt;/code&gt;由于不在范围内，每次都会新建一个对象，所以是两个对象，&lt;code&gt;a is b&lt;/code&gt;为&lt;code&gt;False&lt;/code&gt;。&lt;br&gt;
同理可以测试&lt;code&gt;256&lt;/code&gt;,&lt;code&gt;257&lt;/code&gt;。前者为&lt;code&gt;True&lt;/code&gt;，后者为&lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;浮点数&#34;&gt;浮点数&lt;/h3&gt;
&lt;p&gt;由于浮点数有无穷多个，所以浮点数并没有常量池。在创建浮点数对象时会直接新建一个对象。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = 0.0
b = 0.0
print(a is b)
print(a == b)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;55817F51B48740FFB2683045795420E3&#34; alt=&#34;image&#34;  /&gt;
从结果中可见，两者值相等，但并不是指向同一内存地址。&lt;/p&gt;
&lt;h3 id=&#34;字符串&#34;&gt;字符串&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;中存在着&lt;code&gt;intern&lt;/code&gt;机制。由于字符串是不可变对象，它对字符串维护着一个字典，每次新建一个字符串变量时，会先查询字典中是否已经有该字符串值。如果有，直接引用。如果没有再新建。这个机制决定了&lt;strong&gt;字符串值相等，则一定指向相同的对象。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tuple&#34;&gt;Tuple&lt;/h3&gt;
&lt;p&gt;对于元组，虽然其是不可变对象，但在底层实现无&lt;code&gt;intern&lt;/code&gt;机制，就是单纯的一个可以迭代的数组，存放着元素。每次创建都会开辟地址。所以新建两个值相同的变量会创建两个对象。使用&lt;code&gt;&amp;quot;==&amp;quot;&lt;/code&gt;判断。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = (1,2,3)
b = (1,2,3)
print(a == b)
print(a is b)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;1FCBEFDD80094EE9A4C28BBE87CDCF79&#34; alt=&#34;image&#34;  /&gt;
结果也印证了这一点。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h2 id=&#34;python中判断列表为空&#34;&gt;python中判断列表为空&lt;/h2&gt;
&lt;p&gt;判断列表为空是一个非常基础的问题。但是也有很多写法。&lt;/p&gt;
&lt;h3 id=&#34;方法1len&#34;&gt;方法1：len()&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;list = []
if len(list) == 0:
    print(&#39;list is empty&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;方法2直接使用if判断&#34;&gt;方法2：直接使用if判断&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;list = []
if not list:
    print(&#39;list is empty&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;直接使用&lt;code&gt;list&lt;/code&gt;作为判断标准，则空列表相当于&lt;code&gt;False&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;方法3使用进行判断&#34;&gt;方法3：使用&lt;code&gt;==&lt;/code&gt;进行判断&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;EmptyList = []
list = []
if list==EmptyList:
    print(&#39;list is empty&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;: Python中与Java不同。Java中&lt;code&gt;==&lt;/code&gt;用于判断两个变量是否指向同一个对象，即地址是否相同。但是&lt;code&gt;Python&lt;/code&gt;中不是，&lt;code&gt;Python&lt;/code&gt;中，&lt;code&gt;==&lt;/code&gt;用于判断两个变量的值相等。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h2 id=&#34;python中怎么表示自然底数e和浮点数精度epsilon&#34;&gt;python中怎么表示自然底数&lt;code&gt;e&lt;/code&gt;和浮点数精度&lt;code&gt;epsilon&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;自然底数&lt;code&gt;e&lt;/code&gt;可以直接使用&lt;code&gt;math.e&lt;/code&gt;表示。&lt;br&gt;
浮点数精度&lt;code&gt;epsilon&lt;/code&gt;可以使用&lt;code&gt;np.spacing(1)&lt;/code&gt;来表征&lt;code&gt;epsilon&lt;/code&gt;，等效于MATLAB中的&lt;code&gt;eps&lt;/code&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;pytorch中默认维度&#34;&gt;PyTorch中默认维度&lt;/h1&gt;
&lt;p&gt;PyTorch中默认维度: B C H W。&lt;/p&gt;
&lt;p&gt;即&lt;strong&gt;Batch_size，Channel, Height, Width&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;mechanics-of-seq2seq-models-with-attention&#34;&gt;Mechanics of Seq2Seq Models With Attention&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a class=&#34;link&#34; href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;Sequence-to-sequence模型在深度学习领域取得了很多成就。&lt;br&gt;
这文章真的牛逼。&lt;br&gt;
有视频不翻译了。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;adaptivemaxpool&#34;&gt;AdaptiveMaxPool&lt;/h1&gt;
&lt;p&gt;AdaptiveMaxPool是PyTorch中提供的自适应池化层。&lt;/p&gt;
&lt;p&gt;其主要特殊的地方在于：
&lt;strong&gt;无论输入&lt;code&gt;Input&lt;/code&gt;的size是多少，输出的size总为指定的size&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;adaptivemaxpool1d&#34;&gt;AdaptiveMaxPool1d()&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;m = nn.AdaptiveMaxPool1d(3)
input = torch.randn(4,3,7)
output = m(input)   # output的size为(4,3,3)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;adaptivemaxpool2d&#34;&gt;AdaptiveMaxPool2d()&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;m = nn.AdaptiveMaxPool2d((3,6))
input = torch.randn(2,64,8,9)
output = m(input)   # output的size为(2,64,3,6)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;adaptivemaxpool3d&#34;&gt;AdaptiveMaxPool3d()&lt;/h2&gt;
&lt;p&gt;同理&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;torchtopk&#34;&gt;torch.topk&lt;/h1&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;torch.topk(input, k, dim=None, largest=True, sorted=True, *, out = None)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;返回输入tensor&lt;code&gt;input&lt;/code&gt;中，在给定的维度&lt;code&gt;dim&lt;/code&gt;上&lt;code&gt;k&lt;/code&gt;个最大的元素。&lt;/p&gt;
&lt;p&gt;如果&lt;code&gt;dim&lt;/code&gt;没有给定，那么选择输入&lt;code&gt;input&lt;/code&gt;的最后一维。&lt;/p&gt;
&lt;p&gt;如果&lt;code&gt;largest = False&lt;/code&gt;，那么返回&lt;code&gt;k&lt;/code&gt;个最小的元素。&lt;/p&gt;
&lt;p&gt;返回一个&lt;code&gt;namedtuple&lt;/code&gt;类型的元组&lt;code&gt;(values, indices)&lt;/code&gt;，其中&lt;code&gt;indices&lt;/code&gt;是指元素在原数组中的索引。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sorted = True&lt;/code&gt;， 则返回的&lt;code&gt;k&lt;/code&gt;个元素是有序的。&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;input&lt;/strong&gt; (Tensor) &amp;ndash; the input tensor&lt;br&gt;
输入的张量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;k&lt;/strong&gt; (int) &amp;ndash; the k in &amp;ldquo;top-k&amp;rdquo;&lt;br&gt;
返回的k的值&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;dim&lt;/strong&gt;(int, optional) &amp;ndash; the dimension to sort along&lt;br&gt;
指定的排序的维度, &lt;code&gt;dim&lt;/code&gt;若为-1，文档未说明，但是根据实操效果，应该是对最后一维进行search。&lt;br&gt;
如shape为&lt;code&gt;Batch_size x p x q&lt;/code&gt;，返回结果为&lt;code&gt;Batch_size x p x k&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;largest&lt;/strong&gt;(bool, optional) &amp;ndash; controls whether to return largest or smallest elements&lt;br&gt;
True返回最大值，False返回最小值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;sorted&lt;/strong&gt;(bool, optional) &amp;ndash; controls whether to return the elements in sorted order&lt;br&gt;
控制返回的元素是否排序。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;例子&#34;&gt;例子&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;6.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;4.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;5.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;return_types&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;5.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;4.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;3.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;indices&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;如何设置随机种子&#34;&gt;如何设置随机种子&lt;/h1&gt;
&lt;h2 id=&#34;设置随机种子&#34;&gt;设置随机种子&lt;/h2&gt;
&lt;p&gt;应该为torch, numpy,以及Python设置随机种子，并提高torch卷积精度。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;set_seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;PYTHONHASHSEED&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;manual_seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;manual_seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;manual_seed_all&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backends&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudnn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;deterministic&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backends&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudnn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;benchmark&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;False&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;设置随机种子之后仍然发现训练结果不同考虑是数据采样的问题&#34;&gt;设置随机种子之后，仍然发现训练结果不同？考虑是数据采样的问题&lt;/h2&gt;
&lt;p&gt;背景： 数据集需要对数据进行随机采样。从全体数据中sample得到部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;踩坑： 在DataLoader中，num_worker会影响在已经设置好的随机种子下，对数据的采样结果，导致每次拿到的数据均不同。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方法：&lt;/strong&gt; 删除num_worker.&lt;/p&gt;
&lt;p&gt;原始： 相同seed下，每次提取的数据都不一致。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;n&#34;&gt;trainLoader&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_worker&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;修改：相同seed下，每次随机得到的数据一致。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;n&#34;&gt;trainLoader&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;实验对比发现，即使num_worker=1，仍然会导致无法复现。原因分析，num_worker是用于数据提取的多线程数，多线程情况下线程同步问题会导致随机种子在多次实验中波动。删除num_worker即可。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;为什么要有tensorcontiguous&#34;&gt;为什么要有Tensor.contiguous()&lt;/h1&gt;
&lt;h2 id=&#34;tensorcontiguous作用&#34;&gt;Tensor.contiguous()作用&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作用在官方文档里，描述的看似清晰但又模棱两可。例如x是一个Tensor，&lt;code&gt;x.contiguous()&lt;/code&gt;的作用就是返回一个在内存中连续的Tensor，其data与Tensor&lt;code&gt;x&lt;/code&gt;一致。如果源x本来就在内存中连续的话，那就返回其本身。&lt;/p&gt;
&lt;h2 id=&#34;为什么要有tensorcontiguous-1&#34;&gt;为什么要有Tensor.contiguous()?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a class=&#34;link&#34; href=&#34;https://stackoverflow.com/questions/48915810/pytorch-contiguous&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;StackOverflow&amp;ndash;Why do we need contiguous?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在PyTorch中，有些对Tensor的操作并不实际改变tensor的内容，而只是改变如何根据索引检索到tensor的byte location的方式。&lt;br&gt;
这些操作有：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;narrow()&lt;/code&gt;, &lt;code&gt;view()&lt;/code&gt;, &lt;code&gt;expand()&lt;/code&gt;, &lt;code&gt;transpose()&lt;/code&gt;，&lt;code&gt;permute()&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;例如&lt;/em&gt;： 当我们调用&lt;code&gt;transpose()&lt;/code&gt;时，PyTorch并不会生成一个具有新的layout（大概可以翻译为布局）的新tensor。该操作仅仅改变了tensor中的&lt;code&gt;meta information&lt;/code&gt;（元信息），所以offset和stride可以正确作用于新的shape。&lt;strong&gt;但是转置后的tensor和源tensor在事实上是共享同一块内存空间的&lt;/strong&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; x = torch.randn(3,2)
&amp;gt;&amp;gt;&amp;gt; print(x)
tensor([[ 0.9181,  1.4266],
        [-0.1432, -0.7514],
        [ 0.9809, -0.5079]])
        
&amp;gt;&amp;gt;&amp;gt; print(x[0,0])
tensor(0.9181)

&amp;gt;&amp;gt;&amp;gt; y = x.transpose(1,0)
&amp;gt;&amp;gt;&amp;gt; print(y)
tensor([[ 0.9181, -0.1432,  0.9809],
        [ 1.4266, -0.7514, -0.5079]])
        
&amp;gt;&amp;gt;&amp;gt; print(y[0,0])
tensor(0.9181)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这就是&lt;code&gt;contiguous&lt;/code&gt;的来源。上述例子中&lt;code&gt;x&lt;/code&gt;是连续的，但是&lt;code&gt;y&lt;/code&gt;不是，因为&lt;code&gt;y&lt;/code&gt;的内存布局与从头开始新建一个与&lt;code&gt;y&lt;/code&gt;shape相同的tensor的内存布局不同。&lt;br&gt;
需要注意的是，&lt;strong&gt;contiguous&lt;/strong&gt;这个单词有点被误解了，它并不是指tensor的内容在内存块上不连续。字节仍然是分配在同一个内存块上的，问题在于其元素之间的顺序order。&lt;/p&gt;
&lt;p&gt;而当我们调用&lt;code&gt;contiguous()&lt;/code&gt;时，实际上它是会复制一个张量，同时元素之间在内存上的顺序与从零开始新建的相同shape的张量一致。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;克罗内克内积-kronecker-product-otimes&#34;&gt;克罗内克内积 Kronecker product &lt;code&gt;$\otimes$&lt;/code&gt;&lt;/h1&gt;
&lt;h2 id=&#34;11-概述&#34;&gt;1.1 概述&lt;/h2&gt;
&lt;p&gt;克罗内克内积是一种特殊的张量积。任何两个形状的矩阵都可以进行克罗内克内积操作。&lt;/p&gt;
&lt;h2 id=&#34;12-定义-definition&#34;&gt;1.2 定义 Definition&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;$A \otimes B$&lt;/code&gt;的定义：A是&lt;code&gt;mxn&lt;/code&gt;矩阵，B是&lt;code&gt;pxq&lt;/code&gt;矩阵。&lt;code&gt;$A \otimes B$&lt;/code&gt;是&lt;code&gt;mp x nq&lt;/code&gt;的分块矩阵。
&lt;img src=&#34;0D139B55EC8F48C582D1A64A3F93ADC3&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：
&lt;img src=&#34;79F12F407AAC4F98B6C414849EC8AB55&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;13-性质&#34;&gt;1.3 性质&lt;/h2&gt;
&lt;h3 id=&#34;131-双线性结合律&#34;&gt;1.3.1 双线性结合律&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;CAD76AB30CA041BAA00F294AF3EA6462&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;132-不满足交换律&#34;&gt;1.3.2 不满足交换律&lt;/h3&gt;
&lt;h3 id=&#34;133-混合乘积性&#34;&gt;1.3.3 混合乘积性&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;91955BF2C2AE468A963088261ACCC524&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;协方差矩阵详解以及numpy计算协方差矩阵npcov&#34;&gt;协方差矩阵详解以及&lt;code&gt;numpy&lt;/code&gt;计算协方差矩阵(&lt;code&gt;np.cov&lt;/code&gt;)&lt;/h1&gt;
&lt;h2 id=&#34;协方差矩阵详解&#34;&gt;协方差矩阵详解&lt;/h2&gt;
&lt;h3 id=&#34;均值标准差与方差&#34;&gt;均值，标准差与方差&lt;/h3&gt;
&lt;p&gt;由简单的统计学基础知识，我们有如下公式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;\bar X{\rm{ = }}\frac{{\sum\limits_{i = 1}^n {{X_i}} }}{{\rm{n}}}  

S = \sqrt {\frac{{\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2}} }}{{n - 1}}} 

{S^2} = \frac{{\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2}} }}{{n - 1}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中&lt;code&gt;$\bar X$&lt;/code&gt;是样本均值，反映了n个样本观测值的整体大小情况。&lt;br&gt;
&lt;code&gt;$S$&lt;/code&gt;是样本标准差，反应的是样本的离散程度。标准差越大，数据越分散。&lt;br&gt;
&lt;code&gt;$S^2$&lt;/code&gt;是样本方差，是&lt;code&gt;$S$&lt;/code&gt;的平方。&lt;/p&gt;
&lt;p&gt;均值虽然可以在一定程度上反应数据的整体大小，但是仍然不能反应数据的内部离散程度。而标准差和方差弥补了这一点。&lt;br&gt;
但是标准差和方差都是针对一维数组的，即&lt;code&gt;1 x d&lt;/code&gt;数组。该数组的行代表的是一个随机变量（可理解为属性），如工资等。每一列代表一个观测值。如果一个事物具有多种属性，即有多个随机变量，那么我们会得到一个&lt;code&gt;var_num x d&lt;/code&gt;数组。该数组的每一行都是一个随机变量（属性），每一列代表着一个在这些属性维度上的观测值样本。如果我们想要分析该事物，那么仅仅将其剥离为单独的&lt;code&gt;1 x d&lt;/code&gt;去求其标准差是不够的，我们还需要关注这些随机变量（属性）&lt;code&gt;variable&lt;/code&gt;内部之间的联系。如工资和年龄的联系，工资和技术水平的联系等。&lt;br&gt;
所以便自然而然的引入了协方差。&lt;/p&gt;
&lt;h3 id=&#34;协方差&#34;&gt;协方差&lt;/h3&gt;
&lt;p&gt;两个随机变量的协方差反映了这两个随机变量一致的分散程度有多大。&lt;br&gt;
通俗的讲，协方差反映了两个随机变量的正负相关关系。&lt;br&gt;
由方差的公式，我们可以类比得出协方差的公式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;{\mathop{\rm var}} (X) = {S^2} = \frac{{\sum\limits_{i = 1}^n {({X_i} - \bar X)({X_i} - \bar X)} }}{{n - 1}}

{\mathop{\rm cov}} (X,Y) = \frac{{\sum\limits_{i = 1}^n {({X_i} - \bar X)({Y_i} - \bar Y)} }}{{n - 1}} = E((X - E(X))(Y - E(Y)))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;相关系数&lt;code&gt;$\rho$&lt;/code&gt;与协方差直接有如下关系：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;\rho  = \frac{{Cov(X,Y)}}{{{\sigma _X}{\sigma _Y}}} = \frac{{E((X - E(X))(Y - E(Y)))}}{{{\sigma _X}{\sigma _Y}}} = E((\frac{{X - E(X)}}{{{\sigma _X}}})(\frac{{Y - E(Y)}}{{{\sigma _Y}}})
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上述公式可见，相关系数&lt;code&gt;$\rho$&lt;/code&gt;实际上也是一种特殊的协方差。相关系数是数据&lt;code&gt;X&lt;/code&gt;和&lt;code&gt;Y&lt;/code&gt;做了归一化&lt;code&gt;$x = \frac{{(X - \bar X)}}{{{\sigma _X}}}$&lt;/code&gt;,&lt;code&gt;$y = \frac{{(Y - \bar Y)}}{{{\sigma _Y}}}$&lt;/code&gt;之后的协方差。&lt;code&gt;$x,y$&lt;/code&gt;的方差为1，期望为0。有：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;\rho(X,Y) = cov(x,y)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;协方差的意义此时应该很清晰了。&lt;/p&gt;
&lt;h3 id=&#34;协方差矩阵&#34;&gt;协方差矩阵&lt;/h3&gt;
&lt;p&gt;对于具有很多个随机变量的数据，随机变量之间两两都具有一个协方差，这样便形成了一个协方差矩阵。
假设我们有一组数据，其具有三个随机变量，n个观测值：&lt;br&gt;
&lt;img src=&#34;421F341479794E1B85A5C3B77A37A357&#34; alt=&#34;image&#34;  /&gt;&lt;br&gt;
那么其协方差矩阵为：
&lt;img src=&#34;528E0D1E979A425790E98100A7ED6D4F&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;我们可以使用一种便捷的矩阵乘法来计算协方差矩阵。设原数据数组为&lt;code&gt;$X$&lt;/code&gt;。先对X进行处理，求X每一个随机变量的均值。然后对于每一行，减去该行随机变量的均值，得到&lt;code&gt;$X^{&#39;}$&lt;/code&gt;，记协方差矩阵为&lt;code&gt;M&lt;/code&gt;，那么就有:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;M = \frac{{X^{&#39;}{X^{&#39;}}^{T}}}{{n-1}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用代码描述可能更加清晰:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = np.array([[1,2,3],[4,5,7]])
cov1 = np.cov(a)
mean_a = np.mean(a,axis=1,keepdims=True)
tmpa = a-mean_a
cov2 = np.matmul(tmpa,tmpa.T)/(tmpa.shape[1]-1)
print(cov1)
print(cov2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;0C16F50DA6634E6089B414D74F8C10AA&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;numpy计算协方差矩阵npcov&#34;&gt;numpy计算协方差矩阵&lt;code&gt;np.cov()&lt;/code&gt;&lt;/h2&gt;
&lt;h3 id=&#34;语法&#34;&gt;语法&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None,dtype)&lt;/code&gt;&lt;br&gt;
用于计算给定矩阵和权值的协方差矩阵。&lt;/p&gt;
&lt;h3 id=&#34;parameters&#34;&gt;&lt;code&gt;Parameters&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;m:array_like&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;A 1-D or 2-D array containing multiple variables and observations. Each row of m represents a variable, and each column a single observation of all those variables. Also see rowvar below.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一维或者二维数组，包含有多个随机变量和观测值。&lt;code&gt;m&lt;/code&gt;的每一行代表一个随机变量，每一列代表包含所有随机变量的一个观测值。当给一维数组时，相当于计算的就是方差。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;y:array_like,optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;An additional set of variables and observations. y has the same form as that of m.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;额外的一组数据，&lt;code&gt;y&lt;/code&gt;必须在在数据形式上与&lt;code&gt;m&lt;/code&gt;一致。&lt;br&gt;
如果&lt;code&gt;m.shape = (var_num, obs_num)&lt;/code&gt;，那么&lt;code&gt;y.shape&lt;/code&gt;必须在第二维观测值个数上，即&lt;code&gt;shape[1]&lt;/code&gt;与&lt;code&gt;m&lt;/code&gt;保持一致，即&lt;code&gt;y&lt;/code&gt;也得有&lt;code&gt;obs_num&lt;/code&gt;个观测值。实际执行时，会先将这两组数据&lt;code&gt;concatenate&lt;/code&gt;，然后再求解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = np.array([[1,2,3],[4,5,7]])
b = np.array([[1,2,3,4],[4,5,6,7]])
cov = np.cov(a,b)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;执行结果：
&lt;img src=&#34;6C2B8AB8C10C4B17AAD9723E5AA3FA29&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;从执行结果上可见，报错。报错的具体描述便是，两组数据在dimension1不一致。
我们也可以从&lt;code&gt;numpy.cov()&lt;/code&gt;源码中看到：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if y is not None:
    y = array(y, copy=False, ndmin=2, dtype=dtype)
    if not rowvar and y.shape[0] != 1:
        y = y.T
    X = np.concatenate((X, y), axis=0)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可见是对其进行了concatenate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;bias: bool, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Default normalization (False) is by (N - 1), where N is the number of observations given (unbiased estimate). If bias is True, then normalization is by N. These values can be overridden by using the keyword ddof in numpy versions &amp;gt;= 1.5&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;默认的采用无偏估计，即除以&lt;code&gt;(N-1)&lt;/code&gt;，N是样本个数。可以被&lt;code&gt;ddof&lt;/code&gt;所覆盖。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;rowvar : bool, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If &lt;code&gt;rowvar&lt;/code&gt; is True (default), then each row represents a
variable, with observations in the columns. Otherwise, the relationship
is transposed: each column represents a variable, while the rows
contain observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;rowvar&lt;/code&gt;指定了行列谁为随机变量的问题。默认为&lt;code&gt;True&lt;/code&gt;，即行代表一个随机变量。而列代表观测值。如果为&lt;code&gt;False&lt;/code&gt;，那么列代表随机变量，而行代表观测值。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ddof : int, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If not &lt;code&gt;None&lt;/code&gt; the default value implied by &lt;code&gt;bias&lt;/code&gt; is overridden.
Note that &lt;code&gt;ddof=1&lt;/code&gt; will return the unbiased estimate, even if both
&lt;code&gt;fweights&lt;/code&gt; and &lt;code&gt;aweights&lt;/code&gt; are specified, and &lt;code&gt;ddof=0&lt;/code&gt; will return
the simple average. See the notes for the details. The default value
is &lt;code&gt;None&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;.. versionadded:: 1.5
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;fweights : array_like, int, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;1-D array of integer frequency weights; the number of times each
observation vector should be repeated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;.. versionadded:: 1.10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一维int数组，&lt;code&gt;shape[0]&lt;/code&gt;应当与数据的观测值个数一致(即当&lt;code&gt;rowvar=True&lt;/code&gt;时候的&lt;code&gt;shape[1]&lt;/code&gt;)。指定每个观测值的频率权重，即这个观测值向量(&lt;code&gt;column&lt;/code&gt;)应该被重复计算几次。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;aweights : array_like, optional&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;1-D array of observation vector weights. These relative weights are
typically large for observations considered &amp;ldquo;important&amp;rdquo; and smaller for
observations considered less &amp;ldquo;important&amp;rdquo;. If &lt;code&gt;ddof=0&lt;/code&gt; the array of
weights can be used to assign probabilities to observation vectors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;.. versionadded:: 1.10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一维数组，其&lt;code&gt;shape[0]&lt;/code&gt;同样的，应该与观测值个数一致。指定的是每个计算权重，即较重要的观测值其&lt;code&gt;aweight&lt;/code&gt;大一些，不那么重要的可以小一些。当&lt;code&gt;ddof&lt;/code&gt;为0的时候，相当于观测值的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Return：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;out: ndarray:&lt;/strong&gt;  The covariance matrix of the variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;由于不太直观，所以不举例。分析一下源码。&lt;/p&gt;
&lt;h3 id=&#34;源码&#34;&gt;源码&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;    if ddof is not None and ddof != int(ddof):   # 这里说明ddof必须是int类型
        raise ValueError(
            &amp;quot;ddof must be integer&amp;quot;)

    # Handles complex arrays too
    m = np.asarray(m)       # 所以m的输入类型可以是lists, lists of tuples
                            #tuples, tuples of tuples, tuples of lists and ndarrays.
    if m.ndim &amp;gt; 2:          # 不能超过两维
        raise ValueError(&amp;quot;m has more than 2 dimensions&amp;quot;)

    if y is None:           # 如果y是None，返回数组类型取原数组类型
                            # 与np.float64精度高的那一个。
        dtype = np.result_type(m, np.float64)   
    else:                   # 有y输入则先处理y，判断y的维度，再判断数据类型
        y = np.asarray(y)
        if y.ndim &amp;gt; 2:
            raise ValueError(&amp;quot;y has more than 2 dimensions&amp;quot;)
        dtype = np.result_type(m, y, np.float64)

    X = array(m, ndmin=2, dtype=dtype)
    if not rowvar and X.shape[0] != 1:  # 如果rowvar为False就转置
        X = X.T
    if X.shape[0] == 0:
        return np.array([]).reshape(0, 0)
    if y is not None:                    # 对y进行处理
        y = array(y, copy=False, ndmin=2, dtype=dtype)
        if not rowvar and y.shape[0] != 1:  # 判断rowvar是否转置
            y = y.T
        X = np.concatenate((X, y), axis=0)  # concatenate

    if ddof is None:            # 如果未指定ddof
        if bias == 0:           # 如果指定了bias=0,ddof=1,无偏
            ddof = 1
        else:                   # 否则ddof=0
            ddof = 0

    # Get the product of frequencies and weights
    w = None
    if fweights is not None:
        fweights = np.asarray(fweights, dtype=float)
        if not np.all(fweights == np.around(fweights)):  # round进行取整
    # 取整后判断是否全部相等，来判断全都是整数，必须全是整数，否则报错
            raise TypeError(
                &amp;quot;fweights must be integer&amp;quot;)
        if fweights.ndim &amp;gt; 1:  # 必须一维
            raise RuntimeError(
                &amp;quot;cannot handle multidimensional fweights&amp;quot;)
        if fweights.shape[0] != X.shape[1]: # 必须与观测数一致
            raise RuntimeError(
                &amp;quot;incompatible numbers of samples and fweights&amp;quot;)
        if any(fweights &amp;lt; 0):   #必须全部为正值
            raise ValueError(
                &amp;quot;fweights cannot be negative&amp;quot;)
        w = fweights        # 将fweight赋给w
    if aweights is not None:
        aweights = np.asarray(aweights, dtype=float)
        if aweights.ndim &amp;gt; 1:
            raise RuntimeError(
                &amp;quot;cannot handle multidimensional aweights&amp;quot;)
        if aweights.shape[0] != X.shape[1]:
            raise RuntimeError(
                &amp;quot;incompatible numbers of samples and aweights&amp;quot;)
        if any(aweights &amp;lt; 0):
            raise ValueError(
                &amp;quot;aweights cannot be negative&amp;quot;)
        if w is None:
            w = aweights    # 如果fweight为空，就直接把aweight赋给w
        else:
            w *= aweights   # 否则w = fweight * aweight

    avg, w_sum = average(X, axis=1, weights=w, returned=True)
    # 以列为操作单元，求每一个随便变量的所有观测值在权重w下的均值。
    # w_sum为w的所有元素的和（权重和）。
    w_sum = w_sum[0]

    # Determine the normalization
    if w is None:       # 如果w为None，那么直接用X的观测值个数（列数）减ddof
        fact = X.shape[1] - ddof
    elif ddof == 0:  # w不为空，ddof等于0，需要除以的分母就是 w_sum
        fact = w_sum
    elif aweights is None: # w不为空，aweight为空，ddof不为0
    # 直接用 w_sum-ddof(因为此时的w_sum就相当于重复后的观测值个数)
        fact = w_sum - ddof
    else:   # w不为空，aweight也不为空， fweight也不为空，ddof != 0
    # fact就相当于w_sum减去以w为权重的aweight的平均值乘以ddof
    # 当aweigth=None的时候，是这个公式的一个特殊情况
    # 在这里猜测：ddof： duplicated degreeds of freedom   
    # 即重复无效的自由度
        fact = w_sum - ddof*sum(w*aweights)/w_sum

    if fact &amp;lt;= 0:
        warnings.warn(&amp;quot;Degrees of freedom &amp;lt;= 0 for slice&amp;quot;,
                      RuntimeWarning, stacklevel=3)
        fact = 0.0

    X -= avg[:, None]   # X减去均值
    if w is None:
        X_T = X.T
    else:
        X_T = (X*w).T   # 乘以权重 
    c = dot(X, X_T.conj())  # X 乘以 X的转置的复共轭矩阵（对标量而言就是转置）
    c *= np.true_divide(1, fact)    # 再除以fact
    return c.squeeze()  # 删去c中dim为1的维度，输出。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上就是我对np.cov()的全部解读。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;循环神经网络&#34;&gt;循环神经网络&lt;/h1&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;我们知道，在n元语法中，时间步&lt;code&gt;$t$&lt;/code&gt;的词&lt;code&gt;$w_t$&lt;/code&gt;基于前面所有词的条件概率只考虑了最近时间步的&lt;code&gt;$n-1$&lt;/code&gt;个词。如果要考虑比&lt;code&gt;$t-(n-1)$&lt;/code&gt;更早时间步的词对&lt;code&gt;$w_t$&lt;/code&gt;的可能影响，我们需要增大n。&lt;/p&gt;
&lt;p&gt;其下介绍的循环神经网络，它并未刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。&lt;/p&gt;
&lt;h2 id=&#34;不含隐藏状态的神经网络&#34;&gt;不含隐藏状态的神经网络&lt;/h2&gt;
&lt;p&gt;考虑一个含单隐藏层的多层感知机，给定样本数为&lt;code&gt;$n$&lt;/code&gt;、输入个数（特征数或者特征向量维度）为&lt;code&gt;$d$&lt;/code&gt;的小批量数据样本&lt;code&gt;$X \in R^{n \times d}$&lt;/code&gt;，设隐藏层的激活函数为&lt;code&gt;$\phi$&lt;/code&gt;，那么隐藏层的输出&lt;code&gt;$H \in R^{n \times h}$&lt;/code&gt;计算为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;    H = \phi(XW_{xh} + b_h)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中隐藏层权重参数&lt;code&gt;$W_{xh} \in R^{d \times h}$&lt;/code&gt;， 隐藏层偏差参数&lt;code&gt;$b_h \in R^{1 \times h}$&lt;/code&gt;，&lt;code&gt;$h$&lt;/code&gt;为隐藏单元个数。上式相加的两项形状不同，因此按广播机制相加，将隐藏变量&lt;code&gt;$H$&lt;/code&gt;作为输出层的输入，且输出个数为&lt;code&gt;$q$&lt;/code&gt;（如分类问题中的类别数），输出层的输出为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;O = HW_{hq} + b_q
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中输出变量&lt;code&gt;$O \in R^{n \times q}$&lt;/code&gt;，输出层权重参数&lt;code&gt;$W_{hq} \in R^{h \times q}$&lt;/code&gt;，输出层偏差参数&lt;code&gt;$b_q \in R^{1 \times q}$&lt;/code&gt;。如果是分类问题，我们可以使用&lt;code&gt;$softmax(O)$&lt;/code&gt;来计算输出类别的概率分布。&lt;/p&gt;
&lt;h2 id=&#34;含隐藏状态的循环神经网络&#34;&gt;含隐藏状态的循环神经网络&lt;/h2&gt;
&lt;p&gt;现在我们考虑输入数据存在时间相关性的情况，假设&lt;code&gt;$X_t \in R^{n \times d}$&lt;/code&gt;是序列中时间步&lt;code&gt;t&lt;/code&gt;的小批量输入，&lt;code&gt;$H_t \in R^{n \times h}$&lt;/code&gt;是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量&lt;code&gt;$H_{t-1}$&lt;/code&gt;，并引入一个新的权重参数&lt;code&gt;$W_{hh} \in R^{h \times h}$&lt;/code&gt;，该参数用于描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，&lt;strong&gt;时间步&lt;code&gt;t&lt;/code&gt;的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;H_t = \phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;与多层感知机相比，我们在这里添加了&lt;code&gt;$H_{t-1}W_{hh}$&lt;/code&gt;一项。由上式中相邻时间步的隐藏变量&lt;code&gt;$H_t$&lt;/code&gt;,&lt;code&gt;$H_{t-1}$&lt;/code&gt;之间的关系可知，这里的隐藏变量能够捕捉截止到当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或者记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络(&lt;code&gt;recurrent neural network&lt;/code&gt;)。&lt;/p&gt;
&lt;p&gt;循环神经网络有很多种不同的构造方法，含上式所定义的隐藏状态的循环神经网络是极为常见的一种。&lt;/p&gt;
&lt;p&gt;循环神经网络的参数包括隐藏层的权重&lt;code&gt;$W_{xh} \in R^{d \times h}$&lt;/code&gt;、&lt;code&gt;$W_{hh} \in R^{h \times h}$&lt;/code&gt;和偏差&lt;code&gt;$b_h \in R^{1 \times h}$&lt;/code&gt;，以及输出层的权重&lt;code&gt;$W_{hq} \in R^{h \times q}$&lt;/code&gt;和偏差&lt;code&gt;$b_q \in R^{1 \times q}$&lt;/code&gt;。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增长而增长。&lt;/p&gt;
&lt;p&gt;下图展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步&lt;code&gt;$t$&lt;/code&gt;，隐藏状态的计算可以看成是将输入&lt;code&gt;$X_t$&lt;/code&gt;和前一时间步隐藏状态&lt;code&gt;$H_{t-1}$&lt;/code&gt;连结后输入一个激活函数为&lt;code&gt;$\phi$&lt;/code&gt;的全连接层。该全连接层的输出就是当前时间步的隐藏状态&lt;code&gt;$H_t$&lt;/code&gt;，且模型参数为&lt;code&gt;$W_{xh}$&lt;/code&gt;与&lt;code&gt;$W_{hh}$&lt;/code&gt;的连结，偏差为&lt;code&gt;$b_h$&lt;/code&gt;。当前时间步&lt;code&gt;$t$&lt;/code&gt;的隐藏状态&lt;code&gt;$H_t$&lt;/code&gt;将参与下一个时间步&lt;code&gt;$t+1$&lt;/code&gt;的隐藏状态&lt;code&gt;$H_{t+1}$&lt;/code&gt;的计算，并输入到当前时间步的全连接输出层。
&lt;img src=&#34;4A659ED05CF247EFAA71672F650E059C&#34; alt=&#34;image&#34;  /&gt;
隐藏状态中&lt;code&gt;$X_tW_{xh} + H_{t-1}W_{hh} $&lt;/code&gt;的计算等价于&lt;code&gt;$X_t$&lt;/code&gt;与&lt;code&gt;$H_{t-1}$&lt;/code&gt;连结的矩阵乘以&lt;code&gt;$W_{xh}$&lt;/code&gt;与&lt;code&gt;$W_{hh}$&lt;/code&gt;连结后的矩阵。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;
&lt;p&gt;在概率论与统计学中，&lt;strong&gt;狄利克雷分布&lt;/strong&gt; &lt;em&gt;Dirichlet distribution&lt;/em&gt; 常被简记为&lt;code&gt;$Dir(\alpha)$&lt;/code&gt;,是基于一个正实数向量&lt;code&gt;$\alpha&lt;/code&gt;参数的连续多元概率分布族。狄利克雷分布是对&lt;strong&gt;贝塔分布&lt;/strong&gt; &lt;em&gt;beta distribution&lt;/em&gt;的多元泛化，所以它也被称为&lt;strong&gt;多元贝塔分布&lt;/strong&gt; &lt;em&gt;multivariate beta distribution(MBD)&lt;/em&gt;。&lt;br&gt;
狄利克雷分布被广泛作为贝叶斯统计的先验分布使用。同时，狄利克雷分布也是分类分布&lt;em&gt;Categorical distribution&lt;/em&gt; 和多项分布&lt;em&gt;categorical distribution&lt;/em&gt; 的共轭先验。&lt;br&gt;
狄利克雷分布的无限维推广就是狄利克雷过程&lt;em&gt;Dirichlet process&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;#&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;matrix-differentiation矩阵求导&#34;&gt;Matrix Differentiation（矩阵求导）&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;References: &lt;a class=&#34;link&#34; href=&#34;https://atmos.washington.edu/~dennis/MatrixCalculus.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Matrix Differentiation,Rabdak J.Barnes &lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 本文直接从&lt;code&gt;Matrix Differentiation&lt;/code&gt;开始记录，之前的乘法等基础部分不表。&lt;/p&gt;
&lt;h2 id=&#34;convention-3&#34;&gt;Convention 3&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;9CC335C449F7440EA4331DBA07AC03AA&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;m维向量对n维向量求导所得的结果是一个&lt;code&gt;mxn&lt;/code&gt;矩阵,即&lt;code&gt;Jacobian Matrix&lt;/code&gt;。
具体形式见上公式。&lt;/p&gt;
&lt;h2 id=&#34;命题5-proposition-5&#34;&gt;命题5 &lt;code&gt;Proposition 5&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;7650704200474934AD54DFF3471CAD2D&#34; alt=&#34;image&#34;  /&gt;
&lt;img src=&#34;5A99BDE668A54DC089B859F1156FE783&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;即：&lt;code&gt;Ax&lt;/code&gt;对&lt;code&gt;x&lt;/code&gt;求导，结果为A&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;94643FBB47944D5493A0635F9EF9125B&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题6-proposition-6&#34;&gt;命题6 &lt;code&gt;Proposition 6&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;B43F1B72DB0D4E03936EB2B5DEB5DB1C&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;即：&lt;code&gt;y=Ax&lt;/code&gt;，而&lt;code&gt;x&lt;/code&gt;是&lt;code&gt;z&lt;/code&gt;的函数，那么便有&lt;code&gt;$\frac{{\partial {\rm{y}}}}{{\partial z}} = A\frac{{\partial x}}{{\partial z}}$&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof-1&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;36C00E037B2446958E6EA955EDA73128&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题7-proposition-7&#34;&gt;命题7 &lt;code&gt;Proposition 7&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;17486208A23548CCA3297056D7667C4C&#34; alt=&#34;image&#34;  /&gt;
&lt;img src=&#34;F8E60CC41479488989D8DD72C9D06C34&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;对于&lt;code&gt;$\alpha = y^TAx$&lt;/code&gt;分别对&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y&lt;/code&gt;求导的结论。&lt;/p&gt;
&lt;h3 id=&#34;proof-2&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;D583157B81644670B2157EF160F1885B&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题8-proposition-8&#34;&gt;命题8 &lt;code&gt;Proposition 8&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;814A5454BE0E4CF9950217E8962B21E8&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;对于&lt;code&gt;$\alpha = x^TAx$&lt;/code&gt;对&lt;code&gt;x&lt;/code&gt;求导的结论。&lt;/p&gt;
&lt;h3 id=&#34;proof-3&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;23CA571E3DCE4C28812E576C44A46026&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题9-proposition-9&#34;&gt;命题9 &lt;code&gt;Proposition 9&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;0F67B410FE81427AA739C5EE35312BFD&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;即命题8的特例，A是对称矩阵。&lt;/p&gt;
&lt;h2 id=&#34;命题10-proposition-10&#34;&gt;命题10 &lt;code&gt;Proposition 10&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;9EF8BE87FA8E4F3F9D458C63FFE2159D&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;即&lt;code&gt;$\alpha = y^Tx$&lt;/code&gt;，而&lt;code&gt;y&lt;/code&gt;和&lt;code&gt;x&lt;/code&gt;均为向量&lt;code&gt;z&lt;/code&gt;的函数，对&lt;code&gt;z&lt;/code&gt;求导的结果。&lt;/p&gt;
&lt;h3 id=&#34;proof-4&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;E82B2B1E76CF414BAE5B4A2DFF6C95BC&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题11-proposition-11&#34;&gt;命题11 &lt;code&gt;Proposition 11&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;060D8BB6995142088B47628FD445983D&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;命题10的特例，&lt;code&gt;$y=x$&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题12-proposition-12&#34;&gt;命题12 &lt;code&gt;Proposition 12&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;59026ABFAD774C6AAA93533EE07A7D6F&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;对于&lt;code&gt;$\alpha = y^TAx$&lt;/code&gt;,&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y&lt;/code&gt;都是向量&lt;code&gt;z&lt;/code&gt;的函数，对&lt;code&gt;z&lt;/code&gt;求导的结果。&lt;/p&gt;
&lt;h3 id=&#34;proof-5&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;38FC2CC493604C4EAB0A16E28E94752C&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题13-proposition-13&#34;&gt;命题13 &lt;code&gt;Proposition 13&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;B60E42B5CCA44D868E37DF76566B40E6&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;命题12的特例：&lt;code&gt;$y=x$&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;命题14-proposition-14&#34;&gt;命题14 &lt;code&gt;Proposition 14&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;BC88FE57E0944DE3948B4951F018FE2E&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;命题13的特例：&lt;code&gt;A&lt;/code&gt;是对称矩阵&lt;/p&gt;
&lt;h2 id=&#34;命题15-propostion-15&#34;&gt;命题15 &lt;code&gt;Propostion 15&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;F170E399371045DF9F35F6F7D1C26F88&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof-6&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;125A8F79322D473EB74D3DCD7A43D8E3&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;corner-feature-detectorintensity-based&#34;&gt;Corner Feature Detector(Intensity-Based)&lt;/h1&gt;
&lt;p&gt;基于光强比较的角点检测，直接比较光强（像素灰度值），而不计算梯度。所以实时性更好，所需的存储空间更小。&lt;/p&gt;
&lt;h2 id=&#34;susan-角点检测&#34;&gt;SUSAN 角点检测&lt;/h2&gt;
&lt;p&gt;SUSAN 全称Smallest univalue segment assimilating nucleus，最小核同值区。提出者Smith与Brady, 1997.&lt;/p&gt;
&lt;p&gt;SUSAN 使用一个圆形模板和一个圆的中心点，通过圆的中心点象素值与模板圆内其他象素值的比较，统计出与圆中心点象素值近似的象素数量，当这样的象素数量小于某一阈值时，则该圆中心点就被认为是角点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;圆形模板：通常是半径为3.5，37个像素的圆形&lt;/li&gt;
&lt;li&gt;圆形模板中心点：圆心位置的像素&lt;/li&gt;
&lt;li&gt;最小核同值区：像素值与圆心位置像素值接近的区域和（颜色接近的区域）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有两种划分（了解）：&lt;/p&gt;
&lt;p&gt;平滑划分：$c\left(\vec{r}, \vec{r}&lt;em&gt;{0}\right)=e^{-\left(\frac{I(r)-I\left(r&lt;/em&gt;{0}\right)}{t}\right)^{6}}$&lt;/p&gt;
&lt;p&gt;直接划分：$c\left(\vec{r}, \vec{r}&lt;em&gt;{0}\right)=\left{\begin{array}{ll}1 &amp;amp; \text { if }\left|I(\vec{r})-I\left(\vec{r}&lt;/em&gt;{0}\right)\right| \leq t \ 0 &amp;amp; \text { if }\left|I(\vec{r})-I\left(\vec{r}_{0}\right)\right|&amp;gt;t\end{array}\right.$&lt;/p&gt;
&lt;p&gt;像素个数：$n\left(x_{0}, y_{0}\right)=\sum_{(x, y) \neq\left(x_{0}, y_{0}\right)} c(x, y)$&lt;/p&gt;
&lt;p&gt;像素个数与阈值$g$进行比较，以此判断角点。
$$
R\left(\vec{r}&lt;em&gt;{0}\right)=\left{\begin{array}{cc}
g-n\left(\vec{r}&lt;/em&gt;{0}\right) &amp;amp; n\left(\vec{r}_{0}\right)&amp;lt;g \&lt;br&gt;
0 &amp;amp; \text { otherwise }
\end{array}\right.
$$&lt;/p&gt;
&lt;h2 id=&#34;fast-角点检测&#34;&gt;FAST 角点检测&lt;/h2&gt;
&lt;p&gt;实时性好，不具有旋转不变性。&lt;/p&gt;
&lt;p&gt;主要思想：比较中心像素与圆内（这里的圆内指的是圆边经过的像素）16个像素，如果圆内存在n个相邻的像素块都比中心像素的亮度$I_p$加上一个阈值$t$亮，或者都比$I_p - t$暗，则就判断其为角点。n通常选择为12. FAST uses binary comparison with each pixel along a circle pattern against the central pixel。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codefmeister/MarkDownResource/master/FAST_fig1.png&#34; alt=&#34;FAST算法描述，Cover知乎@南沙渔阳&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;算法描述：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;以候选的像素角点为中心，画一个半径为3的圆，这样圆上就会有16个像素块。&lt;/li&gt;
&lt;li&gt;比较pixel 1、pixel 9与中心像素的亮度值。如果它们距离$I_p$都在$p$的阈值内，即比$I_p + t$亮或比$I_p - t$暗，则该像素$p$不可能是角点。&lt;/li&gt;
&lt;li&gt;如果未能排除角点可能性，则判断pixel 5、pixel 13（可以观察到5、13是正交于1和9的）。如果$p$是角点，那么在1,9,5,13中至少有3个像素全部比$I_p + t$亮或全部比$I_p - t$暗。如果未满足该条件，则像素$p$不可能是角点。&lt;/li&gt;
&lt;li&gt;若还不能排除角点的可能性，则对所有16个像素块进行测试。判断n与12的大小。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在SUSAN上进行了改进，FAST效率极高，具有高重复性。但仍然存在一些问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果圆环上的16个像素，一半明显比中心暗，一半明显比中心亮，则也有可能是角点。但会被FAST拒绝。&lt;/li&gt;
&lt;li&gt;检测器的相率取决于像素点判断的顺序，无法保证最优。&lt;/li&gt;
&lt;li&gt;相邻的几个特征像素点容易被重复检测为角点（可以使用非极大值抑制）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;fast-er-角点检测&#34;&gt;FAST-ER 角点检测&lt;/h2&gt;
&lt;p&gt;FAST-ER，Rosten 2010年提出的对FAST算法的改进。&lt;/p&gt;
&lt;p&gt;主要改进在于：使用ID3决策树策略对FAST进行了加速，使得其更快。&lt;/p&gt;
&lt;p&gt;关于ID3决策树，可以参考：https://blog.csdn.net/weixin_43977640/article/details/114336485&lt;/p&gt;
&lt;p&gt;每个像素点$x$都能将全部像素构成的集合$P$分为三个集合$P_d, P_b, P_s$，即比$I_p-t$暗，比$I_p+t$亮，与之类似。这样就可以根据像素点$x$来对决策树进行进一步的划分。&lt;/p&gt;
&lt;p&gt;一个任意的角点集合的信息熵：
$$
H(Q)=(c+\bar{c}) \log _{2}(c+\bar{c})-c \log _{2} c-\bar{c} \log _{2} \bar{c}
$$&lt;/p&gt;
&lt;p&gt;$c$是集合中的角点数量，$\bar c$是集合中的非角点数量。就是传统的信息熵计算方法。只不过把概率拆开写了。&lt;/p&gt;
&lt;p&gt;而依据$x$会将训练集中的全部像素构成的集合划分为三个子集，作者规定他们的贡献度都一样，所以$x$的信息增益：
$$
H_{g}=H(P)-H\left(P_{d}\right)-H\left(P_{s}\right)-H\left(P_{b}\right)
$$&lt;/p&gt;
&lt;p&gt;计算周围16个像素点的信息增益，选择增益最大的$x$，然后进一步递归构建决策树，直到递归结束。&lt;/p&gt;
&lt;p&gt;这样构建出来的决策树可以正确分类训练集中的所有角点，因此非常近似地体现了FAST角点检测器的规则。&lt;/p&gt;
&lt;h2 id=&#34;agast-角点检测&#34;&gt;AGAST 角点检测&lt;/h2&gt;
&lt;p&gt;FAST-ER存在的问题：ID3是贪心算法，可能只是局部最优；构造的树为三叉树，对于机器来说二叉树效率更高。&lt;/p&gt;
&lt;p&gt;AGAST通过在一个扩展的配置空间上训练一个最优的决策树，来优化角点判断的决策。&lt;/p&gt;
&lt;p&gt;这篇Paper我没看。所以只简略说明一下。&lt;/p&gt;
&lt;h2 id=&#34;orb&#34;&gt;ORB&lt;/h2&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description></description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description></description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;语言模型&#34;&gt;语言模型&lt;/h1&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;DCP用到了&lt;code&gt;Transformer&lt;/code&gt;，其很多实现的Motivation都来自于Sq2Sq的启发。所以重新回顾学习语言模型。&lt;/p&gt;
&lt;h2 id=&#34;语言模型-1&#34;&gt;语言模型&lt;/h2&gt;
&lt;p&gt;语言模型是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。 我们可以把一段自然语言文本看做一段离散的时间序列。假设一段长度为T的文本中的词依次为&lt;code&gt;$w_1,w_2,...,w_T$&lt;/code&gt;， 那么在离散的时间序列中，&lt;code&gt;$w_t(1 \le t \le T )$&lt;/code&gt;可以看做在时间步&lt;code&gt;t&lt;/code&gt;的输出。给定一个长度为T的词的序列&lt;code&gt;$w_1,w_2,...,w_T$&lt;/code&gt;。语言模型将计算该序列的概率：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;P(w_1,w_2,...,w_T)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;语言模型的计算&#34;&gt;语言模型的计算&lt;/h2&gt;
&lt;p&gt;假设序列&lt;code&gt;$w_1,w_2,...,w_T$&lt;/code&gt;中的各个词是依次生成的，我们有：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;P(w_1,w_2,...w_T) = \prod\nolimits_{t = 1}^{\rm{T}} {P(w_t|w_1,...,w_{t-1})}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;例如，一段含有四个词的文本序列的概率：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;P(w_1,w_2,w_3,w_4) = P(w_1)P(w2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了计算语言模型，我们需要计算词的概率，以及一个词在给定的前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，词的概率可以通过该词在训练数据集中的相对词频来计算。例如，&lt;code&gt;$P(w_1)$&lt;/code&gt;可以计算为&lt;code&gt;$w_1$&lt;/code&gt;在训练数据集中的词频与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如&lt;code&gt;$P(w_2|w_1)$&lt;/code&gt;可以计算为&lt;code&gt;$w_1,w_2$&lt;/code&gt;两词相邻的频率与&lt;code&gt;$w_1$&lt;/code&gt;词频的比值，即&lt;code&gt;$P(w_1,w_2)$&lt;/code&gt;与&lt;code&gt;$P(w_1)$&lt;/code&gt;之比。以此类推。&lt;/p&gt;
&lt;h2 id=&#34;n元语法&#34;&gt;n元语法&lt;/h2&gt;
&lt;p&gt;当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。n元语法通过马尔可夫假设（并不一定成立）简化了语言模型的计算。这里的马尔科夫假设是指一个词的出现如果只与前面n个词相关，即n阶马尔科夫链。如果&lt;code&gt;$n=1$&lt;/code&gt;，那么有：&lt;code&gt;$P(w_3|w_1,w_2) = P(w_3|w_2)$&lt;/code&gt;。如果基于&lt;code&gt;$n-1$&lt;/code&gt;阶马尔科夫链，我们可以将语言模型改写为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;P(w_1,w_2,...,w_T) \approx \prod\nolimits_{t=1}^{\rm{T}} {P(w_t|w_{t-(n-1)},...,w_{t-1})}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上称为&lt;strong&gt;n元语法(n-grams)&lt;/strong&gt;。它是基于&lt;strong&gt;n-1&lt;/strong&gt;阶马尔科夫链的概率语言模型。当n分别为1，2和3时，我们将其分别称作一元语法(unlgram)、二元语法(blgram)和三元语法(trlgram)。例如，长度为4的序列&lt;code&gt;$w_1,w_2,w_3,w_4$&lt;/code&gt;在一元语法，二元语法和三元语法中的概率分别为：
&lt;img src=&#34;54496B59912A4336B180096EC8F7BBD4&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;当n较小时，n元语法往往并不准确。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Bayesian_inference&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Wikipedia:Bayesian_inference&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Bayesian inference is a method of statistical inference in which Bayes&#39; theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called &amp;ldquo;Bayesian probability&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;贝叶斯推理是一种统计推理方法，利用贝叶斯定理更新假设的概率，来获得更多的证据与信息。贝叶斯推理是统计学尤其是数理统计中的一项重要技术。贝叶斯更新在对于序列化数据的动态分析中非常重要。贝叶斯推断被广泛应用于科学研究，工程等领域。在决策理论哲学中，贝叶斯推理与主观概率密切相关，经常被称为贝叶斯概率。&lt;/p&gt;
&lt;h1 id=&#34;贝叶斯规则简介introduction-to-bayes-rule&#34;&gt;贝叶斯规则简介(&lt;code&gt;Introduction to Bayes&#39; rule&lt;/code&gt;)&lt;/h1&gt;
&lt;h2 id=&#34;形式化解释formal-explanation&#34;&gt;形式化解释(&lt;code&gt;Formal explanation&lt;/code&gt;)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a &amp;ldquo;likelihood function&amp;rdquo; derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes&#39; theorem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;贝叶斯推理根据两个前因式的结果来得到后验概率：一个&lt;strong&gt;先验概率&lt;/strong&gt;；一个由观测数据的统计模型得出的&lt;strong&gt;似然函数&lt;/strong&gt;。贝叶斯推理根据贝叶斯公式来计算后验概率。&lt;/p&gt;
&lt;p&gt;贝叶斯公式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;P(H|E) = \frac{{P(E|H) \cdot P(H)}}{{P(E)}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上述公式中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$H$&lt;/code&gt;代表其概率可能受到数据（下称证据&lt;em&gt;evidence&lt;/em&gt;)影响的任何假设。通常这些假设是相互竞争的，而我们的任务就是决定哪一个是最有可能的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(H)$&lt;/code&gt;,&lt;strong&gt;先验概率&lt;/strong&gt;(&lt;em&gt;prior probability&lt;/em&gt;),是在数据&lt;code&gt;$E$&lt;/code&gt;（即当前得到的证据）被观测到前，对假设&lt;code&gt;$H$&lt;/code&gt;的概率估计。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$E$&lt;/code&gt;,即&lt;strong&gt;证据&lt;/strong&gt;(&lt;em&gt;evidence&lt;/em&gt;),指那些未被用于计算先验概率的新数据。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(H|E)$&lt;/code&gt;，&lt;strong&gt;后验概率&lt;/strong&gt;（&lt;em&gt;posterior probability&lt;/em&gt;），是指&lt;code&gt;$H$&lt;/code&gt;给予&lt;code&gt;$E$&lt;/code&gt;以后的概率，即在观测到证据&lt;code&gt;$E$&lt;/code&gt;以后，更新的概率。后验概率就是我们想要得到的：在当前观测到的证据下，某个假设发生的概率有多大。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(E|H)$&lt;/code&gt;, 是在假设&lt;code&gt;$H$&lt;/code&gt;的前提下观测到证据&lt;code&gt;$E$&lt;/code&gt;的概率，被称为&lt;strong&gt;似然函数&lt;/strong&gt;(&lt;em&gt;likelihood&lt;/em&gt;)。作为固定&lt;code&gt;$H$&lt;/code&gt;下&lt;code&gt;$E$&lt;/code&gt;的函数，它体现了当前证据与给定假设的相容性。似然函数是证据&lt;code&gt;$E$&lt;/code&gt;的函数，而后验概率是假设&lt;code&gt;$H$&lt;/code&gt;的函数。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(E)$&lt;/code&gt;，被称作&lt;strong&gt;边际似然函数&lt;/strong&gt;或者&lt;strong&gt;模型证据&lt;/strong&gt;。该因子对所有被考虑到的可能的假设都相同（可以明显的看出，符号表达式中并没有&lt;code&gt;$H$&lt;/code&gt;），所以该因子不会影响各个假设间的相对概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于不同的&lt;code&gt;$H$&lt;/code&gt;，只有&lt;code&gt;$P(H)$&lt;/code&gt;和&lt;code&gt;$P(E|H)$&lt;/code&gt;这两项在分子上的因子会影响后验概率&lt;code&gt;$P(H|E)$&lt;/code&gt;的值。也就是说，后验概率与其先验概率（固有的可能性）和新获得的似然函数（与新获得的证据的相容性）成正比。&lt;/p&gt;
&lt;p&gt;贝叶斯规则也可以被写成如下形式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;\frac{{P(E|H)P(H)}}{{P(E)}} = \frac{{P(E|H)P(H)}}{{P(E|H)P(H) + P(E|\neg H)P(\neg H)}} = \frac{1}{{1 + (\frac{1}{{P(H)}} - 1)\frac{{P(E|\neg H)}}{{P(E|H)}}}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这是由于：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;P(E) = {P(E|H)P(H) + P(E|\neg H)P(\neg H)}  

P(H)+P(\neg H)=1
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;贝叶斯推理的形式化描述formal-description-of-bayesian-inference&#34;&gt;贝叶斯推理的形式化描述(&lt;code&gt;Formal description of Bayesian inference&lt;/code&gt;)&lt;/h1&gt;
&lt;h2 id=&#34;定义-definitions&#34;&gt;定义 &lt;code&gt;Definitions&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$x$&lt;/code&gt;： 一个数据点，事实上可能是一个值向量&lt;code&gt;vector&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\theta$&lt;/code&gt;，数据点所对应的分布的参数，即&lt;code&gt;$x \sim p(x|\theta)$&lt;/code&gt;。事实上，&lt;code&gt;$\theta$&lt;/code&gt;可能是许多参数组成的向量。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\alpha$&lt;/code&gt;，参数分布的超参数，即&lt;code&gt;$\theta \sim p(\theta | \alpha)$&lt;/code&gt;。可能是由很多超参数构成的一个向量。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$X$&lt;/code&gt;代表采样，一个由&lt;code&gt;$n$&lt;/code&gt;个观测的数据点构成的集合。即&lt;code&gt;$x_1,...,x_n$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;${\tilde x}$&lt;/code&gt;，一个新的数据点，其分布需要被预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;贝叶斯推理-bayesian-inference&#34;&gt;贝叶斯推理 &lt;code&gt;Bayesian inference&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;先验分布&lt;/strong&gt; &lt;em&gt;prior distribution&lt;/em&gt;，是指参数在没有任何新数据被观测到的情况下的概率分布，即&lt;code&gt;$p(\theta|\alpha)$&lt;/code&gt;。先验分布可能不容易确定，在这种情况下，我们可以先采用&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Jeffreys_prior&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jeffrets Prior&lt;/a&gt;去获得一个先验分布的初始值，然后使用观测到的数据进行更新迭代。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;采样分布&lt;/strong&gt; &lt;em&gt;sample distribution&lt;/em&gt;，是指观测数据&lt;code&gt;$X$&lt;/code&gt;在其参数条件下的分布，即&lt;code&gt;$p(X|\theta)$&lt;/code&gt;，更确切的说，由于&lt;code&gt;$\theta$&lt;/code&gt;服从参数条件&lt;code&gt;$\alpha$&lt;/code&gt;下的概率分布，采样分布也可以写为&lt;code&gt;$p(X|\theta,\alpha)$&lt;/code&gt;。但是为了不引起歧义与混淆，我们一般都写为&lt;code&gt;$p(X|\theta)$&lt;/code&gt;。采样分布有时候也被称为&lt;strong&gt;似然函数&lt;/strong&gt;, 尤其是当其被视为是参数&lt;code&gt;$\theta$&lt;/code&gt;的函数时。有时候写作&lt;code&gt;$L(\theta|X) = p(X|\theta)$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;边际似然函数&lt;/strong&gt; &lt;em&gt;marginal likelihood&lt;/em&gt;，有时也被称为证据&lt;em&gt;evidence&lt;/em&gt;，是观测数据&lt;code&gt;marginalized out&lt;/code&gt; &lt;code&gt;$\theta$&lt;/code&gt;后得到的边缘分布，即&lt;code&gt;$p(X|\alpha ) = \int_\theta  {p(X|\theta )p(\theta |\alpha )d\theta }$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后验分布&lt;/strong&gt; &lt;em&gt;posterior distribution&lt;/em&gt; 是指在考虑新观测的数据后的参数分布。它由贝叶斯规则决定，形成了贝叶斯推理的核心。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;p(\theta |X,\alpha ) = \frac{{p(\theta ,X,\alpha )}}{{p(X,\alpha )}} = \frac{{p(X|\theta ,\alpha )p(\theta ,\alpha )}}{{p(X|\alpha )p(\alpha )}} = \frac{{p(X|\theta ,\alpha )p(\theta |\alpha )}}{{p(X|\alpha )}} \propto p(X|\theta ,\alpha )p(\theta |\alpha )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用语言描述就是：&lt;strong&gt;后验正比于先验乘以似然。&lt;/strong&gt; &lt;strong&gt;后验等于似然乘以先验除以证据&lt;/strong&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;posterior is proportional to likelihood times prior&amp;rdquo;, or sometimes as &amp;ldquo;posterior = likelihood times prior, over evidence&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;贝叶斯预测-bayesian-prediction&#34;&gt;贝叶斯预测 &lt;code&gt;Bayesian Prediction&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;后验预测分布&lt;/strong&gt; &lt;em&gt;Posterior predictive distribution&lt;/em&gt;，是新的数据点的概率分布。通过将后验概率边缘化而得到的。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;p(\tilde x|X,\alpha ) = \int {p(\tilde x|\theta )p(\theta |X,\alpha )d\theta } 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;先验预测分布&lt;/strong&gt; &lt;em&gt;Prior predictive distribution&lt;/em&gt;， 是新的数据点的概率分布，在先验概率上边缘化得到的。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;p(\tilde x|\alpha ) = \int {p(\tilde x|\theta )p(\theta |\alpha )d\theta } 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;贝叶斯理论要求使用后验预测分布来进行预测推断，即预测新的，未观测到的数据点的分布。也就是说，&lt;strong&gt;不再将一个固定点作为预测结果，而是返回一个可能点的分布。&lt;/strong&gt; 只有这样才能使用参数&lt;code&gt;$\theta$&lt;/code&gt;的整个后验分布。相比之下，频率统计学中的预测常常需要寻找当前参数下的一个最优点估计，例如通过最大似然或者最大后验估计（MAP）。然后将这个最优点代入点的分布公式中。这样做的缺点是，它没有考虑任何参数的不确定性，所以会降低预测分布的方差。&lt;/p&gt;
&lt;p&gt;两种类型的预测分布都有复合概率分布的形式（所以才有边际似然函数）。事实上，如果先验分布是共轭先验，那么先验分布和后验分布便来自于同一族，可以很容易看出，先验预测分布和后验预测分布同样来自于同一族的复合分布。唯一的不同在于，后验预测分布使用超参数更新后的值，而先验预测分布使用先验分布中出现的超参数的值。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;边际似然函数&#34;&gt;边际似然函数&lt;/h1&gt;
&lt;p&gt;统计学中，&lt;strong&gt;边际似然函数&lt;/strong&gt;（&lt;em&gt;marginal likelihood function&lt;/em&gt; 或 &lt;em&gt;integrated likelihood&lt;/em&gt;）是一种似然函数，其中某些参数变量被边缘化。在贝叶斯统计的背景下，它常常代指证据&lt;em&gt;evidence&lt;/em&gt;或模型证据&lt;em&gt;model evidence&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;概念&#34;&gt;概念&lt;/h2&gt;
&lt;p&gt;给定一组独立同分布的数据点&lt;code&gt;$X = ({x_1}, \ldots ,{x_n})$&lt;/code&gt;,其中&lt;code&gt;${x_i} \sim p({x_i}|\theta )$&lt;/code&gt;,&lt;code&gt;$p({x_i}|\theta )$&lt;/code&gt;是一个概率分布，其参数为&lt;code&gt;$\theta$&lt;/code&gt;，其中&lt;code&gt;$\theta$&lt;/code&gt;本身就是一个随机变量，可以用一个概率分布来描述，即&lt;code&gt;$\theta  \sim p(\theta |\alpha )$&lt;/code&gt;。而&lt;strong&gt;边际似然函数就是求概率&lt;code&gt;$p(X|\alpha)$&lt;/code&gt;是多少，其中参数&lt;code&gt;$\theta$&lt;/code&gt;被边缘化（&lt;em&gt;marginalized out&lt;/em&gt;)而消失&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;p(X|\alpha ) = \int_\theta  {p(X|\theta )p(\theta |\alpha )d\theta } 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上述定义是在贝叶斯统计下提出的。&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;今日收获&#34;&gt;今日收获&lt;/h1&gt;
&lt;h2 id=&#34;收获&#34;&gt;收获&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;可以使用TopK来代替NearestNeighbor，可以加快计算的速度。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;今天的主要问题还是：KNN的反向传播问题。不论是KNN还是TopK，都无法反向传播梯度，为整体训练带来困难。&lt;/p&gt;
&lt;p&gt;找到两个非常有用的网页：
1.&lt;/p&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://codefmeister.github.io/p/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://codefmeister.github.io/p/</guid>
        <description>&lt;h1 id=&#34;高斯混合模型&#34;&gt;高斯混合模型&lt;/h1&gt;
&lt;h2 id=&#34;混合模型概述&#34;&gt;混合模型概述&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with &amp;ldquo;mixture distributions&amp;rdquo; relate to deriving the properties of the overall population from those of the sub-populations, &amp;ldquo;mixture models&amp;rdquo; are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从统计学角度来说，一个混合模型就是一种概率模型，用于表示总体当中子总体的存在，而不需要观测数据集识别出这个观测数据属于哪一个子总体(子分布)。&lt;br&gt;
形式上讲，对应混合分布的一个混合模型，就代表了这个总体的概率密度分布。然而，但需要从子总体的性质推导总体的一些性质时，混合模型能够直接根据总体池的观测值来对子总体的特性进行统计推断，而不需要知道他们的归属信息(属于哪一个子总体)。&lt;/p&gt;
&lt;h2 id=&#34;mixture-model-structure&#34;&gt;Mixture Model Structure&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;A typical finite-dimensional mixture model is a hierarchical model consisting of the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N random variables that are observed, each distributed according to a mixture of K components, with the components belonging to the same parametric family of distributions (e.g., all normal, all Zipfian, etc.) but with different parameters&lt;/li&gt;
&lt;li&gt;N random latent variables specifying the identity of the mixture component of each observation, each distributed according to a K-dimensional categorical distribution&lt;/li&gt;
&lt;li&gt;A set of K mixture weights, which are probabilities that sum to 1.&lt;/li&gt;
&lt;li&gt;A set of K parameters, each specifying the parameter of the corresponding mixture component. In many cases, each &amp;ldquo;parameter&amp;rdquo; is actually a set of parameters. For example, if the mixture components are Gaussian distributions, there will be a mean and variance for each component. If the mixture components are categorical distributions (e.g., when each observation is a token from a finite alphabet of size V), there will be a vector of V probabilities summing to 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, in a Bayesian setting, the mixture weights and parameters will themselves be random variables, and prior distributions will be placed over the variables. In such a case, the weights are typically viewed as a K-dimensional random vector drawn from a Dirichlet distribution (the conjugate prior of the categorical distribution), and the parameters will be distributed according to their respective conjugate priors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个典型的有限维度的混合模型是一个分层的模型，有着如下的&lt;code&gt;components&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$N$&lt;/code&gt;个被观测的随机变量&lt;code&gt;random variables&lt;/code&gt;，每个随机变量都按&lt;code&gt;$K$&lt;/code&gt;个子分布(&lt;code&gt;component&lt;/code&gt;)构成的混合模型而分布，这些子分布都属于同一类分布，但是具体的参数值不同。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$N$&lt;/code&gt;个隐变量&lt;code&gt;latent variables&lt;/code&gt;，每一个隐变量都说明了对应的随机变量所属的子分布是哪一个。每一个隐变量都按&lt;code&gt;$K$&lt;/code&gt;维分类分布（即隐变量的取值只有&lt;code&gt;$K$&lt;/code&gt;个）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$K$&lt;/code&gt;个混合权重，每个混合权重指定了某个子分布所占的总体的权重。混合权重的和加起来应等于1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$K$&lt;/code&gt;个参数组，每一个参数组都对应着一个子分布。如高斯混合模型中，每个参数组中的参数有均值和方差。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，在贝叶斯假设下，混合权重和参数组将本身就是随机变量，每个都会有一个先验分布。在这种情况下，混合权重可以被视为一个&lt;code&gt;$K$&lt;/code&gt;维的随机向量，由狄利克雷分布(分类分布的共轭先验)得出，而参数组将根据各自的先验共轭分布而分布。（关于先验概率与后验概率在这里不表。）&lt;/p&gt;
&lt;p&gt;从数学角度出发，一个基础的参数化的混合模型可以被以下参数所描述：
&lt;img src=&#34;B3A612BB18074788B100784FCB4F70D9&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;参数解读：&lt;br&gt;
&lt;code&gt;$K$&lt;/code&gt; 表示&lt;code&gt;mixture component&lt;/code&gt;的个数，即混合分布中子分布的个数。&lt;br&gt;
&lt;code&gt;$N$&lt;/code&gt; 表示被观测的随机变量的个数。&lt;br&gt;
&lt;code&gt;${\theta _{{\rm{i}} = 1...K}}$&lt;/code&gt;表示第&lt;code&gt;$i$&lt;/code&gt;个子分布&lt;code&gt;component&lt;/code&gt;的参数值。&lt;br&gt;
&lt;code&gt;${\phi _{{\rm{i}} = 1...K}}$&lt;/code&gt;表示混合权重，即某个具体的子分布&lt;code&gt;component&lt;/code&gt;的先验概率。&lt;br&gt;
&lt;code&gt;$\Phi$&lt;/code&gt;表示由&lt;code&gt;${\phi _{{\rm{i}} = 1...K}}$&lt;/code&gt;组成的K维向量，和为1.&lt;br&gt;
&lt;code&gt;${z _{{\rm{i}} = 1...N}}$&lt;/code&gt;表示第&lt;code&gt;$i$&lt;/code&gt;个观测值所属的&lt;code&gt;component&lt;/code&gt;(子分布)。&lt;br&gt;
&lt;code&gt;${x _{{\rm{i}} = 1...N}}$&lt;/code&gt;表示第&lt;code&gt;$i$&lt;/code&gt;个观测的随机变量。&lt;br&gt;
&lt;code&gt;$F(x|\theta )$&lt;/code&gt;表示某个被观测的随机变量在参数组为&lt;code&gt;$\theta$&lt;/code&gt;下的概率分布。&lt;br&gt;
&lt;code&gt;${z _{{\rm{i}} = 1...N}}$&lt;/code&gt;服从以&lt;code&gt;$\Phi$&lt;/code&gt;为概率的分类分布（共&lt;code&gt;$K$&lt;/code&gt;类）。 即：&lt;code&gt;${z_{i = 1...N}} \sim Categorical(\Phi )$&lt;/code&gt;&lt;br&gt;
&lt;code&gt;${x_{i = 1...N}}|{z_{i = 1...N}}$&lt;/code&gt; 服从&lt;code&gt;$F(\theta _{z_i} )$&lt;/code&gt;，即随机变量&lt;code&gt;$x_i$&lt;/code&gt;服从其对应&lt;code&gt;component&lt;/code&gt;（子分布）&lt;code&gt;$z_i$&lt;/code&gt;的参数组&lt;code&gt;$\theta _{z_i}$&lt;/code&gt;指定的概率分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：以上参数都是在不是在贝叶斯假设下的。&lt;/p&gt;
&lt;p&gt;在贝叶斯假设下，所有参数都与随机变量相关，如下图：
&lt;img src=&#34;A57CC57659D44B56BF8FA958F4FC5603&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;参数解读：&lt;br&gt;
&lt;code&gt;$K$&lt;/code&gt;： 同上&lt;br&gt;
&lt;code&gt;$N$&lt;/code&gt;： 同上&lt;br&gt;
&lt;code&gt;$\theta _{i=1...K}$&lt;/code&gt;： 同上&lt;br&gt;
&lt;code&gt;$\phi _{i=1...K}$&lt;/code&gt;： 同上&lt;br&gt;
&lt;code&gt;$\Phi$&lt;/code&gt;： 同上&lt;br&gt;
&lt;code&gt;$z _{i=1...N}$&lt;/code&gt;：同上&lt;br&gt;
&lt;code&gt;$x_{i=1...N}$&lt;/code&gt;：同上&lt;br&gt;
&lt;code&gt;$F(x|\theta)$&lt;/code&gt;：同上&lt;br&gt;
&lt;code&gt;$\alpha$&lt;/code&gt;：各子分布&lt;code&gt;component&lt;/code&gt;参数的共用的超参数&lt;br&gt;
&lt;code&gt;$\beta$&lt;/code&gt;: 混合权重的共用的超参数&lt;br&gt;
&lt;code&gt;$H(\theta|\alpha)$&lt;/code&gt;： 子分布&lt;code&gt;component&lt;/code&gt;参数的先验概率分布，基于参数&lt;code&gt;$\alpha$&lt;/code&gt;。&lt;br&gt;
&lt;code&gt;$\theta _{i=1...K}$&lt;/code&gt;： 服从概率分布&lt;code&gt;$H(\theta|\alpha)$&lt;/code&gt;,即&lt;code&gt;$ \theta _ {i=1...K} \sim H(\theta|\alpha)$&lt;/code&gt;&lt;br&gt;
&lt;code&gt;$\Phi$&lt;/code&gt;： 服从&lt;code&gt;$Symmetric-Dirichlet _K(\beta)$&lt;/code&gt;分布。&lt;br&gt;
&lt;code&gt;$z_{i=1...N}|\Phi$&lt;/code&gt;：服从&lt;code&gt;$Categorical(\phi)$&lt;/code&gt;，即以&lt;code&gt;$\Phi$&lt;/code&gt;为概率的分类分布。&lt;br&gt;
&lt;code&gt;$x_{i=1...N}|z_{i=1...N},\theta_{i=1...K}$&lt;/code&gt;：服从&lt;code&gt;$F(\theta_{z_i})$&lt;/code&gt;的分布。&lt;/p&gt;
&lt;p&gt;我们使用&lt;code&gt;$F$&lt;/code&gt;和&lt;code&gt;$H$&lt;/code&gt;来对观测值和参数进行任意描述。一般来说，&lt;code&gt;$H$&lt;/code&gt;是&lt;code&gt;$F$&lt;/code&gt;的共轭先验。两个最常见的&lt;code&gt;$F$&lt;/code&gt;的选择是：高斯分布，即正态分布（对实值观测值），或者是分类分布（对离散观测值）。其他常见的可以作为混合组件的概率分布有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;二项分布&lt;code&gt;Binomial distribution&lt;/code&gt;: 对于某一事物总数固定，统计其&lt;code&gt;positive occurrence&lt;/code&gt;。如投票等。&lt;/li&gt;
&lt;li&gt;多项分布&lt;code&gt;Multinomial distribution&lt;/code&gt;： 类似于二项分布，不过事情的结果可能不止有两个。&lt;/li&gt;
&lt;li&gt;负二项分布&lt;code&gt;Negative binomial distribution&lt;/code&gt;： 对于二项分布类型的观测值，感兴趣的是在某个给定的次数的&lt;code&gt;positive&lt;/code&gt;结果出现前，&lt;code&gt;negative&lt;/code&gt;结果出现的次数。&lt;/li&gt;
&lt;li&gt;泊松分布&lt;code&gt;Poisson distribution&lt;/code&gt;：统计某一事件在给定时间内发生的次数，该事件具有固定的发生率。&lt;/li&gt;
&lt;li&gt;指数分布&lt;code&gt;Exponential distribution&lt;/code&gt;：某个事件下一次出现所需要的的时间的分布，该事件具有固定的发生率。&lt;/li&gt;
&lt;li&gt;对数正态分布&lt;code&gt;Log-normal distribution&lt;/code&gt;： 用于那些假定呈指数增长的正实数，如收入或者价格。&lt;/li&gt;
&lt;li&gt;多元正态分布&lt;code&gt;Multivariate normal distribution&lt;/code&gt;：即多元高斯分布。结果向量的每一个分量都是一个高斯分布。&lt;/li&gt;
&lt;li&gt;多元t分布&lt;code&gt;Multivariate Student&#39;s-t distribution&lt;/code&gt;：用于重尾相关结果的向量。&lt;/li&gt;
&lt;li&gt;伯努利分布值的向量，对应于例如黑白图像，每个值代表一个像素，可应用于手写识别。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;非贝叶斯假设下的高斯混合模型&#34;&gt;非贝叶斯假设下的高斯混合模型&lt;/h2&gt;
&lt;p&gt;其各个参数为：&lt;br&gt;
&lt;img src=&#34;16877F0AFF554BAB959BFBDFE1B7E062&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;对应上文很容易理解，不再赘述。&lt;br&gt;
图示：
&lt;img src=&#34;78680242A7444DDF9DA77D19A51FFB16&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;贝叶斯假设下的高斯混合模型&#34;&gt;贝叶斯假设下的高斯混合模型&lt;/h2&gt;
&lt;p&gt;其各个参数为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;13052BB3F4344EC1A6E698E3FBDDE94E&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;其中值得特殊说明的是：&lt;br&gt;
&lt;code&gt;${\mu _0},\lambda ,\nu ,\sigma _0^2$&lt;/code&gt;: 是&lt;code&gt;$\theta$&lt;/code&gt;即&lt;code&gt;$\mu $&lt;/code&gt;与&lt;code&gt;$\sigma$&lt;/code&gt;共享的超参数。&lt;br&gt;
&lt;code&gt;$\mu_{i=1...K}$&lt;/code&gt;：&lt;code&gt;$\mu_{i=1...K} \sim N(\mu_0,\lambda\sigma _i^2)$&lt;/code&gt;,即参数&lt;code&gt;$\mu$&lt;/code&gt;服从以&lt;code&gt;$mu_0,\lambda\sigma _i^2$&lt;/code&gt;为参数的高斯分布。&lt;br&gt;
&lt;code&gt;$\sigma_{i=1...K}^2$&lt;/code&gt;：&lt;code&gt;$\sigma_{i=1...K}^2 \sim Inverse-Gamma(\nu,\sigma_0^2)$&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;多元高斯混合模型&#34;&gt;多元高斯混合模型&lt;/h2&gt;
&lt;p&gt;一个贝叶斯高斯混合模型常常被推广去拟合未知的参数向量(下面用粗体表示），或者多元正态分布。在多元分布中（即对具有&lt;code&gt;$N$&lt;/code&gt;个随机变量的向量&lt;code&gt;$\bm{x}$&lt;/code&gt;），我们可以使用高斯混合模型的先验分布的矢量估计来对该&lt;code&gt;$\bm{x}$&lt;/code&gt;进行建模：&lt;br&gt;
&lt;img src=&#34;35B511A935934CA78E4C6A1C3CCDAA2F&#34; alt=&#34;image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;其中第&lt;code&gt;$i$&lt;/code&gt;个向量子分布&lt;code&gt;component&lt;/code&gt;被权重为&lt;code&gt;${\phi _i}$&lt;/code&gt;,方差为&lt;code&gt;$\bm{\mu}$&lt;/code&gt;，协方差矩阵为&lt;code&gt;$\bm{\sum _i}$&lt;/code&gt;的正态分布所定义。为了将这个先验分布纳入贝叶斯估计，这个先验要与已知的分布&lt;code&gt;$p(\bm{x}|\bm{\theta})$&lt;/code&gt;相乘，该分布是数据&lt;code&gt;$\bm{x}$&lt;/code&gt;在待估参数&lt;code&gt;$\bm{\theta}$&lt;/code&gt;上的分布。根据如上阐述，那么后验分布&lt;code&gt;$p(\bm{\theta}|\bm{x})$&lt;/code&gt;也是一个高斯混合分布：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-math&#34; data-lang=&#34;math&#34;&gt;p(\bm{\theta} |\bm{x}) = \sum\limits_{i = 1}^K {{{\tilde \phi }_i}N({\bm{\tilde \mu }_i},{\bm{\tilde \Sigma }_i})} 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中的参数：&lt;code&gt;${\tilde \phi }_i$&lt;/code&gt;，&lt;code&gt;${\bm{\tilde \mu }_i}$&lt;/code&gt;和&lt;code&gt;${\bm{\tilde \Sigma }_i}$&lt;/code&gt;可以使用&lt;strong&gt;EM&lt;/strong&gt;算法进行更新。虽然关于EM算法的参数更新已经很完善了，但是提供对这些参数的初始估计仍然是一个十分活跃的研究领域。必须说明的是，该公式产生了一个完全后验分布的一个封闭形式的解。随机变量&lt;code&gt;$\bm{\theta}$&lt;/code&gt;的估计值可以通过取其中几个估计量的其中一个来获得，如取后验分布的均值或者最大值。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
