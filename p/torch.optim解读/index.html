<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='本文参考 lr_scheduler介绍 以及 PyTorch optim文档
 1 概述 1.1 PyTorch文档：torch.optim解读  下图是optim的文档
  TORCH.OPTIM torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.
 torch.optim简介
torch.optim是PyTorch实现的一个包，里面有各种各样的优化算法，大部分常用的优化算法都已经被支持，接口也十分通用，所以可以用来集成实现更加复杂的系统。
 How to use an optimizer To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.'><title>torch.optim解读</title>

<link rel='canonical' href='https://codefmeister.github.io/p/torch.optim%E8%A7%A3%E8%AF%BB/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='torch.optim解读'>
<meta property='og:description' content='本文参考 lr_scheduler介绍 以及 PyTorch optim文档
 1 概述 1.1 PyTorch文档：torch.optim解读  下图是optim的文档
  TORCH.OPTIM torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.
 torch.optim简介
torch.optim是PyTorch实现的一个包，里面有各种各样的优化算法，大部分常用的优化算法都已经被支持，接口也十分通用，所以可以用来集成实现更加复杂的系统。
 How to use an optimizer To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.'>
<meta property='og:url' content='https://codefmeister.github.io/p/torch.optim%E8%A7%A3%E8%AF%BB/'>
<meta property='og:site_name' content='Codefmeister'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='PyTorch' /><meta property='article:published_time' content='2020-11-21T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2020-11-21T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="torch.optim解读">
<meta name="twitter:description" content="本文参考 lr_scheduler介绍 以及 PyTorch optim文档
 1 概述 1.1 PyTorch文档：torch.optim解读  下图是optim的文档
  TORCH.OPTIM torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.
 torch.optim简介
torch.optim是PyTorch实现的一个包，里面有各种各样的优化算法，大部分常用的优化算法都已经被支持，接口也十分通用，所以可以用来集成实现更加复杂的系统。
 How to use an optimizer To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.">
    </head>
    <body class="">
        <div class="container flex on-phone--column align-items--flex-start extended article-page with-toolbar">
            <aside class="sidebar left-sidebar sticky">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header class="site-info">
        
            <figure class="site-avatar">
                
                    
                    
                    
                        
                        <img src="/img/Icon_hua25ec96b536dfdbbcc947accbc1cb594_86128_300x0_resize_q75_box.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                

                
                    <span class="emoji">🍥</span>
                
            </figure>
        
        <h1 class="site-name"><a href="https://codefmeister.github.io">Codefmeister</a></h1>
        <h2 class="site-description">Major in Computer science</h2>
    </header>

    <ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
    </ol>
</aside>

            <main class="main full-width">
    <div id="article-toolbar">
        <a href="https://codefmeister.github.io" class="back-home">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



            <span>Back</span>
        </a>
    </div>

    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <h2 class="article-title">
        <a href="/p/torch.optim%E8%A7%A3%E8%AF%BB/">torch.optim解读</a>
    </h2>

    <footer class="article-time">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--published">Nov 21, 2020</time>
    </footer></div>
</header>

    <section class="article-content">
    <blockquote>
<p>本文参考
<a class="link" href="https://blog.csdn.net/qyhaill/article/details/103043637"  target="_blank" rel="noopener"
    >lr_scheduler介绍</a> 以及
<a class="link" href="https://pytorch.org/docs/stable/optim.html"  target="_blank" rel="noopener"
    >PyTorch optim文档</a></p>
</blockquote>
<h1 id="1-概述">1 概述</h1>
<h2 id="11-pytorch文档torchoptim解读">1.1 PyTorch文档：torch.optim解读</h2>
<blockquote>
<p>下图是optim的文档</p>
</blockquote>
<blockquote>
<h4 id="torchoptim">TORCH.OPTIM</h4>
<p><code>torch.optim</code> is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.</p>
</blockquote>
<p><strong>torch.optim简介</strong><br>
torch.optim是PyTorch实现的一个包，里面有各种各样的优化算法，大部分常用的优化算法都已经被支持，接口也十分通用，所以可以用来集成实现更加复杂的系统。</p>
<blockquote>
<h4 id="how-to-use-an-optimizer">How to use an optimizer</h4>
<p>To use <code>torch.optim</code> you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</p>
</blockquote>
<p><strong>如何使用PyTorch提供的optimizer</strong><br>
通过<code>torch.optim</code>来创建一个<code>Optimizer</code>对象，这个对象中会保存当前的状态，并且会根据计算的梯度值更新参数。</p>
<blockquote>
<h4 id="constructing-it">Constructing it</h4>
<p>To construct an <code>Optimizer</code> you have to give it an iterable containing the parameters (all should be <code>Variables</code>) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.</p>
<blockquote>
<h5 id="note">NOTE</h5>
<p>If you need to move a model to GPU via <code>.cuda()</code>, please do so before constructing optimizers for it. Parameters of a model after <code>.cuda()</code> will be different objects with those before the call.     <br>
In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.</p>
</blockquote>
</blockquote>
<p><strong>构造Optimizer</strong><br>
构造<code>Optimizer</code>时，需要传入一个包含需要进行优化的所有参数的<code>iterable</code>对象，所有参数都必须是<code>Variables</code>类型。随后可以进一步设置optimizer的其他具体参数，如learning rate, weight decay, etc.<br>
<strong>注意</strong>:<br>
如果需要将模型移到cuda上(通过<code>.cuda</code>命令)，那么必须先移动模型，再对模型构造<code>optimizer</code>。因为调用<code>.cuda</code>前的模型参数与调用<code>.cuda</code>后的模型参数不同。<br>
通常来讲，在使用<code>Optimizer</code>对参数进行优化时，需要保证构造和使用时，被优化的参数保存在同一位置。</p>
<p>以下是实例：</p>
<blockquote>
<p>Example:</p>
<pre><code>optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
</code></pre></blockquote>
<blockquote>
<h4 id="per-parameter-options">Per-parameter options</h4>
<p><code>Optimizer</code>s also support specifying per-parameter options. To do this, instead of passing an iterable of <code>Variable</code>s, pass in an iterable of <code>dict</code>s. Each of them will define a separate parameter group, and should contain a params key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.</p>
<blockquote>
<h5 id="note-1">NOTE</h5>
<p>You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn&rsquo;t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.</p>
</blockquote>
</blockquote>
<p><strong>Per-parameter option</strong><br>
我个人翻译为逐参数选项。<code>Optimizer</code>在构造的时候同样支持对每个参数进行指定。要实现这种功能，我们不再传入一个含有<code>Variable</code>类型参数的<code>iterable</code>对象，而是传入一个<code>dict</code>字典类型的<code>iterable</code>对象。每个字典都定义了一个参数组，该参数组的key值是&quot;params&quot;，而对应的值为一个包含参数的列表。同样的可以利用字典的键值对<code>Optimizer</code>的其他参数进行指定，但是key必须与<code>Optimizer</code>构造器传参时的关键字一致。这些指定的<code>Optimizer</code>的参数会被单独应用于该字典中的<code>params</code>这些参数。<br>
<strong>注意</strong>：你仍然可以在构造器中以关键字方式传入参数，这些参数将被当做默认值使用，如果一组参数没有override这个参数，那么就将自动使用默认的参数值。<br>
以下是实例：</p>
<blockquote>
<p>For example, this is very useful when one wants to specify per-layer learning rates:</p>
<pre><code>optim.SGD([
               {'params': model.base.parameters()},
               {'params': model.classifier.parameters(), 'lr': 1e-3}
           ], lr=1e-2, momentum=0.9)
</code></pre></blockquote>
<blockquote>
<p>This means that model.base&rsquo;s parameters will use the default learning rate of 1e-2, model.classifier&rsquo;s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters.</p>
</blockquote>
<p>对于上面这个例子，首先我们可以看到，传入了一个dict的列表。列表中有两个dict，第一个的<code>params</code>key 对应的是<code>model.base.parameters()</code>，而没有对<code>Optimizer</code>的其他参数进行具体指定。第二个dict的<code>params</code>key对应的是<code>model.classifier.parameters()</code>，此外还有一个键值对，说明了<code>lr</code>的值为1e-3。而在列表之外同时又传入了<code>lr=1e-2</code>,<code>momentum=0.9</code>，这两个值将作为默认值来使用。所以整个<code>Optimizer</code>中，<code>base's parameters</code>将使用默认的学习率<code>1e-2</code>,默认的动量超参数<code>0.9</code>;而<code>classifier.parameters()</code>将使用其dict中提供的学习率<code>1e-3</code>,<code>momentum</code>仍然使用默认值。</p>
<blockquote>
<h4 id="taking-an-optimization-step">Taking an optimization step</h4>
<p>All optimizers implement a <code>step()</code> method, that updates the parameters. It can be used in two ways:</p>
</blockquote>
<blockquote>
<h4 id="optimizerstep"><strong><code>optimizer.step()</code></strong></h4>
</blockquote>
<blockquote>
<p>This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. backward().</p>
</blockquote>
<p><strong>采取优化步骤</strong><br>
所有的Optimizer都实现了<code>step()</code>方法，该方法可以用于更新参数。可以通过两种方式使用<code>.step()</code>进行优化：</p>
<p>第一种方式：<code>optimizer.step()</code><br>
该方法是一个简化后的版本，被大多数optimizer所支持。该函数一般在所有梯度值被更新（或者被计算）后进行调用,如在<code>.backward()</code>后进行调用。</p>
<p>以下是例子：</p>
<blockquote>
<p>Example:</p>
<pre><code>for input, target in dataset:
   optimizer.zero_grad()
   output = model(input)
   loss = loss_fn(output, target)
   loss.backward()
   optimizer.step()
</code></pre></blockquote>
<blockquote>
<h4 id="optimizerstepclosure"><code>optimizer.step(closure)</code></h4>
</blockquote>
<blockquote>
<p>Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.</p>
</blockquote>
<p>第二种方式：<code>optimizer.step(closure)</code><br>
有一些优化算法例如<code>Conjugate Gradient</code>，<code>LBFGS</code>等需要多次重新计算函数，所以需要传入一个闭包<code>closure</code>，闭包中应该实现的操作有：清零梯度，计算损失并返回。<br>
以下是例子：</p>
<blockquote>
<p>Example:</p>
<pre><code>for input, target in dataset:
   def closure():
       optimizer.zero_grad()
       output = model(input)
       loss = loss_fn(output, target)
       loss.backward()
       return loss
   optimizer.step(closure)
</code></pre></blockquote>
<p>具体的各个优化算法的数学原理在此不表，详参手写的笔记本。</p>
<h1 id="2-如何调整学习率">2 如何调整学习率</h1>
<p><code>torch.optim.lr_scheduler</code>模块，提供了一些根据训练次数来调整学习率(learning rate)的方法，一般情况下我们会设置随着epoch的增大而逐渐减小学习率，从而达到更好的训练效果。
而<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code>提供了一些基于训练中某些测量值使得学习率动态下降的办法。</p>
<p>学习率的调整应该放在optimizer更新之后，参考模板：</p>
<pre><code>define scheduler
for epoch in range(1000):
    train(...)
    validate(...)
    scheduler.step()
</code></pre><p><strong>注意</strong>： 在PyTorch 1.1.0之前的版本，学习率的调整应该被放在optimizer更新之前，如果我们1.1.0之后仍然将学习率的调整(即<code>scheduler.step()</code>)放在optimizer&rsquo;s update(即<code>optimizer.step</code>)之前，那么learning rate schedule的第一个值将被跳过。所以如果某个代码是在1.1.0之前的版本开发，移植到高版本进行运行，发现效果变差，可以检查是否将<code>scheduler.step()</code>放在了<code>optimizer.step()</code>之前。</p>
<blockquote>
<p>注：以上部分参考官方文档批示。</p>
</blockquote>
<h2 id="21-torchoptimlr_schedulersteplr">2.1 <code>torch.optim.lr_scheduler.StepLR</code></h2>
<p>首先贴上官方文档：</p>
<blockquote>
<h3 id="torchoptimlr_schedulersteplroptimizer-step_size-gamma01-last_epoch-1-verbosefalse"><code>torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)</code></h3>
</blockquote>
<blockquote>
<p>Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</p>
</blockquote>
<p><code>StepLR</code>可以根据超参数<code>gamma</code>每隔固定的<code>step_size</code>就衰减<code>learning_rate</code>一次。需要说明的是，这种对<code>learning_rate</code>的更新可以与外界的其他变化同时进行。当<code>last_epoch = -1</code>时，将lr置为初始值。</p>
<blockquote>
<p>Parameters</p>
<ul>
<li>optimizer (Optimizer) – Wrapped optimizer.</li>
<li>step_size (int) – Period of learning rate decay.</li>
<li>gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.</li>
<li>last_epoch (int) – The index of last epoch. Default: -1.</li>
<li>verbose (bool) – If True, prints a message to stdout for each update. Default: False.</li>
</ul>
</blockquote>
<p>参数说明</p>
<ul>
<li>optimizer(Optimizer) &mdash;&ndash;用于指定scheduler的应用对象。</li>
<li>step_size(int)&mdash;&ndash;用于指定步长，即几次迭代之后进行一次decay</li>
<li>gamma(float)&mdash;&ndash;学习率衰减的乘法因子，默认值为0.1</li>
<li>last_epoch(int)&mdash;&ndash;更新的边界index，当等于这个值的时候，重置lr，默认为-1</li>
<li>verbose(bool)&mdash;&ndash;如果为True，每次decay会向stdout输出一条信息。默认为false.</li>
</ul>
<p>以下是实例：</p>
<blockquote>
<p>Example</p>
<pre><code># Assuming optimizer uses lr = 0.05 for all groups
# lr = 0.05     if epoch &lt; 30
# lr = 0.005    if 30 &lt;= epoch &lt; 60
# lr = 0.0005   if 60 &lt;= epoch &lt; 90
# ...
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
</code></pre></blockquote>
<p>可见:每经过一个<code>step_size</code>，</p>
<pre><code class="language-math" data-lang="math">lr = lr*gamma 
</code></pre>
</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/pytorch/">PyTorch</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.querySelector(`.article-content`));"></script>
<script>
var katex_config = {
    delimiters: 
    [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false}
    ]
};
</script>
<script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" onload="renderMathInElement(document.body,katex_config)"></script>

    
</article>

    <aside class="related-contents--wrapper">
    
    
</aside>


    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "codefmeister" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2022 Codefmeister
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="1.1.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true" style="display:none">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
            </main>
        </div>
        <script src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"
    integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin="anonymous"></script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<link rel="stylesheet" href="/css/highlight/light.min.css" media="(prefers-color-scheme: light)">
<link rel="stylesheet" href="/css/highlight/dark.min.css" media="(prefers-color-scheme: dark)">

    </body>
</html>
