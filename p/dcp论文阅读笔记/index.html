<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='DCPè®ºæ–‡é˜…è¯»ç¬”è®° è®ºæ–‡  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution åŸºäºICPè¿­ä»£æœ€è¿‘ç‚¹ç®—æ³•ï¼Œæå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„DCPç®—æ³•ã€‚è§£å†³äº†ICPæƒ³è¦é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ—¶é‡åˆ°çš„ä¸€ç³»åˆ—é—®é¢˜ã€‚
æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ICPç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data æ¦‚æ‹¬èµ·æ¥å°±æ˜¯ï¼š å¯»æ‰¾æœ€è¿‘ç‚¹å¯¹å…³ç³»ï¼Œä½¿ç”¨SVDæ±‚è§£åˆšä½“å˜æ¢ã€‚å¦‚æ­¤å¾ªç¯å¾€å¤ã€‚
ç»“åˆè®ºæ–‡ï¼Œä¸ªäººç†è§£å°†ICPç®—æ³•æ‰©å±•åˆ°æ·±åº¦å­¦ä¹ å­˜åœ¨ç€ä»¥ä¸‹çš„éš¾ç‚¹ï¼ˆå¯èƒ½å­˜åœ¨å„ç§é—®é¢˜ï¼Œç¬”è€…æ·±åº¦å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†å¾ˆè–„å¼±ï¼‰ï¼š
 é¦–å…ˆï¼Œç‚¹å¯¹å…³ç³»å¦‚æœæ˜¯ç¡®å®šçš„è¯ï¼Œæ²¿ç€ç½‘ç»œåå‘ä¼ æ’­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚ SVDåˆ†è§£æ±‚è§£åˆšä½“å˜æ¢ï¼Œå¦‚ä½•æ±‚æ¢¯åº¦ï¼Ÿ(Confirmed by paper)  è€Œæ–‡ç« å…‹æœäº†è¿™äº›é—®é¢˜ï¼Œä¸»è¦æœ‰å¦‚ä¸‹è´¡çŒ®ï¼š
 æå‡ºäº†èƒ½å¤Ÿè§£å†³ä¼ ç»ŸICPç®—æ³•è¯•å›¾æ¨å¹¿æ—¶å­˜åœ¨çš„å›°éš¾çš„ä¸€ç³»åˆ—å­ç½‘ç»œæ¶æ„ã€‚ æå‡ºäº†èƒ½è¿›è¡Œpair-wiseé…å‡†çš„ç½‘ç»œæ¶æ„ è¯„ä¼°äº†åœ¨é‡‡ç”¨ä¸åŒè®¾ç½®çš„æƒ…å†µä¸‹çš„ç½‘ç»œè¡¨ç° åˆ†æäº†æ˜¯global featureæœ‰ç”¨è¿˜æ˜¯local featureå¯¹é…å‡†æ›´åŠ æœ‰ç”¨  ç½‘ç»œæ¶æ„ æ¨¡å‹åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
(1) ä¸€ä¸ªå°†è¾“å…¥ç‚¹äº‘æ˜ å°„åˆ°é«˜ç»´ç©ºé—´embeddingçš„æ¨¡å—ï¼Œå…·æœ‰æ‰°åŠ¨ä¸å˜æ€§ï¼ˆæŒ‡DGCNNå½“ç‚¹äº‘è¾“å…¥æ—¶ç‚¹çš„å‰åé¡ºåºå‘ç”Ÿå˜åŒ–ï¼Œè¾“å‡ºä¸ä¼šæœ‰ä»»ä½•æ”¹å˜ï¼‰ æˆ–è€… åˆšä½“å˜æ¢ä¸å˜æ€§ï¼ˆæŒ‡PointNetå¯¹äºæ—‹è½¬å¹³ç§»å…·æœ‰ä¸å˜çš„ç‰¹æ€§ï¼‰ã€‚è¯¥æ¨¡å—çš„ä½œç”¨æ˜¯å¯»æ‰¾ä¸¤ä¸ªè¾“å…¥ç‚¹äº‘ä¹‹é—´çš„ç‚¹çš„å¯¹åº”å…³ç³». å¯é€‰çš„æ¨¡å—æœ‰PointNetï¼ˆFocusäºå…¨å±€ç‰¹å¾ï¼‰ï¼Œ DGCNNï¼ˆç»“åˆå±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼‰ã€‚'><title>DCPè®ºæ–‡é˜…è¯»ç¬”è®°</title>

<link rel='canonical' href='https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='DCPè®ºæ–‡é˜…è¯»ç¬”è®°'>
<meta property='og:description' content='DCPè®ºæ–‡é˜…è¯»ç¬”è®° è®ºæ–‡  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution åŸºäºICPè¿­ä»£æœ€è¿‘ç‚¹ç®—æ³•ï¼Œæå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„DCPç®—æ³•ã€‚è§£å†³äº†ICPæƒ³è¦é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ—¶é‡åˆ°çš„ä¸€ç³»åˆ—é—®é¢˜ã€‚
æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ICPç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data æ¦‚æ‹¬èµ·æ¥å°±æ˜¯ï¼š å¯»æ‰¾æœ€è¿‘ç‚¹å¯¹å…³ç³»ï¼Œä½¿ç”¨SVDæ±‚è§£åˆšä½“å˜æ¢ã€‚å¦‚æ­¤å¾ªç¯å¾€å¤ã€‚
ç»“åˆè®ºæ–‡ï¼Œä¸ªäººç†è§£å°†ICPç®—æ³•æ‰©å±•åˆ°æ·±åº¦å­¦ä¹ å­˜åœ¨ç€ä»¥ä¸‹çš„éš¾ç‚¹ï¼ˆå¯èƒ½å­˜åœ¨å„ç§é—®é¢˜ï¼Œç¬”è€…æ·±åº¦å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†å¾ˆè–„å¼±ï¼‰ï¼š
 é¦–å…ˆï¼Œç‚¹å¯¹å…³ç³»å¦‚æœæ˜¯ç¡®å®šçš„è¯ï¼Œæ²¿ç€ç½‘ç»œåå‘ä¼ æ’­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚ SVDåˆ†è§£æ±‚è§£åˆšä½“å˜æ¢ï¼Œå¦‚ä½•æ±‚æ¢¯åº¦ï¼Ÿ(Confirmed by paper)  è€Œæ–‡ç« å…‹æœäº†è¿™äº›é—®é¢˜ï¼Œä¸»è¦æœ‰å¦‚ä¸‹è´¡çŒ®ï¼š
 æå‡ºäº†èƒ½å¤Ÿè§£å†³ä¼ ç»ŸICPç®—æ³•è¯•å›¾æ¨å¹¿æ—¶å­˜åœ¨çš„å›°éš¾çš„ä¸€ç³»åˆ—å­ç½‘ç»œæ¶æ„ã€‚ æå‡ºäº†èƒ½è¿›è¡Œpair-wiseé…å‡†çš„ç½‘ç»œæ¶æ„ è¯„ä¼°äº†åœ¨é‡‡ç”¨ä¸åŒè®¾ç½®çš„æƒ…å†µä¸‹çš„ç½‘ç»œè¡¨ç° åˆ†æäº†æ˜¯global featureæœ‰ç”¨è¿˜æ˜¯local featureå¯¹é…å‡†æ›´åŠ æœ‰ç”¨  ç½‘ç»œæ¶æ„ æ¨¡å‹åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
(1) ä¸€ä¸ªå°†è¾“å…¥ç‚¹äº‘æ˜ å°„åˆ°é«˜ç»´ç©ºé—´embeddingçš„æ¨¡å—ï¼Œå…·æœ‰æ‰°åŠ¨ä¸å˜æ€§ï¼ˆæŒ‡DGCNNå½“ç‚¹äº‘è¾“å…¥æ—¶ç‚¹çš„å‰åé¡ºåºå‘ç”Ÿå˜åŒ–ï¼Œè¾“å‡ºä¸ä¼šæœ‰ä»»ä½•æ”¹å˜ï¼‰ æˆ–è€… åˆšä½“å˜æ¢ä¸å˜æ€§ï¼ˆæŒ‡PointNetå¯¹äºæ—‹è½¬å¹³ç§»å…·æœ‰ä¸å˜çš„ç‰¹æ€§ï¼‰ã€‚è¯¥æ¨¡å—çš„ä½œç”¨æ˜¯å¯»æ‰¾ä¸¤ä¸ªè¾“å…¥ç‚¹äº‘ä¹‹é—´çš„ç‚¹çš„å¯¹åº”å…³ç³». å¯é€‰çš„æ¨¡å—æœ‰PointNetï¼ˆFocusäºå…¨å±€ç‰¹å¾ï¼‰ï¼Œ DGCNNï¼ˆç»“åˆå±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼‰ã€‚'>
<meta property='og:url' content='https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/'>
<meta property='og:site_name' content='Codefmeister'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='ç‚¹äº‘é…å‡†' /><meta property='article:tag' content='æ·±åº¦å­¦ä¹ ' /><meta property='article:published_time' content='2020-12-22T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2020-12-22T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="DCPè®ºæ–‡é˜…è¯»ç¬”è®°">
<meta name="twitter:description" content="DCPè®ºæ–‡é˜…è¯»ç¬”è®° è®ºæ–‡  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution åŸºäºICPè¿­ä»£æœ€è¿‘ç‚¹ç®—æ³•ï¼Œæå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„DCPç®—æ³•ã€‚è§£å†³äº†ICPæƒ³è¦é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ—¶é‡åˆ°çš„ä¸€ç³»åˆ—é—®é¢˜ã€‚
æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ICPç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data æ¦‚æ‹¬èµ·æ¥å°±æ˜¯ï¼š å¯»æ‰¾æœ€è¿‘ç‚¹å¯¹å…³ç³»ï¼Œä½¿ç”¨SVDæ±‚è§£åˆšä½“å˜æ¢ã€‚å¦‚æ­¤å¾ªç¯å¾€å¤ã€‚
ç»“åˆè®ºæ–‡ï¼Œä¸ªäººç†è§£å°†ICPç®—æ³•æ‰©å±•åˆ°æ·±åº¦å­¦ä¹ å­˜åœ¨ç€ä»¥ä¸‹çš„éš¾ç‚¹ï¼ˆå¯èƒ½å­˜åœ¨å„ç§é—®é¢˜ï¼Œç¬”è€…æ·±åº¦å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†å¾ˆè–„å¼±ï¼‰ï¼š
 é¦–å…ˆï¼Œç‚¹å¯¹å…³ç³»å¦‚æœæ˜¯ç¡®å®šçš„è¯ï¼Œæ²¿ç€ç½‘ç»œåå‘ä¼ æ’­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚ SVDåˆ†è§£æ±‚è§£åˆšä½“å˜æ¢ï¼Œå¦‚ä½•æ±‚æ¢¯åº¦ï¼Ÿ(Confirmed by paper)  è€Œæ–‡ç« å…‹æœäº†è¿™äº›é—®é¢˜ï¼Œä¸»è¦æœ‰å¦‚ä¸‹è´¡çŒ®ï¼š
 æå‡ºäº†èƒ½å¤Ÿè§£å†³ä¼ ç»ŸICPç®—æ³•è¯•å›¾æ¨å¹¿æ—¶å­˜åœ¨çš„å›°éš¾çš„ä¸€ç³»åˆ—å­ç½‘ç»œæ¶æ„ã€‚ æå‡ºäº†èƒ½è¿›è¡Œpair-wiseé…å‡†çš„ç½‘ç»œæ¶æ„ è¯„ä¼°äº†åœ¨é‡‡ç”¨ä¸åŒè®¾ç½®çš„æƒ…å†µä¸‹çš„ç½‘ç»œè¡¨ç° åˆ†æäº†æ˜¯global featureæœ‰ç”¨è¿˜æ˜¯local featureå¯¹é…å‡†æ›´åŠ æœ‰ç”¨  ç½‘ç»œæ¶æ„ æ¨¡å‹åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
(1) ä¸€ä¸ªå°†è¾“å…¥ç‚¹äº‘æ˜ å°„åˆ°é«˜ç»´ç©ºé—´embeddingçš„æ¨¡å—ï¼Œå…·æœ‰æ‰°åŠ¨ä¸å˜æ€§ï¼ˆæŒ‡DGCNNå½“ç‚¹äº‘è¾“å…¥æ—¶ç‚¹çš„å‰åé¡ºåºå‘ç”Ÿå˜åŒ–ï¼Œè¾“å‡ºä¸ä¼šæœ‰ä»»ä½•æ”¹å˜ï¼‰ æˆ–è€… åˆšä½“å˜æ¢ä¸å˜æ€§ï¼ˆæŒ‡PointNetå¯¹äºæ—‹è½¬å¹³ç§»å…·æœ‰ä¸å˜çš„ç‰¹æ€§ï¼‰ã€‚è¯¥æ¨¡å—çš„ä½œç”¨æ˜¯å¯»æ‰¾ä¸¤ä¸ªè¾“å…¥ç‚¹äº‘ä¹‹é—´çš„ç‚¹çš„å¯¹åº”å…³ç³». å¯é€‰çš„æ¨¡å—æœ‰PointNetï¼ˆFocusäºå…¨å±€ç‰¹å¾ï¼‰ï¼Œ DGCNNï¼ˆç»“åˆå±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼‰ã€‚">
    </head>
    <body class="">
        <div class="container flex on-phone--column align-items--flex-start extended article-page with-toolbar">
            <aside class="sidebar left-sidebar sticky">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header class="site-info">
        
            <figure class="site-avatar">
                
                    
                    
                    
                        
                        <img src="/img/Icon_hua25ec96b536dfdbbcc947accbc1cb594_86128_300x0_resize_q75_box.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                

                
                    <span class="emoji">ğŸ¥</span>
                
            </figure>
        
        <h1 class="site-name"><a href="https://codefmeister.github.io">Codefmeister</a></h1>
        <h2 class="site-description">Major in Computer science</h2>
    </header>

    <ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
    </ol>
</aside>

            <main class="main full-width">
    <div id="article-toolbar">
        <a href="https://codefmeister.github.io" class="back-home">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



            <span>Back</span>
        </a>
    </div>

    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <h2 class="article-title">
        <a href="/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">DCPè®ºæ–‡é˜…è¯»ç¬”è®°</a>
    </h2>

    <footer class="article-time">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--published">Dec 22, 2020</time>
    </footer></div>
</header>

    <section class="article-content">
    <h1 id="dcpè®ºæ–‡é˜…è¯»ç¬”è®°">DCPè®ºæ–‡é˜…è¯»ç¬”è®°</h1>
<h2 id="è®ºæ–‡">è®ºæ–‡</h2>
<blockquote>
<p>Deep Closest Point: Learning Representations for Point Cloud Registration<br>
Author: Wang, Yue; Solomon, Justin</p>
</blockquote>
<h2 id="main-attribution">Main Attribution</h2>
<p>åŸºäºICPè¿­ä»£æœ€è¿‘ç‚¹ç®—æ³•ï¼Œæå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„DCPç®—æ³•ã€‚è§£å†³äº†ICPæƒ³è¦é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ—¶é‡åˆ°çš„ä¸€ç³»åˆ—é—®é¢˜ã€‚</p>
<p>æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ICPç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">each</span> <span class="n">iteration</span><span class="p">:</span>
    <span class="n">find</span> <span class="n">corresponding</span> <span class="n">relations</span> <span class="n">of</span> <span class="n">points</span> <span class="n">between</span> <span class="n">two</span> <span class="n">scan</span><span class="p">(</span><span class="n">using</span> <span class="n">KNN</span><span class="p">)</span>
    <span class="n">using</span> <span class="n">SVD</span> <span class="n">to</span> <span class="n">solve</span> <span class="n">Rotation</span> <span class="n">Matrix</span> <span class="ow">and</span> <span class="n">Translation</span> <span class="n">vector</span>
    <span class="n">update</span> <span class="n">cloud</span> <span class="n">Data</span>
</code></pre></div><p>æ¦‚æ‹¬èµ·æ¥å°±æ˜¯ï¼š <!-- raw HTML omitted --><strong>å¯»æ‰¾æœ€è¿‘ç‚¹å¯¹å…³ç³»ï¼Œä½¿ç”¨SVDæ±‚è§£åˆšä½“å˜æ¢ã€‚</strong><!-- raw HTML omitted -->å¦‚æ­¤å¾ªç¯å¾€å¤ã€‚</p>
<p>ç»“åˆè®ºæ–‡ï¼Œä¸ªäººç†è§£å°†ICPç®—æ³•æ‰©å±•åˆ°æ·±åº¦å­¦ä¹ å­˜åœ¨ç€ä»¥ä¸‹çš„éš¾ç‚¹ï¼ˆå¯èƒ½å­˜åœ¨å„ç§é—®é¢˜ï¼Œç¬”è€…æ·±åº¦å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†å¾ˆè–„å¼±ï¼‰ï¼š</p>
<ul>
<li>é¦–å…ˆï¼Œç‚¹å¯¹å…³ç³»å¦‚æœæ˜¯ç¡®å®šçš„è¯ï¼Œæ²¿ç€ç½‘ç»œåå‘ä¼ æ’­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚</li>
<li>SVDåˆ†è§£æ±‚è§£åˆšä½“å˜æ¢ï¼Œå¦‚ä½•æ±‚æ¢¯åº¦ï¼Ÿ(Confirmed by paper)</li>
</ul>
<p>è€Œæ–‡ç« å…‹æœäº†è¿™äº›é—®é¢˜ï¼Œä¸»è¦æœ‰å¦‚ä¸‹è´¡çŒ®ï¼š</p>
<ul>
<li>æå‡ºäº†èƒ½å¤Ÿè§£å†³ä¼ ç»ŸICPç®—æ³•è¯•å›¾æ¨å¹¿æ—¶å­˜åœ¨çš„å›°éš¾çš„ä¸€ç³»åˆ—å­ç½‘ç»œæ¶æ„ã€‚</li>
<li>æå‡ºäº†èƒ½è¿›è¡Œ<code>pair-wise</code>é…å‡†çš„ç½‘ç»œæ¶æ„</li>
<li>è¯„ä¼°äº†åœ¨é‡‡ç”¨ä¸åŒè®¾ç½®çš„æƒ…å†µä¸‹çš„ç½‘ç»œè¡¨ç°</li>
<li>åˆ†æäº†æ˜¯<code>global feature</code>æœ‰ç”¨è¿˜æ˜¯<code>local feature</code>å¯¹é…å‡†æ›´åŠ æœ‰ç”¨</li>
</ul>
<h2 id="ç½‘ç»œæ¶æ„">ç½‘ç»œæ¶æ„</h2>
<p><img src="https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig1.png" alt="image"  />
æ¨¡å‹åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š<br>
(1) ä¸€ä¸ªå°†è¾“å…¥ç‚¹äº‘æ˜ å°„åˆ°é«˜ç»´ç©ºé—´<code>embedding</code>çš„æ¨¡å—ï¼Œå…·æœ‰æ‰°åŠ¨ä¸å˜æ€§ï¼ˆæŒ‡DGCNNå½“ç‚¹äº‘è¾“å…¥æ—¶ç‚¹çš„å‰åé¡ºåºå‘ç”Ÿå˜åŒ–ï¼Œè¾“å‡ºä¸ä¼šæœ‰ä»»ä½•æ”¹å˜ï¼‰ æˆ–è€… åˆšä½“å˜æ¢ä¸å˜æ€§ï¼ˆæŒ‡PointNetå¯¹äºæ—‹è½¬å¹³ç§»å…·æœ‰ä¸å˜çš„ç‰¹æ€§ï¼‰ã€‚è¯¥æ¨¡å—çš„ä½œç”¨æ˜¯<!-- raw HTML omitted --><strong>å¯»æ‰¾ä¸¤ä¸ªè¾“å…¥ç‚¹äº‘ä¹‹é—´çš„ç‚¹çš„å¯¹åº”å…³ç³»</strong><!-- raw HTML omitted -->. å¯é€‰çš„æ¨¡å—æœ‰<!-- raw HTML omitted --><strong>PointNetï¼ˆFocusäºå…¨å±€ç‰¹å¾ï¼‰ï¼Œ DGCNNï¼ˆç»“åˆå±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼‰</strong><!-- raw HTML omitted -->ã€‚</p>
<p>(2) ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›<code>attention</code>çš„Pointerç½‘ç»œæ¨¡å—ï¼Œç”¨äºé¢„æµ‹ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„soft matchingå…³ç³»(<!-- raw HTML omitted --><strong>ç±»ä¼¼äºä¸€ç§åŸºäºæ¦‚ç‡çš„soft matchï¼Œä¹‹æ‰€ä»¥softæ˜¯ç”±äºå®ƒå¹¶æ²¡æœ‰æ˜¾å¼è§„å®šç‚¹$x_i$å¿…é¡»ä¸å“ªä¸ªç‚¹$x_j$æœ‰å¯¹åº”å…³ç³»ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªsoftmaxå¾—åˆ°çš„å„ç‚¹å’ŒæŸç‚¹$x_i$å­˜åœ¨å¯¹åº”å…³ç³»çš„æ¦‚ç‡ä¹˜ä»¥å„ç‚¹æ•°æ®ï¼Œå¾—åˆ°ä¸€ä¸ªç±»ä¼¼äºæ¦‚ç‡çš„å¯¹åº”ç‚¹åæ ‡</strong><!-- raw HTML omitted -->ã€‚ è¯¥æ¨¡å—é‡‡ç”¨çš„æ˜¯<!-- raw HTML omitted --><strong>Transformer</strong><!-- raw HTML omitted --></p>
<p>(3) ä¸€ä¸ª<strong>å¯å¾®</strong>çš„<code>SVD</code>åˆ†è§£å±‚ï¼Œç”¨äºè¾“å‡ºåˆšä½“å˜æ¢çŸ©é˜µã€‚</p>
<h2 id="é—®é¢˜é˜è¿°">é—®é¢˜é˜è¿°</h2>
<p>ç†Ÿæ‚‰ç‚¹äº‘é…å‡†çš„åŒå­¦åº”è¯¥çŸ¥é“ï¼Œé—®é¢˜ååˆ†æ¸…æ™°ã€‚è¿™é‡Œç›´æ¥ç²˜ä¸€ä¸‹åŸæ–‡ã€‚<br>
<!-- raw HTML omitted -->
<!-- raw HTML omitted --></p>
<p>å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä½œè€…åˆ†æäº†ä¸€ä¸‹ICPçš„ç®—æ³•æ­¥éª¤ã€‚å’Œæˆ‘ä»¬ä¸Šé¢æè¿°çš„ä¸€æ ·ã€‚<!-- raw HTML omitted --><strong>å°±æ˜¯ç”¨ä¸Šæ¬¡æ›´æ–°åçš„ä¿¡æ¯å¯»æ‰¾æœ€è¿‘å…³ç³»ï¼Œç„¶åç”¨å¯»æ‰¾åˆ°çš„å¯¹åº”å…³ç³»SVDæ±‚è§£å¾—åˆ°$R,t$.</strong><!-- raw HTML omitted --> æ‰€ä»¥å¦‚æœåˆå§‹å€¼ä¸€å¼€å§‹ç”Ÿæˆçš„æ˜¯å¾ˆå·®çš„<code>corresponding relation</code>ï¼Œé‚£ä¹ˆä¸€ä¸‹å°±ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</p>
<p>è€Œä½œè€…çš„æ€è·¯å°±æ˜¯ï¼š<!-- raw HTML omitted --><strong>ä½¿ç”¨å­¦ä¹ çš„ç½‘ç»œæ¥å¾—åˆ°ç‰¹å¾ï¼Œé€šè¿‡ç‰¹å¾è·å¾—ä¸€ä¸ªæ›´å¥½çš„å¯¹åº”å…³ç³»$m(\cdot)$ï¼Œç”¨è¿™ä¸ª$m(\cdot)$å»è®¡ç®—åˆšä½“å˜æ¢ä¿¡æ¯ã€‚</strong><!-- raw HTML omitted --></p>
<h2 id="ä»£ç åˆ†æä¸å¯¹åº”æ¨¡å—è¯¦è§£">ä»£ç åˆ†æä¸å¯¹åº”æ¨¡å—è¯¦è§£</h2>
<p>æˆ‘ä»¬é‡‡ç”¨ä¸€ç§<code>Top-Down</code>çš„è§†è§’æ¥åˆ†ææ•´ä¸ªä»£ç ã€‚å…ˆä»æ•´ä½“å…¥æ‰‹ï¼Œç„¶åé€æ¸æ‹†è§£æ¨¡å—è¿›è¡Œåˆ†æã€‚</p>
<h3 id="æ•´ä½“æ¨¡å—">æ•´ä½“æ¨¡å—</h3>
<p>DCPç½‘ç»œç»“æ„åˆ†ä¸ºä¸‰ä¸ªPartï¼Œä»ä»£ç ä¸­å°±å¯ä»¥å¾ˆæ¸…æ™°çš„çœ‹å‡ºæ¥ï¼šç¬¬ä¸€ä¸ªModuleæ¨¡å—<code>emd_nn</code>ç”¨äºæŠ½è±¡ç‰¹å¾ï¼Œç¬¬äºŒä¸ªModuleæ¨¡å—<code>pointer</code>ç”¨äºmatch,ç¬¬ä¸‰ä¸ªModuleæ¨¡å—<code>head</code>ç”¨äºæ±‚è§£åˆšä½“å˜æ¢çŸ©é˜µï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DCP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>       <span class="c1"># args æ˜¯ä¸€ä¸ªå­˜æ”¾å„ç§å‚æ•°çš„namespace</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DCP</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">embdims</span>    <span class="c1"># æ¬²æŠ½è±¡åˆ°çš„ç‰¹å¾ç»´åº¦ï¼Œdefaultä¸º 512</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle</span>         <span class="c1"># baçš„åˆšä½“å˜æ¢å…³ç³»æ˜¯å¦é‡æ–°è¿›å…¥ç½‘ç»œè®¡ç®—</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">==</span> <span class="s1">&#39;pointnet&#39;</span><span class="p">:</span>   <span class="c1"># emb_nnå°±æ˜¯ä¸Šæ–‡æ‰€è¯´çš„ç¬¬ä¸€ä¸ªæ¨¡å—,è‹¥é€‰æ‹©PointNet</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">=</span> <span class="n">PointNet</span><span class="p">(</span><span class="n">emb_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">==</span> <span class="s1">&#39;dgcnn&#39;</span><span class="p">:</span>    <span class="c1"># è‹¥é€‰æ‹©DGCNN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">=</span> <span class="n">DGCNN</span><span class="p">(</span><span class="n">emb_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Not implemented&#39;</span><span class="p">)</span>      <span class="c1"># å…¶ä»–ç½‘ç»œå°šæœªå®ç°</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">pointer</span> <span class="o">==</span> <span class="s1">&#39;identity&#39;</span><span class="p">:</span>              <span class="c1"># ä¸ä½¿ç”¨Transformer, hard match</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">pointer</span> <span class="o">==</span> <span class="s1">&#39;transformer&#39;</span><span class="p">:</span>         <span class="c1"># soft matching by tranformer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Not implemented&#39;</span><span class="p">)</span>  
        
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="s1">&#39;mlp&#39;</span><span class="p">:</span>                      <span class="c1"># ç›´æ¥ç”¨MLPé¢„æµ‹è¾“å‡ºçŸ©é˜µ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLPHead</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="s1">&#39;svd&#39;</span><span class="p">:</span>                    <span class="c1"># ä½¿ç”¨å¯å¾®çš„SVDåˆ†è§£å±‚</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">SVDHead</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&#34;Not implemented&#34;</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">src_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span>    <span class="c1"># Module Part 1             (batch_size, emb_dims, num_points)</span>

        <span class="n">src_embedding_p</span><span class="p">,</span> <span class="n">tgt_embedding_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span><span class="p">(</span><span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt_embedding</span><span class="p">)</span>  <span class="c1"># Module Part 2</span>

        <span class="n">src_embedding</span> <span class="o">=</span> <span class="n">src_embedding</span> <span class="o">+</span> <span class="n">src_embedding_p</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="n">tgt_embedding</span> <span class="o">+</span> <span class="n">tgt_embedding_p</span>

        <span class="n">rotation_ab</span><span class="p">,</span> <span class="n">translation_ab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt_embedding</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> <span class="c1"># Module Part 3</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span><span class="p">:</span>
            <span class="n">rotation_ba</span><span class="p">,</span> <span class="n">translation_ba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">tgt_embedding</span><span class="p">,</span> <span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rotation_ba</span> <span class="o">=</span> <span class="n">rotation_ab</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">translation_ba</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rotation_ba</span><span class="p">,</span> <span class="n">translation_ab</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rotation_ab</span><span class="p">,</span> <span class="n">translation_ab</span><span class="p">,</span> <span class="n">rotation_ba</span><span class="p">,</span> <span class="n">translation_ba</span>
</code></pre></div><h3 id="ç”¨äºæŠ½è±¡featureçš„module1emb_nn">ç”¨äºæŠ½è±¡featureçš„Module1ï¼šemb_nn</h3>
<p>è€ƒè™‘emb_nnï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š å…¶ä¸€æ˜¯PointNetï¼Œ å…¶äºŒæ˜¯<a class="link" href="https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"  target="_blank" rel="noopener"
    >DGCNN</a>.</p>
<p>PointNetæŠ½è±¡çš„ç‰¹å¾æ˜¯<code>global feature</code>ï¼Œ è€ŒDGCNNç»“åˆäº†<code>local feature</code>å’Œ<code>global feature</code>.</p>
<p>æˆ‘ä»¬å¸Œæœ›å¾—åˆ°çš„æ˜¯å¯¹<!-- raw HTML omitted --><strong>æ¯ä¸€ä¸ªç‚¹æŠ½è±¡è€Œå¾—çš„ç‰¹å¾</strong><!-- raw HTML omitted -->ï¼ˆå³æ¯ä¸€ä¸ªç‚¹éƒ½æœ‰å…¶embedding)ï¼Œå¹¶åˆ©ç”¨ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´ç‚¹çš„<code>embedding</code>æ¥ç”Ÿæˆæ˜ å°„å…³ç³»ï¼ˆå³Matchå…³ç³»ï¼‰. æ‰€ä»¥æˆ‘ä»¬è¦å¾—åˆ°çš„æ˜¯<code>per-point feature</code>è€Œä¸æ˜¯<code>one feature per cloud</code>ã€‚</p>
<p>å‡ºäºä¸Šè¿°åŸå› ï¼Œæˆ‘ä»¬åœ¨æœ€åä¸€å±‚çš„èšåˆå‡½æ•°<code>aggregation function</code>ä¹‹å‰ç”Ÿæˆæ¯ä¸ªç‚¹çš„<code>representation</code>ã€‚
$$
F_X = {x_1^L,x_2^L, &hellip;, x_i^L,&hellip;,x_N^L}
$$
$$
F_Y = {y_1^L, y_2^L, &hellip;, y_i^L, &hellip;, y_N^L}
$$
ä¸Šæ ‡Lä»£è¡¨ç¬¬Lå±‚çš„è¾“å‡ºï¼ˆå‡å®šå…±æœ‰Lå±‚ï¼‰ã€‚</p>
<h4 id="pointnet">PointNet</h4>
<p>$x_i^l$æ˜¯ç¬¬$i$ä¸ªç‚¹åœ¨ç¬¬$l$å±‚åçš„<code>embedding</code>ï¼Œè€Œ$h_{\theta}^l$æ˜¯ç¬¬$l$å±‚çš„éçº¿æ€§æ˜ å°„å‡½æ•°ã€‚PointNetçš„<code>forward mechanism</code>å¯ä»¥ç”¨å¦‚ä¸‹å…¬å¼ç»™å‡ºï¼š
$$
x_i^l = h_{\theta}^l(x_i^{l-1})
$$
ä½œè€…<code>@WangYue</code>åœ¨githubä¸Šå…¬å¸ƒçš„ä»£ç ï¼Œä½¿ç”¨çš„<code>PointNet</code>çš„ç½‘ç»œæ¶æ„å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">PointNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dims</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PointNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">emb_dims</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">emb_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div><p>ä»ä¸Šè¿°ä»£ç ä¸­ï¼Œå¯ä»¥çœ‹å‡ºï¼Œ<!-- raw HTML omitted --><strong>ä½œè€…ä½¿ç”¨çš„PointNetå¹¶æ²¡æœ‰<code>input-transform</code>å’Œ<code>feature-transform</code>è¿™ä¸¤ä¸ªModule</strong><!-- raw HTML omitted -->ï¼Œç›¸å½“äºåªåº”ç”¨MLPä¸æ–­å¯¹è¾“å…¥ç‚¹äº‘è¿›è¡ŒæŠ½è±¡ï¼Œç›´åˆ°é«˜ç»´ç©ºé—´ã€‚
<!-- raw HTML omitted --></p>
<p><!-- raw HTML omitted --><em>å­˜ç–‘</em>ï¼šä¸ºä»€ä¹ˆä¸åŠ <code>Transform-Net</code>ï¼Ÿ å¦‚æœåŠ ä¸Šæ•ˆæœè®­ç»ƒæ•ˆæœå¦‚ä½•? æ²¡æœ‰catï¼Œcatä¹‹åæ•ˆæœå¦‚ä½•ï¼Ÿ<!-- raw HTML omitted --></p>
<h4 id="dgcnn">DGCNN</h4>
<p>DGCNNæ˜¯ä½œè€…<code>@WangYue</code>æå‡ºçš„ä¸€ç§ç½‘ç»œæ¶æ„ï¼Œå…¶ç‰¹ç‚¹æ˜¯<code>EdgeConv</code>ã€‚å¯ä»¥ç»“åˆå…¨å±€ç‰¹å¾ä¸å±€éƒ¨ç‰¹å¾ã€‚
$$
x_i^l = f({{} h_{\theta}^l(x_i^{l-1},x_j^{l-1}); \forall j \in N_i {}})
$$
$f$æ˜¯æ¯ä¸€å±‚åçš„èšåˆå‡½æ•°ã€‚$N_i$æŒ‡çš„æ˜¯å’Œç‚¹$x_i$å­˜åœ¨KNNå…³ç³»çš„ç‚¹çš„é›†åˆã€‚</p>
<p><code>get_graph_feature</code>æ˜¯è¿”å›<code>egde-feature</code>çš„å‡½æ•°ã€‚è¿™å¹¶ä¸æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œä»£ç ç®€è¦ç²˜è´´ä¸€ä¸‹ã€‚</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">knn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">inner</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">pairwise_distance</span> <span class="o">=</span> <span class="o">-</span><span class="n">xx</span> <span class="o">-</span> <span class="n">inner</span> <span class="o">-</span> <span class="n">xx</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># (batch_size, num_points, k)</span>
    <span class="k">return</span> <span class="n">idx</span>


<span class="k">def</span> <span class="nf">get_graph_feature</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="c1"># x = x.squeeze()</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">knn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, num_points, k)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

    <span class="n">idx_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_points</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">idx_base</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span>
                    <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># (batch_size, num_points, num_dims)  -&gt; (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)</span>
    <span class="n">feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_points</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">feature</span> <span class="o">=</span> <span class="n">feature</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">feature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">feature</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">feature</span>
</code></pre></div><p>è€Œç½‘ç»œä¸­ä½¿ç”¨çš„DGCNNä»£ç å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DGCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dims</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DGCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">emb_dims</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">emb_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">,</span> <span class="n">num_points</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>    
        <span class="k">return</span> <span class="n">x</span>            <span class="c1"># (batch_size, emb_dims, num_points)</span>
</code></pre></div><p>å¯ä»¥æ˜æ˜¾å‘ç°ä¸åŸDGCNNä¸åŒçš„åœ°æ–¹æ˜¯: <!-- raw HTML omitted -->ä½œè€…è¿™é‡Œæ¯æ¬¡forwardå‰ä¼ æ—¶ï¼Œå¹¶æ²¡æœ‰å†å¯¹æŠ½è±¡å‡ºæ¥çš„featureå¯»æ‰¾knnè¿›è¡Œè¿›ä¸€æ­¥æŠ½è±¡ã€‚è€Œæ˜¯å•çº¯çš„ä¸æ–­ç»è¿‡MLPã€‚<!-- raw HTML omitted --> å¯¹æ¯”ä¸€ä¸‹è¯¥éƒ¨åˆ†åŸä»£ç ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>      <span class="c1"># (batch_size, 3, num_points) --&gt; (batch_size, 3*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 3*2, num_points, k) --&gt; (batch_size, 64, num_points, k)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># (batch_size, 64, num_points, k) --&gt; (batch_size, 64, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>     <span class="c1"># (batch_size, 64, num_points) --&gt; (batch_size, 64*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 64*2, num_points, k) --&gt; (batch_size, 64, num_points, k)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># (batch_size, 64, num_points, k) --&gt; (batch_size, 64, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>     <span class="c1"># (batch_size, 64, num_points) --&gt; (batch_size, 64*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 64*2, num_points, k) --&gt; (batch_size, 128, num_points, k)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># (batch_size, 128, num_points, k) --&gt; (batch_size, 128, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>     <span class="c1"># (batch_size, 128, num_points) --&gt; (batch_size, 128*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 128*2, num_points, k) --&gt; (batch_size, 256, num_points, k)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>       <span class="c1"># (batch_size, 256, num_points, k) --&gt; (batch_size, 256, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 64+64+128+256, num_points)</span>
</code></pre></div><p>å·®åˆ«ååˆ†æ˜æ˜¾ã€‚ç›¸å½“äºç½‘ç»œç»“æ„ä¸­çº¢æ¡†çš„éƒ¨åˆ†æ¶ˆå¤±äº†ï¼š
<!-- raw HTML omitted --></p>
<p><!-- raw HTML omitted -->æ­¤å¤„åŒæ ·å­˜ç–‘<!-- raw HTML omitted -->ï¼Œä½œè€…åœ¨è®ºæ–‡é‡ŒæœªæåŠæ­¤ç»†èŠ‚ã€‚</p>
<h3 id="ç”¨äºmatchå¯»æ‰¾ç‚¹å¯¹å…³ç³»çš„module2">ç”¨äºMatchï¼ˆå¯»æ‰¾ç‚¹å¯¹å…³ç³»ï¼‰çš„Module2</h3>
<h4 id="åŸºäºattentionæœºåˆ¶çš„transformer">åŸºäºAttentionæœºåˆ¶çš„Transformer</h4>
<p>ä½¿ç”¨Attentionæœºåˆ¶çš„åˆè¡·åœ¨äºï¼šæˆ‘ä»¬æƒ³è®©é…å‡†å˜å¾—æ›´åŠ <code>task specify</code>ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸å†ç‹¬ç«‹åœ°å…³æ³¨ä¸¤ä¸ªè¾“å…¥ç‚¹äº‘$X$,$Y$çš„<code>embedding feature</code>ï¼Œè€Œæ˜¯å…³æ³¨$X$,$Y$ä¹‹é—´çš„ä¸€äº›è”åˆç‰¹æ€§ã€‚äºæ˜¯ä¹ï¼Œè‡ªç„¶è€Œç„¶çš„æƒ³åˆ°äº†<code>Attention</code>æœºåˆ¶ã€‚åŸºäº<code>attention</code>ï¼Œè®¾è®¡ä¸€ä¸ªå¯ä»¥æ•æ‰<code>self-attention</code>å’Œ<code>conditional attention</code>çš„æ¨¡å—ï¼Œç”¨äºå­¦ä¹ $X,Y$ç‚¹äº‘ä¹‹é—´çš„æŸäº›è”åˆä¿¡æ¯ã€‚</p>
<p>æˆ‘ä»¬å°†ç”±ä¸Šä¸€ä¸ªModuleå¯¹ä¸¤ä¸ªç‚¹äº‘å„è‡ªç‹¬ç«‹ç”Ÿæˆçš„embeddingç‰¹å¾$F_X,F_Y$ä½œä¸ºè¾“å…¥ï¼Œé‚£ä¹ˆå°±æœ‰ï¼š
$$
\Phi_X = F_X + \phi(F_X,F_Y)
$$
$$
\Phi_Y = F_Y + \phi(F_Y,F_X)
$$
å…¶ä¸­ï¼Œ$\phi$æ˜¯Transformerå­¦ä¹ å¾—åˆ°çš„æ˜ å°„å‡½æ•°:$\phi: R^{N \times P} \times R^{N \times P} \to R^{N \times P}$.</p>
<p>æˆ‘ä»¬å°†$\phi$å½“åšä¸€ä¸ªæ®‹å·®é¡¹ï¼Œ<!-- raw HTML omitted --><strong>åŸºäº$F_X,F_Y$çš„è¾“å…¥é¡ºåº</strong><!-- raw HTML omitted -->ï¼Œä¸º$F_X,F_Y$æä¾›ä¸€ä¸ªé™„åŠ çš„æ”¹å˜é¡¹ã€‚</p>
<blockquote>
<p>Notice we treat $\phi$ as a residual term, providing an additive change to $F_X$ and $F_Y$ depending on the order of its input.</p>
</blockquote>
<p>ä¹‹æ‰€ä»¥é‡‡å–å°†$F_X \to \Phi_X$çš„<strong>Motivation</strong>ï¼šä»¥ä¸€ç§é€‚åº”$Y$ä¸­ç‚¹çš„ç»„ç»‡ç»“æ„ï¼ˆä¸ªäººç†è§£å³è¾“å…¥é¡ºåºï¼‰çš„æ–¹å¼æ”¹å˜$X$çš„<code>Feature embedding</code>ã€‚å¯¹$F_Y \to \Phi_Y$ï¼ŒåŠ¨æœºç›¸åŒã€‚</p>
<blockquote>
<p>The idea here is that the map $F_X \to \Phi_X$ modifies the features associated to the points in X in a fashion that is knowledgeable about the structure of $Y$.</p>
</blockquote>
<p>é€‰æ‹©Transformeræä¾›çš„éå¯¹ç§°å‡½æ•°ä½œä¸º$\phi$ã€‚Transformerç”±ä¸€äº›å †å çš„<code>encoder-decoder</code>ç»„æˆï¼Œæ˜¯ä¸€ç§è§£å†³Seq2Seqé—®é¢˜çš„ç»å…¸æ¶æ„ã€‚å…³äºTransformerçš„æ›´å¤šä¿¡æ¯ï¼Œç§»æ­¥æˆ‘çš„å¦ä¸€ç¯‡åšå®¢<a class="link" href="https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/"  target="_blank" rel="noopener"
    >ã€Šå›¾è§£Transformerï¼ˆè¯‘ï¼‰ã€‹</a></p>
<p>æ­¤Moduleä¸­ï¼Œ<code>encoder</code>æ¥æ”¶$F_X$å¹¶é€šè¿‡<code>self-attention layer</code>å’Œ<code>MLP</code>æŠŠå®ƒç¼–ç åˆ°å…¶<code>embedding space</code>ï¼Œè€Œ<code>decoder</code>æœ‰ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼Œç¬¬ä¸€ä¸ªéƒ¨åˆ†æ¥æ”¶å¦ä¸€ä¸ªé›†åˆ$F_Y$, ç„¶ååƒ<code>encoder</code>ä¸€æ ·å°†ä¹‹ç¼–ç åˆ°<code>embedding space</code>ã€‚å¦ä¸€ä¸ªéƒ¨åˆ†ä½¿ç”¨<code>co-attention</code>å¯¹ä¸¤ä¸ªå·²ç»æ˜ å°„åˆ°<code>embedding space</code>çš„ç‚¹äº‘è¿›è¡Œå¤„ç†ã€‚ æ‰€ä»¥è¾“å‡º$\Phi_Y$,$\Phi_Y$æ—¢å«æœ‰$F_X$çš„ä¿¡æ¯ï¼Œåˆå«æœ‰$F_Y$çš„ä¿¡æ¯ã€‚</p>
<p>è¿™é‡Œçš„<strong>Motivation</strong>æ˜¯ï¼šå°†ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„åŒ¹é…å…³ç³»é—®é¢˜(<code>match problem</code>)ç±»æ¯”ä¸ºSq2Sqé—®é¢˜ã€‚ï¼ˆç‚¹äº‘æ˜¯åœ¨ä¸¤ä¸ªè¾“å…¥çš„ç‚¹çš„åºåˆ—ä¸­å¯»æ‰¾å¯¹åº”å…³ç³»ï¼Œè€ŒSq2Sqé—®é¢˜æ˜¯åœ¨è¾“å…¥å¥å­ä¸­å¯»æ‰¾å•è¯ä¹‹é—´çš„è”ç³»å…³ç³»ï¼‰ã€‚</p>
<p>ä¸ºäº†é¿å…ä¸å¯å¾®åˆ†çš„<code>hard assignment</code>ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¦‚ç‡è§’åº¦çš„ä¸€ç§æ–¹å¼æ¥ç”Ÿæˆ<code>soft map</code>ï¼Œå°†ä¸€ä¸ªç‚¹äº‘æ˜ å°„åˆ°å¦ä¸€ä¸ªç‚¹äº‘ã€‚æ‰€ä»¥ï¼Œæ¯ä¸€ä¸ª$x_i \in X$éƒ½è¢«èµ‹äºˆäº†ä¸€ä¸ªæ¦‚ç‡å‘é‡ï¼š
$$
m(x_i,Y) = softmax(\Phi_y \Phi_{x_i}^T)
$$
åœ¨è¿™é‡Œï¼Œ$\Phi_Y \in R^{N \times P}$ä»£è¡¨Yç»è¿‡<code>Attention Module</code>åç”Ÿæˆçš„<code>embedding</code>ã€‚è€Œ$\Phi_{x_i}$ä»£è¡¨çŸ©é˜µ$\Phi_X$çš„ç¬¬$i$è¡Œã€‚ æ‰€ä»¥å¯ä»¥å°†$m(x_i, Y)$ çœ‹åšä¸€ä¸ªå°†æ¯ä¸ª$x_i$æ˜ å°„åˆ°$Y$ä¸­å…ƒç´ çš„<code>soft pointer</code>ã€‚</p>
<p>ä¸‹é¢æˆ‘ä»¬å…³æ³¨æ–‡ä¸­Transformerçš„å®ç°ã€‚å…¶æ¶æ„ä¸ºï¼š
<!-- raw HTML omitted --></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">ff_dims</span>     <span class="c1"># Feed_forward Dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span>     <span class="c1"># Multihead Attentionçš„å¤´æ•°</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">()</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">)</span>
        <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">),</span>
                                    <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">),</span>
                                    <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(),</span>
                                    <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(),</span>
                                    <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">())</span>
        
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>      <span class="c1"># batch_size, emb_dims, num_points</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>      <span class="c1"># batch_size, num_points, emb_dims</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">src_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt_embedding</span>
                
</code></pre></div><p>ä¸Šè¿°ä»£ç æ˜¯Transformerçš„å®ç°ã€‚çœ‹èµ·æ¥æœ‰ç‚¹ç»•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å…³æ³¨å…¶forwardå‡½æ•°.</p>
<p>æµå…¥Transformerçš„dataï¼š <code>src,tgt: (batch_size, emb_dims, num_points)</code><br>
ç»è¿‡transopose.contiguous:  <code>src,tgt: (batch_size, num_points, emb_dims)</code><br>
éšåå°†<code>src, tgt</code>ä¼ å…¥<code>self.model</code>ï¼Œå¾—åˆ°äº†<code>tgt_embedding, src_embedding</code>.</p>
<p>å…³æ³¨<code>self.model</code>ï¼Œåœ¨<code>__init__</code>ä¸­å®šä¹‰äº†<code>self.model</code>:</p>
<pre><code>    self.model = EncoderDecoder(Encoder(EncoderLayer(self.emb_dims, c(attn), c(ff), self.dropout), self.N),
                                Decoder(DecoderLayer(self.emb_dims, c(attn), c(attn), c(ff), self.dropout),self.N),
                                nn.Sequential(),
                                nn.Sequential(),
                                nn.Sequential())
</code></pre><p><code>self.model</code> æ•´ä½“æ˜¯ä¸€ä¸ª<code>EncoderDecoder</code>ç±»ã€‚å…¶ä¼ å…¥çš„å‚æ•°æœ‰ä¸€ä¸ª<code>Encoder</code>ï¼Œä¸€ä¸ª<code>Decoder</code>ï¼Œä¸‰ä¸ª<code>Sequential</code>.</p>
<p>è€Œ<code>Encoder</code>ä¼ å…¥çš„å‚æ•°æœ‰ä¸¤ä¸ªï¼Œç¬¬ä¸€ä¸ªæ˜¯<code>EncoderLayer</code>ï¼Œç¬¬äºŒä¸ªæ˜¯<code>self.N</code>ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°çš„<code>EncoderLayer</code>ä¼ å…¥äº†å››ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯<code>self.emb_dims</code>, <code>c(attn)</code>, <code>c(ff)</code>, <code>self.dropout</code>.</p>
<p><code>Decoder</code>ä¼ å…¥çš„å‚æ•°ä¹Ÿæ˜¯ä¸¤ä¸ªï¼Œç¬¬ä¸€ä¸ªæ˜¯<code>DecoderLayer</code>ï¼Œç¬¬äºŒä¸ªæ˜¯<code>self.N</code>ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°çš„<code>DecoderLayer</code>ä¼ å…¥äº†äº”ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯<code>self.emb_dims</code>, <code>c(attn)</code>, <code>c(attn)</code>, <code>c(ff)</code>, <code>self.dropout</code>.</p>
<p>è¿™é‡Œçš„<code>c</code>æ˜¯<code>copy.deepcopy()</code>ï¼Œå³æ·±æ‹·è´ã€‚å®Œå…¨å¤åˆ¶ä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼Œæ‰€ä»¥è¿™äº›ç½‘ç»œä¹‹é—´å‚æ•°å¹¶ä¸å…±äº«ã€‚</p>
<p>é¦–å…ˆæ¥å…³æ³¨EncoderDecoderç±»ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">src_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="c1"># Take in and process masked src and target sequences</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span>
</code></pre></div><p>ä»ä¸Šè¿°ä»£ç å¯ä»¥çœ‹å‡ºï¼ŒEncoderDecoderç±»æ„é€ æ—¶ä¼ å…¥çš„äº”ä¸ªå‚æ•°åˆ†åˆ«ä¸ºï¼š <code>encoder, decoder, src_embed, tgt_embed, generator</code>.</p>
<p>å…¶å‰ä¼ æœºåˆ¶<code>forward</code>æ˜¯<code>self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</code>ã€‚å³å…ˆå°†<code>src,src_mask</code>è¿›è¡Œ<code>encode</code>,å°†ç¼–ç åçš„ç»“æœåŒ<code>src_mask, tgt, tgt_mask</code>ä¸€åŒ<code>decode</code>ã€‚</p>
<p>è€Œ<code>encode</code>å‡½æ•°ï¼Œæ˜¯è¿™æ ·å®šä¹‰çš„: <code>self.encoder(self.src_embed(src), src_mask)</code>, å³å…ˆå°†<code>src</code>é€šè¿‡<code>src_embed</code>ç½‘ç»œï¼Œç„¶åæ ¹æ®å…¶<code>mask</code>å†é€šè¿‡<code>encoder</code>ç½‘ç»œã€‚</p>
<p>è€Œ<code>decode</code>å‡½æ•°ï¼Œæ˜¯ï¼š<code>self.generator(self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask))</code>, ä¹Ÿå°±æ˜¯è¯´, <code>tgt</code>å…ˆç»è¿‡<code>tgt_embed</code>ç½‘ç»œï¼Œç„¶åéš<code>memory, src_mask, tgt_mask</code>ä¸€åŒä¼ å…¥<code>decoder</code>ç½‘ç»œï¼Œ<code>decoder</code>ç½‘ç»œçš„è¾“å‡ºå†æµå…¥<code>generator</code>ç½‘ç»œã€‚</p>
<p>æ‰€ä»¥æˆ‘è‡ªå·±æ¢³ç†äº†ä¸€ä¸‹æ•´ä¸ªEncoderDecoderå¤§æ¦‚ç»“æ„å¦‚ä¸‹ï¼š</p>
<!-- raw HTML omitted -->
<p>è¿›ä¸€æ­¥å…³æ³¨<code>Encoder</code>ä¸<code>Decoder</code>ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">clones</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)])</span>


<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div><p>ä»ä¸Šè¿°ä»£ç ï¼Œå¯ä»¥çœ‹å‡ºï¼š <code>Encoder</code>åœ¨æ„é€ æ—¶éœ€è¦ä¼ å…¥ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªä¸º<code>layer</code>, ä¸€ä¸ªä¸º<code>N</code>ã€‚è€Œåœ¨æ„é€ å‡½æ•°ä¸­ï¼Œé€šè¿‡è°ƒç”¨<code>clones</code>æ–¹æ³•å°†ä¼ å…¥çš„<code>layer</code>æ·±å¤åˆ¶(<code>deepcopy</code>)äº†Næ¬¡ï¼Œå¹¶ä½œä¸ºä¸€ä¸ª<code>ModuleList</code>å­˜å‚¨åœ¨<code>self.layers</code>æˆå‘˜å˜é‡ä¸­ã€‚</p>
<p><code>clones</code>æ–¹æ³•çš„æ‰§è¡Œæ•ˆæœå¯ä»ä¸‹ä¾‹ä¸­çª¥è§ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;net&#39;</span><span class="p">,</span><span class="n">net</span><span class="p">)</span>
<span class="n">net_clones</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;net_clones&#39;</span><span class="p">,</span> <span class="n">net_clones</span><span class="p">)</span>
</code></pre></div><p>å…¶æ‰§è¡Œç»“æœæ˜¯ï¼šå°†<code>net</code>å¤åˆ¶äº†3æ¬¡ï¼Œè£…åœ¨ä¸€ä¸ª<code>ModuleList</code>ä¸­è¿”å›ã€‚å¹¶ä¸”å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™é‡Œæ˜¯<strong>copy.deepcopy()</strong>ï¼Œæ·±å¤åˆ¶ï¼Œæ‰€ä»¥å‚æ•°ä¹‹é—´ä¸å…±äº«ã€‚</p>
<!-- raw HTML omitted -->
<p>è€ŒLayerNormå®šä¹‰å¦‚ä¸‹:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>         <span class="c1"># æœ€åä¸€ç»´çš„å‡å€¼</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>           <span class="c1"># æœ€åä¸€ç»´çš„æ ‡å‡†å·®</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_2</span>      <span class="c1"># å¯¹æœ€åä¸€ç»´è¿›è¡ŒNorm</span>
</code></pre></div><p>å¯è§LayerNormçš„ä½œç”¨æ˜¯å¯¹è¾“å…¥çš„æ•°æ®<code>x</code>ï¼Œå¯¹å…¶æœ€åä¸€ç»´è¿›è¡Œå½’ä¸€åŒ–æ“ä½œã€‚</p>
<p>è€ŒEncoderæ•´ä¸ªçš„å‰ä¼ æœºåˆ¶ä¸ºï¼šå¯¹äºæ„é€ æ—¶ç”Ÿæˆçš„ModuleList,ä¾æ¬¡å°†<code>x</code>é€šè¿‡<code>ModuleList</code>ä¸­çš„æ¯ä¸ªç½‘ç»œ<code>layer</code>ï¼Œå³<code>x = layer(x, mask)</code>, ç„¶åå†å°†è¾“å‡ºé€šè¿‡LayerNormå¯¹æœ€åä¸€ç»´è¿›è¡Œå½’ä¸€åŒ–æ“ä½œè¿”å›ã€‚ Encoderçš„ç½‘ç»œç»“æ„å›¾å¦‚ä¸‹ï¼š</p>
<!-- raw HTML omitted -->
<p>è¿›ä¸€æ­¥å…³æ³¨ï¼Œ<code>Encoder</code>åœ¨<code>EncoderDecoder</code>æ„é€ æ—¶ï¼Œä¼ å…¥çš„layerå‚æ•°ä¸ºï¼š<code>EncoderLayer(self.emb_dims, c(attn), c(ff), self.dropout)</code>ã€‚ è§£è¯»<code>EncoderLayer</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">)</span>
</code></pre></div><p><code>EncoderLayer</code>åœ¨æ„é€ æ—¶éœ€è¦ä¼ å…¥çš„å‚æ•°æœ‰ï¼š<code>size, self_attn, feed_forward, dropout</code>ã€‚ å…¶ä¸­<code>self_attn, feed_forward</code>ä¸¤ä¸ªç½‘ç»œï¼Œä»¥åŠ<code>size</code>ä½œä¸ºæˆå‘˜å˜é‡å­˜å‚¨ã€‚è€Œå¦ä¸€ä¸ªæˆå‘˜å˜é‡<code>sublayer</code>é€šè¿‡<code>clones</code>æ–¹æ³•å°†<code>SublayerConnection(size, dropout)</code>å¤åˆ¶ä¸¤éï¼Œå­˜ç€ä¸€ä¸ªModuleList.</p>
<p>è§‚å¯Ÿ<code>SublayerConnection</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SublayerConnection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div><p><code>SublayerConnection</code>åœ¨æ„é€ æ—¶ï¼Œåªä¼ å…¥äº†<code>size, dropout</code>ä¸¤ä¸ªå‚æ•°ï¼Œç”¨äºæ„é€ LayerNormã€‚è€Œå‰ä¼ <code>forward</code>çš„æ—¶å€™ï¼Œè¿”å›çš„æ˜¯<code>x + sublayer(self.norm(x))</code>ï¼Œ å³å°†<code>x</code>é€šè¿‡äº†<code>LayerNorm</code>åï¼Œå†é€šè¿‡ä½œä¸ºå‚æ•°ä¼ å…¥çš„ç½‘ç»œ<code>sublayer</code>ï¼Œæœ€åä¸<code>x</code>ç›¸åŠ ï¼Œè¿”å›ã€‚</p>
<p>ææ¸…æ¥š<code>SublayerConnection</code>çš„æœºåˆ¶åï¼Œæˆ‘ä»¬å›çœ‹<code>EncoderLayer</code>çš„<code>forward</code>æœºåˆ¶ï¼š<code>x</code>å…ˆé€šè¿‡ä¸€ä¸ªä¼ å…¥ç½‘ç»œä¸º<code>attn</code>çš„<code>sublayer</code>ï¼Œç„¶åå†é€šè¿‡ä¸€ä¸ªä¼ å…¥ç½‘ç»œä¸º<code>feedforward</code>çš„<code>sublayer</code>. <code>EncoderLayer</code>ç½‘ç»œç»“æ„å¦‚å›¾ï¼š</p>
<!-- raw HTML omitted -->
<p>æ¥ä¸‹æ¥å…³æ³¨<code>attn</code>ï¼Œå³<code>MultiHeadedAttention</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>   <span class="c1"># (nbatches, h, num_points, num_points)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (nbatches, h, num_points, num_points)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>


<span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="c1"># h: number of heads ;  d_model: dims of model(emb_dims)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># we assume d_v always equals d_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span>         <span class="c1"># d_k æ˜¯æ¯ä¸ªheadçš„dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Same mask applied to all h heads</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))]</span>      <span class="c1"># (nbatches, h, num_points, d_k)</span>

        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span>
        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># 3) &#34;Concat&#34; using a view and apply a final linear.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</code></pre></div><p>MultiHeadAttentionå…¶æ„é€ å‡½æ•°æœ‰ä¸¤ä¸ªå‚æ•°ï¼š<code>h, d_model</code>. å…¶ä¸­<code>h</code>æ˜¯<code>head</code>çš„ä¸ªæ•°ï¼Œè€Œ<code>d_model</code>å®é™…ä¸Šå°±æ˜¯<code>emb_dims</code>ã€‚æˆ‘ä»¬æ€»æ˜¯è§„å®š<code>emb_dims</code>æ˜¯å¯ä»¥æ•´é™¤<code>h</code>çš„ï¼Œå¦åˆ™æ¯ä¸ª<code>attention</code>çš„ç»´åº¦ä¸æ˜¯æ•´æ•°ã€‚<code>d_k = d_model // h</code>å³æ˜¯æ¯ä¸ª<code>head</code>çš„ç»´åº¦ã€‚åŒæ—¶åœ¨æ„é€ æ—¶ï¼Œåœ¨self.linearsä¸­å­˜å‚¨äº†ä¸€ä¸ª<code>ModuleList</code>,<code>ModuleList</code>ä¸­æœ‰å››ä¸ª<code>Linear</code>çº¿æ€§æ˜ å°„<code>d_model --&gt; d_model</code>ã€‚</p>
<p>MultiHeadAttentionçš„å‰ä¼ æœºåˆ¶ï¼š</p>
<p>(1) é€šè¿‡çº¿æ€§æ˜ å°„<code>linear projection</code>ï¼Œç”Ÿæˆ<code>query, key, value</code>.</p>
<p>ç”¨<code>zip</code>æ–¹æ³•å°†ä¸‰ä¸ªçº¿æ€§æ˜ å°„ç»‘å®šåˆ°<code>query, key, value</code>ä¸Šï¼Œç›¸å½“äºæŒ‡å®šäº†å…¶ç”Ÿæˆçš„çŸ©é˜µã€‚ï¼ˆè¿™é‡Œçš„<code>query, key, value</code>åªæ˜¯ç”¨äºç”Ÿæˆ<code>query, key, value</code>çš„åŸå§‹æ•°æ®ï¼Œäº‹å®ä¸Šéƒ½æ˜¯<code>x</code>ï¼‰ã€‚ å°†<code>query, key, value</code>åˆ†åˆ«é€šè¿‡å¯¹åº”çš„<code>Linear Projection</code>æŠ•å½±ç”ŸæˆçœŸæ­£çš„<code>query, key, value</code>ã€‚ è¾“å…¥çš„<code>x</code>çš„<code>shape</code>æ˜¯<code>nbatches, num_points, emb_dims</code> (<code>emb_dim</code> å³ <code>d_model</code>). ç»è¿‡å¯¹åº”çš„<code>Linear Projection</code>åï¼Œç”Ÿæˆçš„<code>shape</code>æ˜¯<code>nbatches, num_points, emb_dims</code>, é€šè¿‡<code>view()</code>å˜ä¸º<code>nbatches, num_points, self.h, self.d_k</code>ã€‚ éšååˆè¿›è¡Œäº†<code>transpose(1,2).contiguous()</code>, é‚£ä¹ˆæœ€åç”Ÿæˆçš„<code>query, key, value</code>çš„<code>shape</code>æ˜¯<code>nbatches, self.h, num_points, self.d_k</code>.</p>
<p>éœ€è¦è¯´æ˜çš„æ˜¯ï¼ŒæŒ‰ç…§Transformerçš„ç†è®ºï¼ŒMultiHeadAttentionçš„ç”ŸæˆçŸ©é˜µ(å³æˆ‘ä»¬åœ¨ä¸Šé¢ç”¨çš„æŠ•å½±åº”è¯¥æ˜¯æ¯ä¸ªheadæœ‰ä¸€ä¸ªå•ç‹¬çš„projection)ï¼Œä½†æ˜¯å› ä¸ºè¿™ä¸ªprojectionæ˜¯å­¦ä¹ å¾—åˆ°çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬åªç”¨ä¸€ä¸ª<code>projection</code>ç„¶åå†è¿›è¡Œ<code>view()</code>åˆ†å‰²å¾—åˆ°<code>MultiHead</code>ï¼Œåœ¨ç†è®ºä¸Šåº”è¯¥èƒ½å¾—åˆ°ç›¸åŒçš„æ•ˆæœã€‚</p>
<p>(2) æ ¹æ®å¾—åˆ°çš„<code>query, key, value</code>è®¡ç®—Self-Attention.</p>
<p>Self-Attentionçš„è®¡ç®—ï¼š $softmax(\frac{Q \times K^T}{\sqrt{d_k}}) V$</p>
<!-- raw HTML omitted -->
<p>è¿”å›çš„<code>z</code>çš„shapeä¸ºï¼š<code>nbatches, self.h, num_points, self.d_k</code></p>
<p>(3) é€šè¿‡<code>view</code>è¿›è¡Œæ‰€è°“çš„<code>Concatenate</code>ï¼Œå°†ä¹‹åº”ç”¨äº<code>Linear</code>ç½‘ç»œï¼Œè¾“å‡ºã€‚</p>
<p>é¦–å…ˆè¿›è¡Œä¸€ä¸ª<code>transpose(1,2)</code>,éšåæ”¹å˜å…¶å†…å­˜åˆ†å¸ƒ<code>contiguous</code>ï¼Œç„¶åå†é€šè¿‡<code>view()</code>ï¼Œç›¸å½“äºæŠŠå¤šä¸ªå¤´çš„<code>attention</code>æ‹¼æ¥èµ·æ¥ã€‚æ­¤æ—¶çš„shapeä¸ºï¼š<code>nbatches, num_points, h * d_k</code>.</p>
<p>å†åº”ç”¨äºç¬¬å››ä¸ª<code>Linear</code>ä¸Šï¼Œè¾“å‡ºçš„shape: (<code>nbatches, num_points, d_model</code>)</p>
<p>æ•´ä¸ª<code>Attention</code>çš„ç½‘ç»œç»“æ„å¦‚ä¸‹ï¼š
<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>ä¸‹é¢å…³æ³¨<code>PositionwiseFeedForward</code>ç½‘ç»œç»“æ„ï¼š</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div><p><code>PositionwistFeedForward</code>çš„ç½‘ç»œç»“æ„æ¯”è¾ƒç®€å•ï¼Œä¸å†å•ç‹¬åˆ†æã€‚
æ•´ä¸ª<code>Encoder</code>çš„ç»“æ„åˆ†æå®Œæ¯•ã€‚ ä¸‹é¢å†å…³æ³¨ä¸€ä¸‹<code>Decoder</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Decoder is made of self-attn, src-attn, and feed forward(defined below)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">tgt_mask</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">src_mask</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
</code></pre></div><p>Decoderçš„ç½‘ç»œç»“æ„å¦‚å›¾ï¼š</p>
<!-- raw HTML omitted -->
<p>è€ŒDecoderLayerï¼Œæ¥æ”¶çš„å˜é‡æœ‰<code>x</code>,<code>memory</code>,<code>src_mask</code>,<code>tgt_mask</code>ï¼Œç›¸å½“äºå…ˆè®¡ç®—<code>self-attention</code>,å†è®¡ç®—<code>co-attention</code>ï¼Œæœ€å<code>feed-forward</code>ï¼Œå…¶ç½‘ç»œç»“æ„ä¸ºï¼š
<!-- raw HTML omitted --></p>
<p>è‡³æ­¤Transformeråº”è¯¥å·²ç»åˆ†ææ¸…æ¥šäº†ã€‚</p>
<h3 id="ç”¨äºsvdæ±‚è§£çš„æ¨¡å—module3">ç”¨äºSVDæ±‚è§£çš„æ¨¡å—Module3</h3>
<p>æˆ‘ä»¬çš„æœ€ç»ˆç›®çš„æ˜¯æ±‚å‡ºåˆšä½“å˜æ¢çŸ©é˜µã€‚ä½¿ç”¨ä¸Šä¸€ä¸ª<code>Module</code>ä¸­è®¡ç®—å‡ºçš„<code>soft pointer</code>ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªå¹³å‡æ„ä¹‰ä¸Šçš„<code>match point</code>ã€‚
$$
\hat{y_i} = (Y_m)^T m(x_i,Y)
$$
è¿™é‡Œï¼Œ$Y_m$æ˜¯æŒ‡ä¸€ä¸ª$R^{N \times 3}$çš„çŸ©é˜µï¼ŒåŒ…å«ç€$Y$ä¸­æ‰€æœ‰ç‚¹çš„ä¿¡æ¯ã€‚ æ ¹æ®è¿™æ ·ä¸€ç§<code>match</code>å…³ç³»ï¼Œä¾¿å¯ä»¥é€šè¿‡<code>SVD</code>æ±‚è§£å‡ºåˆšä½“å˜æ¢çŸ©é˜µã€‚</p>
<p>ä¸ºäº†èƒ½å¤Ÿä½¿å¾—æ¢¯åº¦åå‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹SVDè¿›è¡Œæ±‚å¯¼ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨SVDæ±‚è§£çš„åªæ˜¯3x3ï¼Œå¯ä»¥ä½¿ç”¨å…¶å¯¼æ•°çš„è¿‘ä¼¼å½¢å¼ã€‚SVDæ±‚å¯¼çš„è¯¦ç»†è¯·å‚è€ƒè®ºæ–‡:</p>
<blockquote>
<p>Estimating the jacobian of the singular value decomposition: Theory and applications</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SVDHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SVDHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reflect</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reflect</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">src_embedding</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">src</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">d_k</span> <span class="o">=</span> <span class="n">src_embedding</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">src_embedding</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">tgt_embedding</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">src_corr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

        <span class="n">src_centered</span> <span class="o">=</span> <span class="n">src</span> <span class="o">-</span> <span class="n">src_mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">src_corr_centered</span> <span class="o">=</span> <span class="n">src_corr</span> <span class="o">-</span> <span class="n">src_corr</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">src_centered</span><span class="p">,</span> <span class="n">src_corr_centered</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

        <span class="n">U</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="p">[],[],[]</span>
        <span class="n">R</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
            <span class="n">r_det</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">r_det</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reflect</span><span class="p">)</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

            <span class="n">R</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

            <span class="n">U</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">S</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">V</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="o">-</span><span class="n">R</span><span class="p">,</span> <span class="n">src</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">))</span> <span class="o">+</span> <span class="n">src_corr</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">R</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
</code></pre></div><p><!-- raw HTML omitted -->ç–‘æƒ‘<!-- raw HTML omitted -->ï¼š æ²¡åœ¨æºç ä¸­çœ‹åˆ°æœ‰é’ˆå¯¹SVDæ±‚å¯¼çš„ä¼˜åŒ–ã€‚</p>
<h3 id="æŸå¤±å‡½æ•°">æŸå¤±å‡½æ•°</h3>
<p>$$
Loss = ||R_{XY}^TR_{XY}^g - I||^2 + ||t_{XY} - t_{XY}^g||^2 + \lambda||\theta||^2
$$</p>
<h3 id="å…³äºdcp_v1å’Œdcp_v2">å…³äºDCP_v1å’ŒDCP_v2</h3>
<p>DCP_v1 æ²¡æœ‰åº”ç”¨<code>Attention</code>æœºåˆ¶ã€‚</p>
<p>DCP_v2 åº”ç”¨äº†<code>Attention</code>æœºåˆ¶ã€‚</p>
<h2 id="æ·±å…¥æ¢ç©¶">æ·±å…¥æ¢ç©¶</h2>
<h3 id="å…³äºç‰¹å¾æå–é€‰æ‹©pointnetè¿˜æ˜¯dgcnn">å…³äºç‰¹å¾æå–ï¼šé€‰æ‹©PointNetè¿˜æ˜¯DGCNN?</h3>
<p>PointNet å­¦ä¹ çš„æ˜¯å…¨å±€ç‰¹å¾ï¼Œ è€ŒDGCNNé€šè¿‡æ„å»º<code>k-NN</code> Graphå­¦ä¹ åˆ°çš„æ˜¯å±€éƒ¨é›†åˆç‰¹å¾ã€‚</p>
<p>ä»æ–‡ä¸­çš„å®éªŒç»“æœå¯ä»¥çœ‹å‡ºï¼Œä½¿ç”¨DGCNNä½œä¸ºemb_netï¼Œæ¯”PointNetçš„æ€§èƒ½å§‹ç»ˆè¦å¥½ã€‚
<!-- raw HTML omitted --></p>
<h3 id="å…³äºè®¡ç®—åˆšä½“å˜æ¢é€‰æ‹©mlpè¿˜æ˜¯svd">å…³äºè®¡ç®—åˆšä½“å˜æ¢ï¼šé€‰æ‹©MLPè¿˜æ˜¯SVDï¼Ÿ</h3>
<p>MLPåœ¨ç†è®ºä¸Šå¯ä»¥æ¨¡æ‹Ÿä»»ä½•éçº¿æ€§æ˜ å°„ã€‚ è€ŒSVDæ˜¯é’ˆå¯¹ä»»åŠ¡è¿›è¡Œæœ‰ç›®çš„æ€§è®¾è®¡çš„ç½‘ç»œã€‚</p>
<p>ä»æ–‡ä¸­çš„å®éªŒç»“æœå¯ä»¥çœ‹å‡ºï¼Œä½¿ç”¨SVDè®¡ç®—<code>rigid transformation</code>æ€»æ˜¯æ›´ä¼˜ã€‚
<!-- raw HTML omitted --></p>
<h1 id="ç»“è¯­">ç»“è¯­</h1>
<p>ä»¥ä¸Šå°±æ˜¯æˆ‘ä¸ªäººå¯¹DCPçš„ç¬”è®°è®°å½•ä»¥åŠä¸€äº›è§£è¯»ã€‚</p>
<p>å¦‚æœè§‰å¾—æ–‡ç« å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç‚¹èµç•™è¨€äº¤æµã€‚ç»™ä½œè€…ä¹°æ¯å’–å•¡å°±æ›´æ„Ÿæ¿€ä¸è¿‡äº†ï¼
<!-- raw HTML omitted --></p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/%E7%82%B9%E4%BA%91%E9%85%8D%E5%87%86/">ç‚¹äº‘é…å‡†</a>
        
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">æ·±åº¦å­¦ä¹ </a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.querySelector(`.article-content`));"></script>
<script>
var katex_config = {
    delimiters: 
    [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false}
    ]
};
</script>
<script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" onload="renderMathInElement(document.body,katex_config)"></script>

    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="">
    <a href="/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">
        
        

        <div class="article-details">
            <h2 class="article-title">DGCNNè®ºæ–‡è§£è¯»</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>


    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "codefmeister" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2020 Codefmeister
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="1.1.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true" style="display:none">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
            </main>
        </div>
        <script src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"
    integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin="anonymous"></script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<link rel="stylesheet" href="/css/highlight/light.min.css" media="(prefers-color-scheme: light)">
<link rel="stylesheet" href="/css/highlight/dark.min.css" media="(prefers-color-scheme: dark)">

    </body>
</html>
