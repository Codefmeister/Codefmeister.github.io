<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞ ËÆ∫Êñá  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution Âü∫‰∫éICPËø≠‰ª£ÊúÄËøëÁÇπÁÆóÊ≥ïÔºåÊèêÂá∫Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑDCPÁÆóÊ≥ï„ÄÇËß£ÂÜ≥‰∫ÜICPÊÉ≥Ë¶ÅÈááÁî®Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÊó∂ÈÅáÂà∞ÁöÑ‰∏ÄÁ≥ªÂàóÈóÆÈ¢ò„ÄÇ
Êàë‰ª¨ÂÖàÂõûÈ°æ‰∏Ä‰∏ãICPÁÆóÊ≥ïÁöÑÂü∫Êú¨Ê≠•È™§Ôºö
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data Ê¶ÇÊã¨Ëµ∑Êù•Â∞±ÊòØÔºö ÂØªÊâæÊúÄËøëÁÇπÂØπÂÖ≥Á≥ªÔºå‰ΩøÁî®SVDÊ±ÇËß£Âàö‰ΩìÂèòÊç¢„ÄÇÂ¶ÇÊ≠§Âæ™ÁéØÂæÄÂ§ç„ÄÇ
ÁªìÂêàËÆ∫ÊñáÔºå‰∏™‰∫∫ÁêÜËß£Â∞ÜICPÁÆóÊ≥ïÊâ©Â±ïÂà∞Ê∑±Â∫¶Â≠¶‰π†Â≠òÂú®ÁùÄ‰ª•‰∏ãÁöÑÈöæÁÇπÔºàÂèØËÉΩÂ≠òÂú®ÂêÑÁßçÈóÆÈ¢òÔºåÁ¨îËÄÖÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÂæàËñÑÂº±ÔºâÔºö
 È¶ñÂÖàÔºåÁÇπÂØπÂÖ≥Á≥ªÂ¶ÇÊûúÊòØÁ°ÆÂÆöÁöÑËØùÔºåÊ≤øÁùÄÁΩëÁªúÂèçÂêë‰º†Êí≠ÂèØËÉΩÂ≠òÂú®ÈóÆÈ¢ò„ÄÇ SVDÂàÜËß£Ê±ÇËß£Âàö‰ΩìÂèòÊç¢ÔºåÂ¶Ç‰ΩïÊ±ÇÊ¢ØÂ∫¶Ôºü(Confirmed by paper)  ËÄåÊñáÁ´†ÂÖãÊúç‰∫ÜËøô‰∫õÈóÆÈ¢òÔºå‰∏ªË¶ÅÊúâÂ¶Ç‰∏ãË¥°ÁåÆÔºö
 ÊèêÂá∫‰∫ÜËÉΩÂ§üËß£ÂÜ≥‰º†ÁªüICPÁÆóÊ≥ïËØïÂõæÊé®ÂπøÊó∂Â≠òÂú®ÁöÑÂõ∞ÈöæÁöÑ‰∏ÄÁ≥ªÂàóÂ≠êÁΩëÁªúÊû∂ÊûÑ„ÄÇ ÊèêÂá∫‰∫ÜËÉΩËøõË°åpair-wiseÈÖçÂáÜÁöÑÁΩëÁªúÊû∂ÊûÑ ËØÑ‰º∞‰∫ÜÂú®ÈááÁî®‰∏çÂêåËÆæÁΩÆÁöÑÊÉÖÂÜµ‰∏ãÁöÑÁΩëÁªúË°®Áé∞ ÂàÜÊûê‰∫ÜÊòØglobal featureÊúâÁî®ËøòÊòØlocal featureÂØπÈÖçÂáÜÊõ¥Âä†ÊúâÁî®  ÁΩëÁªúÊû∂ÊûÑ Ê®°ÂûãÂåÖÂê´‰∏â‰∏™ÈÉ®ÂàÜÔºö
(1) ‰∏Ä‰∏™Â∞ÜËæìÂÖ•ÁÇπ‰∫ëÊò†Â∞ÑÂà∞È´òÁª¥Á©∫Èó¥embeddingÁöÑÊ®°ÂùóÔºåÂÖ∑ÊúâÊâ∞Âä®‰∏çÂèòÊÄßÔºàÊåáDGCNNÂΩìÁÇπ‰∫ëËæìÂÖ•Êó∂ÁÇπÁöÑÂâçÂêéÈ°∫Â∫èÂèëÁîüÂèòÂåñÔºåËæìÂá∫‰∏ç‰ºöÊúâ‰ªª‰ΩïÊîπÂèòÔºâ ÊàñËÄÖ Âàö‰ΩìÂèòÊç¢‰∏çÂèòÊÄßÔºàÊåáPointNetÂØπ‰∫éÊóãËΩ¨Âπ≥ÁßªÂÖ∑Êúâ‰∏çÂèòÁöÑÁâπÊÄßÔºâ„ÄÇËØ•Ê®°ÂùóÁöÑ‰ΩúÁî®ÊòØÂØªÊâæ‰∏§‰∏™ËæìÂÖ•ÁÇπ‰∫ë‰πãÈó¥ÁöÑÁÇπÁöÑÂØπÂ∫îÂÖ≥Á≥ª. ÂèØÈÄâÁöÑÊ®°ÂùóÊúâPointNetÔºàFocus‰∫éÂÖ®Â±ÄÁâπÂæÅÔºâÔºå DGCNNÔºàÁªìÂêàÂ±ÄÈÉ®ÁâπÂæÅÂíåÂÖ®Â±ÄÁâπÂæÅÔºâ„ÄÇ'><title>DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞</title>

<link rel='canonical' href='https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞'>
<meta property='og:description' content='DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞ ËÆ∫Êñá  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution Âü∫‰∫éICPËø≠‰ª£ÊúÄËøëÁÇπÁÆóÊ≥ïÔºåÊèêÂá∫Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑDCPÁÆóÊ≥ï„ÄÇËß£ÂÜ≥‰∫ÜICPÊÉ≥Ë¶ÅÈááÁî®Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÊó∂ÈÅáÂà∞ÁöÑ‰∏ÄÁ≥ªÂàóÈóÆÈ¢ò„ÄÇ
Êàë‰ª¨ÂÖàÂõûÈ°æ‰∏Ä‰∏ãICPÁÆóÊ≥ïÁöÑÂü∫Êú¨Ê≠•È™§Ôºö
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data Ê¶ÇÊã¨Ëµ∑Êù•Â∞±ÊòØÔºö ÂØªÊâæÊúÄËøëÁÇπÂØπÂÖ≥Á≥ªÔºå‰ΩøÁî®SVDÊ±ÇËß£Âàö‰ΩìÂèòÊç¢„ÄÇÂ¶ÇÊ≠§Âæ™ÁéØÂæÄÂ§ç„ÄÇ
ÁªìÂêàËÆ∫ÊñáÔºå‰∏™‰∫∫ÁêÜËß£Â∞ÜICPÁÆóÊ≥ïÊâ©Â±ïÂà∞Ê∑±Â∫¶Â≠¶‰π†Â≠òÂú®ÁùÄ‰ª•‰∏ãÁöÑÈöæÁÇπÔºàÂèØËÉΩÂ≠òÂú®ÂêÑÁßçÈóÆÈ¢òÔºåÁ¨îËÄÖÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÂæàËñÑÂº±ÔºâÔºö
 È¶ñÂÖàÔºåÁÇπÂØπÂÖ≥Á≥ªÂ¶ÇÊûúÊòØÁ°ÆÂÆöÁöÑËØùÔºåÊ≤øÁùÄÁΩëÁªúÂèçÂêë‰º†Êí≠ÂèØËÉΩÂ≠òÂú®ÈóÆÈ¢ò„ÄÇ SVDÂàÜËß£Ê±ÇËß£Âàö‰ΩìÂèòÊç¢ÔºåÂ¶Ç‰ΩïÊ±ÇÊ¢ØÂ∫¶Ôºü(Confirmed by paper)  ËÄåÊñáÁ´†ÂÖãÊúç‰∫ÜËøô‰∫õÈóÆÈ¢òÔºå‰∏ªË¶ÅÊúâÂ¶Ç‰∏ãË¥°ÁåÆÔºö
 ÊèêÂá∫‰∫ÜËÉΩÂ§üËß£ÂÜ≥‰º†ÁªüICPÁÆóÊ≥ïËØïÂõæÊé®ÂπøÊó∂Â≠òÂú®ÁöÑÂõ∞ÈöæÁöÑ‰∏ÄÁ≥ªÂàóÂ≠êÁΩëÁªúÊû∂ÊûÑ„ÄÇ ÊèêÂá∫‰∫ÜËÉΩËøõË°åpair-wiseÈÖçÂáÜÁöÑÁΩëÁªúÊû∂ÊûÑ ËØÑ‰º∞‰∫ÜÂú®ÈááÁî®‰∏çÂêåËÆæÁΩÆÁöÑÊÉÖÂÜµ‰∏ãÁöÑÁΩëÁªúË°®Áé∞ ÂàÜÊûê‰∫ÜÊòØglobal featureÊúâÁî®ËøòÊòØlocal featureÂØπÈÖçÂáÜÊõ¥Âä†ÊúâÁî®  ÁΩëÁªúÊû∂ÊûÑ Ê®°ÂûãÂåÖÂê´‰∏â‰∏™ÈÉ®ÂàÜÔºö
(1) ‰∏Ä‰∏™Â∞ÜËæìÂÖ•ÁÇπ‰∫ëÊò†Â∞ÑÂà∞È´òÁª¥Á©∫Èó¥embeddingÁöÑÊ®°ÂùóÔºåÂÖ∑ÊúâÊâ∞Âä®‰∏çÂèòÊÄßÔºàÊåáDGCNNÂΩìÁÇπ‰∫ëËæìÂÖ•Êó∂ÁÇπÁöÑÂâçÂêéÈ°∫Â∫èÂèëÁîüÂèòÂåñÔºåËæìÂá∫‰∏ç‰ºöÊúâ‰ªª‰ΩïÊîπÂèòÔºâ ÊàñËÄÖ Âàö‰ΩìÂèòÊç¢‰∏çÂèòÊÄßÔºàÊåáPointNetÂØπ‰∫éÊóãËΩ¨Âπ≥ÁßªÂÖ∑Êúâ‰∏çÂèòÁöÑÁâπÊÄßÔºâ„ÄÇËØ•Ê®°ÂùóÁöÑ‰ΩúÁî®ÊòØÂØªÊâæ‰∏§‰∏™ËæìÂÖ•ÁÇπ‰∫ë‰πãÈó¥ÁöÑÁÇπÁöÑÂØπÂ∫îÂÖ≥Á≥ª. ÂèØÈÄâÁöÑÊ®°ÂùóÊúâPointNetÔºàFocus‰∫éÂÖ®Â±ÄÁâπÂæÅÔºâÔºå DGCNNÔºàÁªìÂêàÂ±ÄÈÉ®ÁâπÂæÅÂíåÂÖ®Â±ÄÁâπÂæÅÔºâ„ÄÇ'>
<meta property='og:url' content='https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/'>
<meta property='og:site_name' content='Codefmeister'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='ÁÇπ‰∫ëÈÖçÂáÜ' /><meta property='article:tag' content='Ê∑±Â∫¶Â≠¶‰π†' /><meta property='article:published_time' content='2020-12-22T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2020-12-22T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞">
<meta name="twitter:description" content="DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞ ËÆ∫Êñá  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution Âü∫‰∫éICPËø≠‰ª£ÊúÄËøëÁÇπÁÆóÊ≥ïÔºåÊèêÂá∫Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑDCPÁÆóÊ≥ï„ÄÇËß£ÂÜ≥‰∫ÜICPÊÉ≥Ë¶ÅÈááÁî®Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÊó∂ÈÅáÂà∞ÁöÑ‰∏ÄÁ≥ªÂàóÈóÆÈ¢ò„ÄÇ
Êàë‰ª¨ÂÖàÂõûÈ°æ‰∏Ä‰∏ãICPÁÆóÊ≥ïÁöÑÂü∫Êú¨Ê≠•È™§Ôºö
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data Ê¶ÇÊã¨Ëµ∑Êù•Â∞±ÊòØÔºö ÂØªÊâæÊúÄËøëÁÇπÂØπÂÖ≥Á≥ªÔºå‰ΩøÁî®SVDÊ±ÇËß£Âàö‰ΩìÂèòÊç¢„ÄÇÂ¶ÇÊ≠§Âæ™ÁéØÂæÄÂ§ç„ÄÇ
ÁªìÂêàËÆ∫ÊñáÔºå‰∏™‰∫∫ÁêÜËß£Â∞ÜICPÁÆóÊ≥ïÊâ©Â±ïÂà∞Ê∑±Â∫¶Â≠¶‰π†Â≠òÂú®ÁùÄ‰ª•‰∏ãÁöÑÈöæÁÇπÔºàÂèØËÉΩÂ≠òÂú®ÂêÑÁßçÈóÆÈ¢òÔºåÁ¨îËÄÖÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÂæàËñÑÂº±ÔºâÔºö
 È¶ñÂÖàÔºåÁÇπÂØπÂÖ≥Á≥ªÂ¶ÇÊûúÊòØÁ°ÆÂÆöÁöÑËØùÔºåÊ≤øÁùÄÁΩëÁªúÂèçÂêë‰º†Êí≠ÂèØËÉΩÂ≠òÂú®ÈóÆÈ¢ò„ÄÇ SVDÂàÜËß£Ê±ÇËß£Âàö‰ΩìÂèòÊç¢ÔºåÂ¶Ç‰ΩïÊ±ÇÊ¢ØÂ∫¶Ôºü(Confirmed by paper)  ËÄåÊñáÁ´†ÂÖãÊúç‰∫ÜËøô‰∫õÈóÆÈ¢òÔºå‰∏ªË¶ÅÊúâÂ¶Ç‰∏ãË¥°ÁåÆÔºö
 ÊèêÂá∫‰∫ÜËÉΩÂ§üËß£ÂÜ≥‰º†ÁªüICPÁÆóÊ≥ïËØïÂõæÊé®ÂπøÊó∂Â≠òÂú®ÁöÑÂõ∞ÈöæÁöÑ‰∏ÄÁ≥ªÂàóÂ≠êÁΩëÁªúÊû∂ÊûÑ„ÄÇ ÊèêÂá∫‰∫ÜËÉΩËøõË°åpair-wiseÈÖçÂáÜÁöÑÁΩëÁªúÊû∂ÊûÑ ËØÑ‰º∞‰∫ÜÂú®ÈááÁî®‰∏çÂêåËÆæÁΩÆÁöÑÊÉÖÂÜµ‰∏ãÁöÑÁΩëÁªúË°®Áé∞ ÂàÜÊûê‰∫ÜÊòØglobal featureÊúâÁî®ËøòÊòØlocal featureÂØπÈÖçÂáÜÊõ¥Âä†ÊúâÁî®  ÁΩëÁªúÊû∂ÊûÑ Ê®°ÂûãÂåÖÂê´‰∏â‰∏™ÈÉ®ÂàÜÔºö
(1) ‰∏Ä‰∏™Â∞ÜËæìÂÖ•ÁÇπ‰∫ëÊò†Â∞ÑÂà∞È´òÁª¥Á©∫Èó¥embeddingÁöÑÊ®°ÂùóÔºåÂÖ∑ÊúâÊâ∞Âä®‰∏çÂèòÊÄßÔºàÊåáDGCNNÂΩìÁÇπ‰∫ëËæìÂÖ•Êó∂ÁÇπÁöÑÂâçÂêéÈ°∫Â∫èÂèëÁîüÂèòÂåñÔºåËæìÂá∫‰∏ç‰ºöÊúâ‰ªª‰ΩïÊîπÂèòÔºâ ÊàñËÄÖ Âàö‰ΩìÂèòÊç¢‰∏çÂèòÊÄßÔºàÊåáPointNetÂØπ‰∫éÊóãËΩ¨Âπ≥ÁßªÂÖ∑Êúâ‰∏çÂèòÁöÑÁâπÊÄßÔºâ„ÄÇËØ•Ê®°ÂùóÁöÑ‰ΩúÁî®ÊòØÂØªÊâæ‰∏§‰∏™ËæìÂÖ•ÁÇπ‰∫ë‰πãÈó¥ÁöÑÁÇπÁöÑÂØπÂ∫îÂÖ≥Á≥ª. ÂèØÈÄâÁöÑÊ®°ÂùóÊúâPointNetÔºàFocus‰∫éÂÖ®Â±ÄÁâπÂæÅÔºâÔºå DGCNNÔºàÁªìÂêàÂ±ÄÈÉ®ÁâπÂæÅÂíåÂÖ®Â±ÄÁâπÂæÅÔºâ„ÄÇ">
    </head>
    <body class="">
        <div class="container flex on-phone--column align-items--flex-start extended article-page with-toolbar">
            <aside class="sidebar left-sidebar sticky">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header class="site-info">
        
            <figure class="site-avatar">
                
                    
                    
                    
                        
                        <img src="/img/Icon_hua25ec96b536dfdbbcc947accbc1cb594_86128_300x0_resize_q75_box.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                

                
                    <span class="emoji">üç•</span>
                
            </figure>
        
        <h1 class="site-name"><a href="https://codefmeister.github.io">Codefmeister</a></h1>
        <h2 class="site-description">Major in Computer science</h2>
    </header>

    <ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
    </ol>
</aside>

            <main class="main full-width">
    <div id="article-toolbar">
        <a href="https://codefmeister.github.io" class="back-home">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



            <span>Back</span>
        </a>
    </div>

    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <h2 class="article-title">
        <a href="/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞</a>
    </h2>

    <footer class="article-time">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--published">Dec 22, 2020</time>
    </footer></div>
</header>

    <section class="article-content">
    <h1 id="dcpËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞">DCPËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞</h1>
<h2 id="ËÆ∫Êñá">ËÆ∫Êñá</h2>
<blockquote>
<p>Deep Closest Point: Learning Representations for Point Cloud Registration<br>
Author: Wang, Yue; Solomon, Justin</p>
</blockquote>
<h2 id="main-attribution">Main Attribution</h2>
<p>Âü∫‰∫éICPËø≠‰ª£ÊúÄËøëÁÇπÁÆóÊ≥ïÔºåÊèêÂá∫Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑDCPÁÆóÊ≥ï„ÄÇËß£ÂÜ≥‰∫ÜICPÊÉ≥Ë¶ÅÈááÁî®Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÊó∂ÈÅáÂà∞ÁöÑ‰∏ÄÁ≥ªÂàóÈóÆÈ¢ò„ÄÇ</p>
<p>Êàë‰ª¨ÂÖàÂõûÈ°æ‰∏Ä‰∏ãICPÁÆóÊ≥ïÁöÑÂü∫Êú¨Ê≠•È™§Ôºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">each</span> <span class="n">iteration</span><span class="p">:</span>
    <span class="n">find</span> <span class="n">corresponding</span> <span class="n">relations</span> <span class="n">of</span> <span class="n">points</span> <span class="n">between</span> <span class="n">two</span> <span class="n">scan</span><span class="p">(</span><span class="n">using</span> <span class="n">KNN</span><span class="p">)</span>
    <span class="n">using</span> <span class="n">SVD</span> <span class="n">to</span> <span class="n">solve</span> <span class="n">Rotation</span> <span class="n">Matrix</span> <span class="ow">and</span> <span class="n">Translation</span> <span class="n">vector</span>
    <span class="n">update</span> <span class="n">cloud</span> <span class="n">Data</span>
</code></pre></div><p>Ê¶ÇÊã¨Ëµ∑Êù•Â∞±ÊòØÔºö <!-- raw HTML omitted --><strong>ÂØªÊâæÊúÄËøëÁÇπÂØπÂÖ≥Á≥ªÔºå‰ΩøÁî®SVDÊ±ÇËß£Âàö‰ΩìÂèòÊç¢„ÄÇ</strong><!-- raw HTML omitted -->Â¶ÇÊ≠§Âæ™ÁéØÂæÄÂ§ç„ÄÇ</p>
<p>ÁªìÂêàËÆ∫ÊñáÔºå‰∏™‰∫∫ÁêÜËß£Â∞ÜICPÁÆóÊ≥ïÊâ©Â±ïÂà∞Ê∑±Â∫¶Â≠¶‰π†Â≠òÂú®ÁùÄ‰ª•‰∏ãÁöÑÈöæÁÇπÔºàÂèØËÉΩÂ≠òÂú®ÂêÑÁßçÈóÆÈ¢òÔºåÁ¨îËÄÖÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÂæàËñÑÂº±ÔºâÔºö</p>
<ul>
<li>È¶ñÂÖàÔºåÁÇπÂØπÂÖ≥Á≥ªÂ¶ÇÊûúÊòØÁ°ÆÂÆöÁöÑËØùÔºåÊ≤øÁùÄÁΩëÁªúÂèçÂêë‰º†Êí≠ÂèØËÉΩÂ≠òÂú®ÈóÆÈ¢ò„ÄÇ</li>
<li>SVDÂàÜËß£Ê±ÇËß£Âàö‰ΩìÂèòÊç¢ÔºåÂ¶Ç‰ΩïÊ±ÇÊ¢ØÂ∫¶Ôºü(Confirmed by paper)</li>
</ul>
<p>ËÄåÊñáÁ´†ÂÖãÊúç‰∫ÜËøô‰∫õÈóÆÈ¢òÔºå‰∏ªË¶ÅÊúâÂ¶Ç‰∏ãË¥°ÁåÆÔºö</p>
<ul>
<li>ÊèêÂá∫‰∫ÜËÉΩÂ§üËß£ÂÜ≥‰º†ÁªüICPÁÆóÊ≥ïËØïÂõæÊé®ÂπøÊó∂Â≠òÂú®ÁöÑÂõ∞ÈöæÁöÑ‰∏ÄÁ≥ªÂàóÂ≠êÁΩëÁªúÊû∂ÊûÑ„ÄÇ</li>
<li>ÊèêÂá∫‰∫ÜËÉΩËøõË°å<code>pair-wise</code>ÈÖçÂáÜÁöÑÁΩëÁªúÊû∂ÊûÑ</li>
<li>ËØÑ‰º∞‰∫ÜÂú®ÈááÁî®‰∏çÂêåËÆæÁΩÆÁöÑÊÉÖÂÜµ‰∏ãÁöÑÁΩëÁªúË°®Áé∞</li>
<li>ÂàÜÊûê‰∫ÜÊòØ<code>global feature</code>ÊúâÁî®ËøòÊòØ<code>local feature</code>ÂØπÈÖçÂáÜÊõ¥Âä†ÊúâÁî®</li>
</ul>
<h2 id="ÁΩëÁªúÊû∂ÊûÑ">ÁΩëÁªúÊû∂ÊûÑ</h2>
<p><img src="https://raw.githubusercontent.com/Codefmeister/MarkdownResource/master/DCP_fig1.png" alt="image"  />
Ê®°ÂûãÂåÖÂê´‰∏â‰∏™ÈÉ®ÂàÜÔºö<br>
(1) ‰∏Ä‰∏™Â∞ÜËæìÂÖ•ÁÇπ‰∫ëÊò†Â∞ÑÂà∞È´òÁª¥Á©∫Èó¥<code>embedding</code>ÁöÑÊ®°ÂùóÔºåÂÖ∑ÊúâÊâ∞Âä®‰∏çÂèòÊÄßÔºàÊåáDGCNNÂΩìÁÇπ‰∫ëËæìÂÖ•Êó∂ÁÇπÁöÑÂâçÂêéÈ°∫Â∫èÂèëÁîüÂèòÂåñÔºåËæìÂá∫‰∏ç‰ºöÊúâ‰ªª‰ΩïÊîπÂèòÔºâ ÊàñËÄÖ Âàö‰ΩìÂèòÊç¢‰∏çÂèòÊÄßÔºàÊåáPointNetÂØπ‰∫éÊóãËΩ¨Âπ≥ÁßªÂÖ∑Êúâ‰∏çÂèòÁöÑÁâπÊÄßÔºâ„ÄÇËØ•Ê®°ÂùóÁöÑ‰ΩúÁî®ÊòØ<!-- raw HTML omitted --><strong>ÂØªÊâæ‰∏§‰∏™ËæìÂÖ•ÁÇπ‰∫ë‰πãÈó¥ÁöÑÁÇπÁöÑÂØπÂ∫îÂÖ≥Á≥ª</strong><!-- raw HTML omitted -->. ÂèØÈÄâÁöÑÊ®°ÂùóÊúâ<!-- raw HTML omitted --><strong>PointNetÔºàFocus‰∫éÂÖ®Â±ÄÁâπÂæÅÔºâÔºå DGCNNÔºàÁªìÂêàÂ±ÄÈÉ®ÁâπÂæÅÂíåÂÖ®Â±ÄÁâπÂæÅÔºâ</strong><!-- raw HTML omitted -->„ÄÇ</p>
<p>(2) ‰∏Ä‰∏™Âü∫‰∫éÊ≥®ÊÑèÂäõ<code>attention</code>ÁöÑPointerÁΩëÁªúÊ®°ÂùóÔºåÁî®‰∫éÈ¢ÑÊµã‰∏§‰∏™ÁÇπ‰∫ë‰πãÈó¥ÁöÑsoft matchingÂÖ≥Á≥ª(<!-- raw HTML omitted --><strong>Á±ª‰ºº‰∫é‰∏ÄÁßçÂü∫‰∫éÊ¶ÇÁéáÁöÑsoft matchÔºå‰πãÊâÄ‰ª•softÊòØÁî±‰∫éÂÆÉÂπ∂Ê≤°ÊúâÊòæÂºèËßÑÂÆöÁÇπ$x_i$ÂøÖÈ°ª‰∏éÂì™‰∏™ÁÇπ$x_j$ÊúâÂØπÂ∫îÂÖ≥Á≥ªÔºåËÄåÊòØÈÄöËøá‰∏Ä‰∏™softmaxÂæóÂà∞ÁöÑÂêÑÁÇπÂíåÊüêÁÇπ$x_i$Â≠òÂú®ÂØπÂ∫îÂÖ≥Á≥ªÁöÑÊ¶ÇÁéá‰πò‰ª•ÂêÑÁÇπÊï∞ÊçÆÔºåÂæóÂà∞‰∏Ä‰∏™Á±ª‰ºº‰∫éÊ¶ÇÁéáÁöÑÂØπÂ∫îÁÇπÂùêÊ†á</strong><!-- raw HTML omitted -->„ÄÇ ËØ•Ê®°ÂùóÈááÁî®ÁöÑÊòØ<!-- raw HTML omitted --><strong>Transformer</strong><!-- raw HTML omitted --></p>
<p>(3) ‰∏Ä‰∏™<strong>ÂèØÂæÆ</strong>ÁöÑ<code>SVD</code>ÂàÜËß£Â±ÇÔºåÁî®‰∫éËæìÂá∫Âàö‰ΩìÂèòÊç¢Áü©Èòµ„ÄÇ</p>
<h2 id="ÈóÆÈ¢òÈòêËø∞">ÈóÆÈ¢òÈòêËø∞</h2>
<p>ÁÜüÊÇâÁÇπ‰∫ëÈÖçÂáÜÁöÑÂêåÂ≠¶Â∫îËØ•Áü•ÈÅìÔºåÈóÆÈ¢òÂçÅÂàÜÊ∏ÖÊô∞„ÄÇËøôÈáåÁõ¥Êé•Á≤ò‰∏Ä‰∏ãÂéüÊñá„ÄÇ<br>
<!-- raw HTML omitted -->
<!-- raw HTML omitted --></p>
<p>ÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºå‰ΩúËÄÖÂàÜÊûê‰∫Ü‰∏Ä‰∏ãICPÁöÑÁÆóÊ≥ïÊ≠•È™§„ÄÇÂíåÊàë‰ª¨‰∏äÈù¢ÊèèËø∞ÁöÑ‰∏ÄÊ†∑„ÄÇ<!-- raw HTML omitted --><strong>Â∞±ÊòØÁî®‰∏äÊ¨°Êõ¥Êñ∞ÂêéÁöÑ‰ø°ÊÅØÂØªÊâæÊúÄËøëÂÖ≥Á≥ªÔºåÁÑ∂ÂêéÁî®ÂØªÊâæÂà∞ÁöÑÂØπÂ∫îÂÖ≥Á≥ªSVDÊ±ÇËß£ÂæóÂà∞$R,t$.</strong><!-- raw HTML omitted --> ÊâÄ‰ª•Â¶ÇÊûúÂàùÂßãÂÄº‰∏ÄÂºÄÂßãÁîüÊàêÁöÑÊòØÂæàÂ∑ÆÁöÑ<code>corresponding relation</code>ÔºåÈÇ£‰πà‰∏Ä‰∏ãÂ∞±‰ºöÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇ</p>
<p>ËÄå‰ΩúËÄÖÁöÑÊÄùË∑ØÂ∞±ÊòØÔºö<!-- raw HTML omitted --><strong>‰ΩøÁî®Â≠¶‰π†ÁöÑÁΩëÁªúÊù•ÂæóÂà∞ÁâπÂæÅÔºåÈÄöËøáÁâπÂæÅËé∑Âæó‰∏Ä‰∏™Êõ¥Â•ΩÁöÑÂØπÂ∫îÂÖ≥Á≥ª$m(\cdot)$ÔºåÁî®Ëøô‰∏™$m(\cdot)$ÂéªËÆ°ÁÆóÂàö‰ΩìÂèòÊç¢‰ø°ÊÅØ„ÄÇ</strong><!-- raw HTML omitted --></p>
<h2 id="‰ª£Á†ÅÂàÜÊûê‰∏éÂØπÂ∫îÊ®°ÂùóËØ¶Ëß£">‰ª£Á†ÅÂàÜÊûê‰∏éÂØπÂ∫îÊ®°ÂùóËØ¶Ëß£</h2>
<p>Êàë‰ª¨ÈááÁî®‰∏ÄÁßç<code>Top-Down</code>ÁöÑËßÜËßíÊù•ÂàÜÊûêÊï¥‰∏™‰ª£Á†Å„ÄÇÂÖà‰ªéÊï¥‰ΩìÂÖ•ÊâãÔºåÁÑ∂ÂêéÈÄêÊ∏êÊãÜËß£Ê®°ÂùóËøõË°åÂàÜÊûê„ÄÇ</p>
<h3 id="Êï¥‰ΩìÊ®°Âùó">Êï¥‰ΩìÊ®°Âùó</h3>
<p>DCPÁΩëÁªúÁªìÊûÑÂàÜ‰∏∫‰∏â‰∏™PartÔºå‰ªé‰ª£Á†Å‰∏≠Â∞±ÂèØ‰ª•ÂæàÊ∏ÖÊô∞ÁöÑÁúãÂá∫Êù•ÔºöÁ¨¨‰∏Ä‰∏™ModuleÊ®°Âùó<code>emd_nn</code>Áî®‰∫éÊäΩË±°ÁâπÂæÅÔºåÁ¨¨‰∫å‰∏™ModuleÊ®°Âùó<code>pointer</code>Áî®‰∫ématch,Á¨¨‰∏â‰∏™ModuleÊ®°Âùó<code>head</code>Áî®‰∫éÊ±ÇËß£Âàö‰ΩìÂèòÊç¢Áü©ÈòµÔºåÂÖ∑‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DCP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>       <span class="c1"># args ÊòØ‰∏Ä‰∏™Â≠òÊîæÂêÑÁßçÂèÇÊï∞ÁöÑnamespace</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DCP</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">embdims</span>    <span class="c1"># Ê¨≤ÊäΩË±°Âà∞ÁöÑÁâπÂæÅÁª¥Â∫¶Ôºådefault‰∏∫ 512</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle</span>         <span class="c1"># baÁöÑÂàö‰ΩìÂèòÊç¢ÂÖ≥Á≥ªÊòØÂê¶ÈáçÊñ∞ËøõÂÖ•ÁΩëÁªúËÆ°ÁÆó</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">==</span> <span class="s1">&#39;pointnet&#39;</span><span class="p">:</span>   <span class="c1"># emb_nnÂ∞±ÊòØ‰∏äÊñáÊâÄËØ¥ÁöÑÁ¨¨‰∏Ä‰∏™Ê®°Âùó,Ëã•ÈÄâÊã©PointNet</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">=</span> <span class="n">PointNet</span><span class="p">(</span><span class="n">emb_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">==</span> <span class="s1">&#39;dgcnn&#39;</span><span class="p">:</span>    <span class="c1"># Ëã•ÈÄâÊã©DGCNN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span> <span class="o">=</span> <span class="n">DGCNN</span><span class="p">(</span><span class="n">emb_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Not implemented&#39;</span><span class="p">)</span>      <span class="c1"># ÂÖ∂‰ªñÁΩëÁªúÂ∞öÊú™ÂÆûÁé∞</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">pointer</span> <span class="o">==</span> <span class="s1">&#39;identity&#39;</span><span class="p">:</span>              <span class="c1"># ‰∏ç‰ΩøÁî®Transformer, hard match</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">pointer</span> <span class="o">==</span> <span class="s1">&#39;transformer&#39;</span><span class="p">:</span>         <span class="c1"># soft matching by tranformer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Not implemented&#39;</span><span class="p">)</span>  
        
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="s1">&#39;mlp&#39;</span><span class="p">:</span>                      <span class="c1"># Áõ¥Êé•Áî®MLPÈ¢ÑÊµãËæìÂá∫Áü©Èòµ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLPHead</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="s1">&#39;svd&#39;</span><span class="p">:</span>                    <span class="c1"># ‰ΩøÁî®ÂèØÂæÆÁöÑSVDÂàÜËß£Â±Ç</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">SVDHead</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&#34;Not implemented&#34;</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">src_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_nn</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span>    <span class="c1"># Module Part 1             (batch_size, emb_dims, num_points)</span>

        <span class="n">src_embedding_p</span><span class="p">,</span> <span class="n">tgt_embedding_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span><span class="p">(</span><span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt_embedding</span><span class="p">)</span>  <span class="c1"># Module Part 2</span>

        <span class="n">src_embedding</span> <span class="o">=</span> <span class="n">src_embedding</span> <span class="o">+</span> <span class="n">src_embedding_p</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="n">tgt_embedding</span> <span class="o">+</span> <span class="n">tgt_embedding_p</span>

        <span class="n">rotation_ab</span><span class="p">,</span> <span class="n">translation_ab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt_embedding</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> <span class="c1"># Module Part 3</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span><span class="p">:</span>
            <span class="n">rotation_ba</span><span class="p">,</span> <span class="n">translation_ba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">tgt_embedding</span><span class="p">,</span> <span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rotation_ba</span> <span class="o">=</span> <span class="n">rotation_ab</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">translation_ba</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rotation_ba</span><span class="p">,</span> <span class="n">translation_ab</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rotation_ab</span><span class="p">,</span> <span class="n">translation_ab</span><span class="p">,</span> <span class="n">rotation_ba</span><span class="p">,</span> <span class="n">translation_ba</span>
</code></pre></div><h3 id="Áî®‰∫éÊäΩË±°featureÁöÑmodule1emb_nn">Áî®‰∫éÊäΩË±°featureÁöÑModule1Ôºöemb_nn</h3>
<p>ËÄÉËôëemb_nnÔºåÊàë‰ª¨Êúâ‰∏§‰∏™ÈÄâÊã©Ôºö ÂÖ∂‰∏ÄÊòØPointNetÔºå ÂÖ∂‰∫åÊòØ<a class="link" href="https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"  target="_blank" rel="noopener"
    >DGCNN</a>.</p>
<p>PointNetÊäΩË±°ÁöÑÁâπÂæÅÊòØ<code>global feature</code>Ôºå ËÄåDGCNNÁªìÂêà‰∫Ü<code>local feature</code>Âíå<code>global feature</code>.</p>
<p>Êàë‰ª¨Â∏åÊúõÂæóÂà∞ÁöÑÊòØÂØπ<!-- raw HTML omitted --><strong>ÊØè‰∏Ä‰∏™ÁÇπÊäΩË±°ËÄåÂæóÁöÑÁâπÂæÅ</strong><!-- raw HTML omitted -->ÔºàÂç≥ÊØè‰∏Ä‰∏™ÁÇπÈÉΩÊúâÂÖ∂embedding)ÔºåÂπ∂Âà©Áî®‰∏§‰∏™ÁÇπ‰∫ë‰πãÈó¥ÁÇπÁöÑ<code>embedding</code>Êù•ÁîüÊàêÊò†Â∞ÑÂÖ≥Á≥ªÔºàÂç≥MatchÂÖ≥Á≥ªÔºâ. ÊâÄ‰ª•Êàë‰ª¨Ë¶ÅÂæóÂà∞ÁöÑÊòØ<code>per-point feature</code>ËÄå‰∏çÊòØ<code>one feature per cloud</code>„ÄÇ</p>
<p>Âá∫‰∫é‰∏äËø∞ÂéüÂõ†ÔºåÊàë‰ª¨Âú®ÊúÄÂêé‰∏ÄÂ±ÇÁöÑËÅöÂêàÂáΩÊï∞<code>aggregation function</code>‰πãÂâçÁîüÊàêÊØè‰∏™ÁÇπÁöÑ<code>representation</code>„ÄÇ
$$
F_X = {x_1^L,x_2^L, &hellip;, x_i^L,&hellip;,x_N^L}
$$
$$
F_Y = {y_1^L, y_2^L, &hellip;, y_i^L, &hellip;, y_N^L}
$$
‰∏äÊ†áL‰ª£Ë°®Á¨¨LÂ±ÇÁöÑËæìÂá∫ÔºàÂÅáÂÆöÂÖ±ÊúâLÂ±ÇÔºâ„ÄÇ</p>
<h4 id="pointnet">PointNet</h4>
<p>$x_i^l$ÊòØÁ¨¨$i$‰∏™ÁÇπÂú®Á¨¨$l$Â±ÇÂêéÁöÑ<code>embedding</code>ÔºåËÄå$h_{\theta}^l$ÊòØÁ¨¨$l$Â±ÇÁöÑÈùûÁ∫øÊÄßÊò†Â∞ÑÂáΩÊï∞„ÄÇPointNetÁöÑ<code>forward mechanism</code>ÂèØ‰ª•Áî®Â¶Ç‰∏ãÂÖ¨ÂºèÁªôÂá∫Ôºö
$$
x_i^l = h_{\theta}^l(x_i^{l-1})
$$
‰ΩúËÄÖ<code>@WangYue</code>Âú®github‰∏äÂÖ¨Â∏ÉÁöÑ‰ª£Á†ÅÔºå‰ΩøÁî®ÁöÑ<code>PointNet</code>ÁöÑÁΩëÁªúÊû∂ÊûÑÂ¶Ç‰∏ãÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">PointNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dims</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PointNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">emb_dims</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">emb_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div><p>‰ªé‰∏äËø∞‰ª£Á†Å‰∏≠ÔºåÂèØ‰ª•ÁúãÂá∫Ôºå<!-- raw HTML omitted --><strong>‰ΩúËÄÖ‰ΩøÁî®ÁöÑPointNetÂπ∂Ê≤°Êúâ<code>input-transform</code>Âíå<code>feature-transform</code>Ëøô‰∏§‰∏™Module</strong><!-- raw HTML omitted -->ÔºåÁõ∏ÂΩì‰∫éÂè™Â∫îÁî®MLP‰∏çÊñ≠ÂØπËæìÂÖ•ÁÇπ‰∫ëËøõË°åÊäΩË±°ÔºåÁõ¥Âà∞È´òÁª¥Á©∫Èó¥„ÄÇ
<!-- raw HTML omitted --></p>
<p><!-- raw HTML omitted --><em>Â≠òÁñë</em>Ôºö‰∏∫‰ªÄ‰πà‰∏çÂä†<code>Transform-Net</code>Ôºü Â¶ÇÊûúÂä†‰∏äÊïàÊûúËÆ≠ÁªÉÊïàÊûúÂ¶Ç‰Ωï? Ê≤°ÊúâcatÔºåcat‰πãÂêéÊïàÊûúÂ¶Ç‰ΩïÔºü<!-- raw HTML omitted --></p>
<h4 id="dgcnn">DGCNN</h4>
<p>DGCNNÊòØ‰ΩúËÄÖ<code>@WangYue</code>ÊèêÂá∫ÁöÑ‰∏ÄÁßçÁΩëÁªúÊû∂ÊûÑÔºåÂÖ∂ÁâπÁÇπÊòØ<code>EdgeConv</code>„ÄÇÂèØ‰ª•ÁªìÂêàÂÖ®Â±ÄÁâπÂæÅ‰∏éÂ±ÄÈÉ®ÁâπÂæÅ„ÄÇ
$$
x_i^l = f({{} h_{\theta}^l(x_i^{l-1},x_j^{l-1}); \forall j \in N_i {}})
$$
$f$ÊòØÊØè‰∏ÄÂ±ÇÂêéÁöÑËÅöÂêàÂáΩÊï∞„ÄÇ$N_i$ÊåáÁöÑÊòØÂíåÁÇπ$x_i$Â≠òÂú®KNNÂÖ≥Á≥ªÁöÑÁÇπÁöÑÈõÜÂêà„ÄÇ</p>
<p><code>get_graph_feature</code>ÊòØËøîÂõû<code>egde-feature</code>ÁöÑÂáΩÊï∞„ÄÇËøôÂπ∂‰∏çÊòØÊàë‰ª¨ÂÖ≥Ê≥®ÁöÑÈáçÁÇπÔºå‰ª£Á†ÅÁÆÄË¶ÅÁ≤òË¥¥‰∏Ä‰∏ã„ÄÇ</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">knn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">inner</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">pairwise_distance</span> <span class="o">=</span> <span class="o">-</span><span class="n">xx</span> <span class="o">-</span> <span class="n">inner</span> <span class="o">-</span> <span class="n">xx</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># (batch_size, num_points, k)</span>
    <span class="k">return</span> <span class="n">idx</span>


<span class="k">def</span> <span class="nf">get_graph_feature</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="c1"># x = x.squeeze()</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">knn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, num_points, k)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

    <span class="n">idx_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_points</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">idx_base</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span>
                    <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># (batch_size, num_points, num_dims)  -&gt; (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)</span>
    <span class="n">feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_points</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">feature</span> <span class="o">=</span> <span class="n">feature</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">feature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">feature</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">feature</span>
</code></pre></div><p>ËÄåÁΩëÁªú‰∏≠‰ΩøÁî®ÁöÑDGCNN‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DGCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dims</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DGCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">emb_dims</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">emb_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_dims</span><span class="p">,</span> <span class="n">num_points</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>    
        <span class="k">return</span> <span class="n">x</span>            <span class="c1"># (batch_size, emb_dims, num_points)</span>
</code></pre></div><p>ÂèØ‰ª•ÊòéÊòæÂèëÁé∞‰∏éÂéüDGCNN‰∏çÂêåÁöÑÂú∞ÊñπÊòØ: <!-- raw HTML omitted -->‰ΩúËÄÖËøôÈáåÊØèÊ¨°forwardÂâç‰º†Êó∂ÔºåÂπ∂Ê≤°ÊúâÂÜçÂØπÊäΩË±°Âá∫Êù•ÁöÑfeatureÂØªÊâæknnËøõË°åËøõ‰∏ÄÊ≠•ÊäΩË±°„ÄÇËÄåÊòØÂçïÁ∫ØÁöÑ‰∏çÊñ≠ÁªèËøáMLP„ÄÇ<!-- raw HTML omitted --> ÂØπÊØî‰∏Ä‰∏ãËØ•ÈÉ®ÂàÜÂéü‰ª£Á†ÅÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>      <span class="c1"># (batch_size, 3, num_points) --&gt; (batch_size, 3*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 3*2, num_points, k) --&gt; (batch_size, 64, num_points, k)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># (batch_size, 64, num_points, k) --&gt; (batch_size, 64, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>     <span class="c1"># (batch_size, 64, num_points) --&gt; (batch_size, 64*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 64*2, num_points, k) --&gt; (batch_size, 64, num_points, k)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># (batch_size, 64, num_points, k) --&gt; (batch_size, 64, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>     <span class="c1"># (batch_size, 64, num_points) --&gt; (batch_size, 64*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 64*2, num_points, k) --&gt; (batch_size, 128, num_points, k)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># (batch_size, 128, num_points, k) --&gt; (batch_size, 128, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">get_graph_feature</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>     <span class="c1"># (batch_size, 128, num_points) --&gt; (batch_size, 128*2, num_points, k)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># (batch_size, 128*2, num_points, k) --&gt; (batch_size, 256, num_points, k)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>       <span class="c1"># (batch_size, 256, num_points, k) --&gt; (batch_size, 256, num_points)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 64+64+128+256, num_points)</span>
</code></pre></div><p>Â∑ÆÂà´ÂçÅÂàÜÊòéÊòæ„ÄÇÁõ∏ÂΩì‰∫éÁΩëÁªúÁªìÊûÑ‰∏≠Á∫¢Ê°ÜÁöÑÈÉ®ÂàÜÊ∂àÂ§±‰∫ÜÔºö
<!-- raw HTML omitted --></p>
<p><!-- raw HTML omitted -->Ê≠§Â§ÑÂêåÊ†∑Â≠òÁñë<!-- raw HTML omitted -->Ôºå‰ΩúËÄÖÂú®ËÆ∫ÊñáÈáåÊú™ÊèêÂèäÊ≠§ÁªÜËäÇ„ÄÇ</p>
<h3 id="Áî®‰∫ématchÂØªÊâæÁÇπÂØπÂÖ≥Á≥ªÁöÑmodule2">Áî®‰∫éMatchÔºàÂØªÊâæÁÇπÂØπÂÖ≥Á≥ªÔºâÁöÑModule2</h3>
<h4 id="Âü∫‰∫éattentionÊú∫Âà∂ÁöÑtransformer">Âü∫‰∫éAttentionÊú∫Âà∂ÁöÑTransformer</h4>
<p>‰ΩøÁî®AttentionÊú∫Âà∂ÁöÑÂàùË°∑Âú®‰∫éÔºöÊàë‰ª¨ÊÉ≥ËÆ©ÈÖçÂáÜÂèòÂæóÊõ¥Âä†<code>task specify</code>„ÄÇ‰πüÂ∞±ÊòØËØ¥Ôºå‰∏çÂÜçÁã¨Á´ãÂú∞ÂÖ≥Ê≥®‰∏§‰∏™ËæìÂÖ•ÁÇπ‰∫ë$X$,$Y$ÁöÑ<code>embedding feature</code>ÔºåËÄåÊòØÂÖ≥Ê≥®$X$,$Y$‰πãÈó¥ÁöÑ‰∏Ä‰∫õËÅîÂêàÁâπÊÄß„ÄÇ‰∫éÊòØ‰πéÔºåËá™ÁÑ∂ËÄåÁÑ∂ÁöÑÊÉ≥Âà∞‰∫Ü<code>Attention</code>Êú∫Âà∂„ÄÇÂü∫‰∫é<code>attention</code>ÔºåËÆæËÆ°‰∏Ä‰∏™ÂèØ‰ª•ÊçïÊçâ<code>self-attention</code>Âíå<code>conditional attention</code>ÁöÑÊ®°ÂùóÔºåÁî®‰∫éÂ≠¶‰π†$X,Y$ÁÇπ‰∫ë‰πãÈó¥ÁöÑÊüê‰∫õËÅîÂêà‰ø°ÊÅØ„ÄÇ</p>
<p>Êàë‰ª¨Â∞ÜÁî±‰∏ä‰∏Ä‰∏™ModuleÂØπ‰∏§‰∏™ÁÇπ‰∫ëÂêÑËá™Áã¨Á´ãÁîüÊàêÁöÑembeddingÁâπÂæÅ$F_X,F_Y$‰Ωú‰∏∫ËæìÂÖ•ÔºåÈÇ£‰πàÂ∞±ÊúâÔºö
$$
\Phi_X = F_X + \phi(F_X,F_Y)
$$
$$
\Phi_Y = F_Y + \phi(F_Y,F_X)
$$
ÂÖ∂‰∏≠Ôºå$\phi$ÊòØTransformerÂ≠¶‰π†ÂæóÂà∞ÁöÑÊò†Â∞ÑÂáΩÊï∞:$\phi: R^{N \times P} \times R^{N \times P} \to R^{N \times P}$.</p>
<p>Êàë‰ª¨Â∞Ü$\phi$ÂΩìÂÅö‰∏Ä‰∏™ÊÆãÂ∑ÆÈ°πÔºå<!-- raw HTML omitted --><strong>Âü∫‰∫é$F_X,F_Y$ÁöÑËæìÂÖ•È°∫Â∫è</strong><!-- raw HTML omitted -->Ôºå‰∏∫$F_X,F_Y$Êèê‰æõ‰∏Ä‰∏™ÈôÑÂä†ÁöÑÊîπÂèòÈ°π„ÄÇ</p>
<blockquote>
<p>Notice we treat $\phi$ as a residual term, providing an additive change to $F_X$ and $F_Y$ depending on the order of its input.</p>
</blockquote>
<p>‰πãÊâÄ‰ª•ÈááÂèñÂ∞Ü$F_X \to \Phi_X$ÁöÑ<strong>Motivation</strong>Ôºö‰ª•‰∏ÄÁßçÈÄÇÂ∫î$Y$‰∏≠ÁÇπÁöÑÁªÑÁªáÁªìÊûÑÔºà‰∏™‰∫∫ÁêÜËß£Âç≥ËæìÂÖ•È°∫Â∫èÔºâÁöÑÊñπÂºèÊîπÂèò$X$ÁöÑ<code>Feature embedding</code>„ÄÇÂØπ$F_Y \to \Phi_Y$ÔºåÂä®Êú∫Áõ∏Âêå„ÄÇ</p>
<blockquote>
<p>The idea here is that the map $F_X \to \Phi_X$ modifies the features associated to the points in X in a fashion that is knowledgeable about the structure of $Y$.</p>
</blockquote>
<p>ÈÄâÊã©TransformerÊèê‰æõÁöÑÈùûÂØπÁß∞ÂáΩÊï∞‰Ωú‰∏∫$\phi$„ÄÇTransformerÁî±‰∏Ä‰∫õÂ†ÜÂè†ÁöÑ<code>encoder-decoder</code>ÁªÑÊàêÔºåÊòØ‰∏ÄÁßçËß£ÂÜ≥Seq2SeqÈóÆÈ¢òÁöÑÁªèÂÖ∏Êû∂ÊûÑ„ÄÇÂÖ≥‰∫éTransformerÁöÑÊõ¥Â§ö‰ø°ÊÅØÔºåÁßªÊ≠•ÊàëÁöÑÂè¶‰∏ÄÁØáÂçöÂÆ¢<a class="link" href="https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/"  target="_blank" rel="noopener"
    >„ÄäÂõæËß£TransformerÔºàËØëÔºâ„Äã</a></p>
<p>Ê≠§Module‰∏≠Ôºå<code>encoder</code>Êé•Êî∂$F_X$Âπ∂ÈÄöËøá<code>self-attention layer</code>Âíå<code>MLP</code>ÊääÂÆÉÁºñÁ†ÅÂà∞ÂÖ∂<code>embedding space</code>ÔºåËÄå<code>decoder</code>Êúâ‰∏§‰∏™ÈÉ®ÂàÜÁªÑÊàêÔºåÁ¨¨‰∏Ä‰∏™ÈÉ®ÂàÜÊé•Êî∂Âè¶‰∏Ä‰∏™ÈõÜÂêà$F_Y$, ÁÑ∂ÂêéÂÉè<code>encoder</code>‰∏ÄÊ†∑Â∞Ü‰πãÁºñÁ†ÅÂà∞<code>embedding space</code>„ÄÇÂè¶‰∏Ä‰∏™ÈÉ®ÂàÜ‰ΩøÁî®<code>co-attention</code>ÂØπ‰∏§‰∏™Â∑≤ÁªèÊò†Â∞ÑÂà∞<code>embedding space</code>ÁöÑÁÇπ‰∫ëËøõË°åÂ§ÑÁêÜ„ÄÇ ÊâÄ‰ª•ËæìÂá∫$\Phi_Y$,$\Phi_Y$Êó¢Âê´Êúâ$F_X$ÁöÑ‰ø°ÊÅØÔºåÂèàÂê´Êúâ$F_Y$ÁöÑ‰ø°ÊÅØ„ÄÇ</p>
<p>ËøôÈáåÁöÑ<strong>Motivation</strong>ÊòØÔºöÂ∞Ü‰∏§‰∏™ÁÇπ‰∫ë‰πãÈó¥ÁöÑÂåπÈÖçÂÖ≥Á≥ªÈóÆÈ¢ò(<code>match problem</code>)Á±ªÊØî‰∏∫Sq2SqÈóÆÈ¢ò„ÄÇÔºàÁÇπ‰∫ëÊòØÂú®‰∏§‰∏™ËæìÂÖ•ÁöÑÁÇπÁöÑÂ∫èÂàó‰∏≠ÂØªÊâæÂØπÂ∫îÂÖ≥Á≥ªÔºåËÄåSq2SqÈóÆÈ¢òÊòØÂú®ËæìÂÖ•Âè•Â≠ê‰∏≠ÂØªÊâæÂçïËØç‰πãÈó¥ÁöÑËÅîÁ≥ªÂÖ≥Á≥ªÔºâ„ÄÇ</p>
<p>‰∏∫‰∫ÜÈÅøÂÖç‰∏çÂèØÂæÆÂàÜÁöÑ<code>hard assignment</code>ÔºåÊàë‰ª¨‰ΩøÁî®Ê¶ÇÁéáËßíÂ∫¶ÁöÑ‰∏ÄÁßçÊñπÂºèÊù•ÁîüÊàê<code>soft map</code>ÔºåÂ∞Ü‰∏Ä‰∏™ÁÇπ‰∫ëÊò†Â∞ÑÂà∞Âè¶‰∏Ä‰∏™ÁÇπ‰∫ë„ÄÇÊâÄ‰ª•ÔºåÊØè‰∏Ä‰∏™$x_i \in X$ÈÉΩË¢´Ëµã‰∫à‰∫Ü‰∏Ä‰∏™Ê¶ÇÁéáÂêëÈáèÔºö
$$
m(x_i,Y) = softmax(\Phi_y \Phi_{x_i}^T)
$$
Âú®ËøôÈáåÔºå$\Phi_Y \in R^{N \times P}$‰ª£Ë°®YÁªèËøá<code>Attention Module</code>ÂêéÁîüÊàêÁöÑ<code>embedding</code>„ÄÇËÄå$\Phi_{x_i}$‰ª£Ë°®Áü©Èòµ$\Phi_X$ÁöÑÁ¨¨$i$Ë°å„ÄÇ ÊâÄ‰ª•ÂèØ‰ª•Â∞Ü$m(x_i, Y)$ ÁúãÂÅö‰∏Ä‰∏™Â∞ÜÊØè‰∏™$x_i$Êò†Â∞ÑÂà∞$Y$‰∏≠ÂÖÉÁ¥†ÁöÑ<code>soft pointer</code>„ÄÇ</p>
<p>‰∏ãÈù¢Êàë‰ª¨ÂÖ≥Ê≥®Êñá‰∏≠TransformerÁöÑÂÆûÁé∞„ÄÇÂÖ∂Êû∂ÊûÑ‰∏∫Ôºö
<!-- raw HTML omitted --></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">ff_dims</span>     <span class="c1"># Feed_forward Dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span>     <span class="c1"># Multihead AttentionÁöÑÂ§¥Êï∞</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">()</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">)</span>
        <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">),</span>
                                    <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">),</span>
                                    <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(),</span>
                                    <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(),</span>
                                    <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">())</span>
        
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>      <span class="c1"># batch_size, emb_dims, num_points</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>      <span class="c1"># batch_size, num_points, emb_dims</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">src_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">src_embedding</span><span class="p">,</span> <span class="n">tgt_embedding</span>
                
</code></pre></div><p>‰∏äËø∞‰ª£Á†ÅÊòØTransformerÁöÑÂÆûÁé∞„ÄÇÁúãËµ∑Êù•ÊúâÁÇπÁªïÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÂÖ≥Ê≥®ÂÖ∂forwardÂáΩÊï∞.</p>
<p>ÊµÅÂÖ•TransformerÁöÑdataÔºö <code>src,tgt: (batch_size, emb_dims, num_points)</code><br>
ÁªèËøátransopose.contiguous:  <code>src,tgt: (batch_size, num_points, emb_dims)</code><br>
ÈöèÂêéÂ∞Ü<code>src, tgt</code>‰º†ÂÖ•<code>self.model</code>ÔºåÂæóÂà∞‰∫Ü<code>tgt_embedding, src_embedding</code>.</p>
<p>ÂÖ≥Ê≥®<code>self.model</code>ÔºåÂú®<code>__init__</code>‰∏≠ÂÆö‰πâ‰∫Ü<code>self.model</code>:</p>
<pre><code>    self.model = EncoderDecoder(Encoder(EncoderLayer(self.emb_dims, c(attn), c(ff), self.dropout), self.N),
                                Decoder(DecoderLayer(self.emb_dims, c(attn), c(attn), c(ff), self.dropout),self.N),
                                nn.Sequential(),
                                nn.Sequential(),
                                nn.Sequential())
</code></pre><p><code>self.model</code> Êï¥‰ΩìÊòØ‰∏Ä‰∏™<code>EncoderDecoder</code>Á±ª„ÄÇÂÖ∂‰º†ÂÖ•ÁöÑÂèÇÊï∞Êúâ‰∏Ä‰∏™<code>Encoder</code>Ôºå‰∏Ä‰∏™<code>Decoder</code>Ôºå‰∏â‰∏™<code>Sequential</code>.</p>
<p>ËÄå<code>Encoder</code>‰º†ÂÖ•ÁöÑÂèÇÊï∞Êúâ‰∏§‰∏™ÔºåÁ¨¨‰∏Ä‰∏™ÊòØ<code>EncoderLayer</code>ÔºåÁ¨¨‰∫å‰∏™ÊòØ<code>self.N</code>„ÄÇ‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™ÂèÇÊï∞ÁöÑ<code>EncoderLayer</code>‰º†ÂÖ•‰∫ÜÂõõ‰∏™ÂèÇÊï∞ÔºåÂàÜÂà´ÊòØ<code>self.emb_dims</code>, <code>c(attn)</code>, <code>c(ff)</code>, <code>self.dropout</code>.</p>
<p><code>Decoder</code>‰º†ÂÖ•ÁöÑÂèÇÊï∞‰πüÊòØ‰∏§‰∏™ÔºåÁ¨¨‰∏Ä‰∏™ÊòØ<code>DecoderLayer</code>ÔºåÁ¨¨‰∫å‰∏™ÊòØ<code>self.N</code>„ÄÇ‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™ÂèÇÊï∞ÁöÑ<code>DecoderLayer</code>‰º†ÂÖ•‰∫Ü‰∫î‰∏™ÂèÇÊï∞ÔºåÂàÜÂà´ÊòØ<code>self.emb_dims</code>, <code>c(attn)</code>, <code>c(attn)</code>, <code>c(ff)</code>, <code>self.dropout</code>.</p>
<p>ËøôÈáåÁöÑ<code>c</code>ÊòØ<code>copy.deepcopy()</code>ÔºåÂç≥Ê∑±Êã∑Ë¥ù„ÄÇÂÆåÂÖ®Â§çÂà∂‰∏Ä‰∏™Êñ∞ÁöÑÂØπË±°ÔºåÊâÄ‰ª•Ëøô‰∫õÁΩëÁªú‰πãÈó¥ÂèÇÊï∞Âπ∂‰∏çÂÖ±‰∫´„ÄÇ</p>
<p>È¶ñÂÖàÊù•ÂÖ≥Ê≥®EncoderDecoderÁ±ªÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">src_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="c1"># Take in and process masked src and target sequences</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span>
</code></pre></div><p>‰ªé‰∏äËø∞‰ª£Á†ÅÂèØ‰ª•ÁúãÂá∫ÔºåEncoderDecoderÁ±ªÊûÑÈÄ†Êó∂‰º†ÂÖ•ÁöÑ‰∫î‰∏™ÂèÇÊï∞ÂàÜÂà´‰∏∫Ôºö <code>encoder, decoder, src_embed, tgt_embed, generator</code>.</p>
<p>ÂÖ∂Ââç‰º†Êú∫Âà∂<code>forward</code>ÊòØ<code>self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</code>„ÄÇÂç≥ÂÖàÂ∞Ü<code>src,src_mask</code>ËøõË°å<code>encode</code>,Â∞ÜÁºñÁ†ÅÂêéÁöÑÁªìÊûúÂêå<code>src_mask, tgt, tgt_mask</code>‰∏ÄÂêå<code>decode</code>„ÄÇ</p>
<p>ËÄå<code>encode</code>ÂáΩÊï∞ÔºåÊòØËøôÊ†∑ÂÆö‰πâÁöÑ: <code>self.encoder(self.src_embed(src), src_mask)</code>, Âç≥ÂÖàÂ∞Ü<code>src</code>ÈÄöËøá<code>src_embed</code>ÁΩëÁªúÔºåÁÑ∂ÂêéÊ†πÊçÆÂÖ∂<code>mask</code>ÂÜçÈÄöËøá<code>encoder</code>ÁΩëÁªú„ÄÇ</p>
<p>ËÄå<code>decode</code>ÂáΩÊï∞ÔºåÊòØÔºö<code>self.generator(self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask))</code>, ‰πüÂ∞±ÊòØËØ¥, <code>tgt</code>ÂÖàÁªèËøá<code>tgt_embed</code>ÁΩëÁªúÔºåÁÑ∂ÂêéÈöè<code>memory, src_mask, tgt_mask</code>‰∏ÄÂêå‰º†ÂÖ•<code>decoder</code>ÁΩëÁªúÔºå<code>decoder</code>ÁΩëÁªúÁöÑËæìÂá∫ÂÜçÊµÅÂÖ•<code>generator</code>ÁΩëÁªú„ÄÇ</p>
<p>ÊâÄ‰ª•ÊàëËá™Â∑±Ê¢≥ÁêÜ‰∫Ü‰∏Ä‰∏ãÊï¥‰∏™EncoderDecoderÂ§ßÊ¶ÇÁªìÊûÑÂ¶Ç‰∏ãÔºö</p>
<!-- raw HTML omitted -->
<p>Ëøõ‰∏ÄÊ≠•ÂÖ≥Ê≥®<code>Encoder</code>‰∏é<code>Decoder</code>Ôºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">clones</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)])</span>


<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div><p>‰ªé‰∏äËø∞‰ª£Á†ÅÔºåÂèØ‰ª•ÁúãÂá∫Ôºö <code>Encoder</code>Âú®ÊûÑÈÄ†Êó∂ÈúÄË¶Å‰º†ÂÖ•‰∏§‰∏™ÂèÇÊï∞Ôºå‰∏Ä‰∏™‰∏∫<code>layer</code>, ‰∏Ä‰∏™‰∏∫<code>N</code>„ÄÇËÄåÂú®ÊûÑÈÄ†ÂáΩÊï∞‰∏≠ÔºåÈÄöËøáË∞ÉÁî®<code>clones</code>ÊñπÊ≥ïÂ∞Ü‰º†ÂÖ•ÁöÑ<code>layer</code>Ê∑±Â§çÂà∂(<code>deepcopy</code>)‰∫ÜNÊ¨°ÔºåÂπ∂‰Ωú‰∏∫‰∏Ä‰∏™<code>ModuleList</code>Â≠òÂÇ®Âú®<code>self.layers</code>ÊàêÂëòÂèòÈáè‰∏≠„ÄÇ</p>
<p><code>clones</code>ÊñπÊ≥ïÁöÑÊâßË°åÊïàÊûúÂèØ‰ªé‰∏ã‰æã‰∏≠Á™•ËßÅÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;net&#39;</span><span class="p">,</span><span class="n">net</span><span class="p">)</span>
<span class="n">net_clones</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;net_clones&#39;</span><span class="p">,</span> <span class="n">net_clones</span><span class="p">)</span>
</code></pre></div><p>ÂÖ∂ÊâßË°åÁªìÊûúÊòØÔºöÂ∞Ü<code>net</code>Â§çÂà∂‰∫Ü3Ê¨°ÔºåË£ÖÂú®‰∏Ä‰∏™<code>ModuleList</code>‰∏≠ËøîÂõû„ÄÇÂπ∂‰∏îÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåËøôÈáåÊòØ<strong>copy.deepcopy()</strong>ÔºåÊ∑±Â§çÂà∂ÔºåÊâÄ‰ª•ÂèÇÊï∞‰πãÈó¥‰∏çÂÖ±‰∫´„ÄÇ</p>
<!-- raw HTML omitted -->
<p>ËÄåLayerNormÂÆö‰πâÂ¶Ç‰∏ã:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>         <span class="c1"># ÊúÄÂêé‰∏ÄÁª¥ÁöÑÂùáÂÄº</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>           <span class="c1"># ÊúÄÂêé‰∏ÄÁª¥ÁöÑÊ†áÂáÜÂ∑Æ</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_2</span>      <span class="c1"># ÂØπÊúÄÂêé‰∏ÄÁª¥ËøõË°åNorm</span>
</code></pre></div><p>ÂèØËßÅLayerNormÁöÑ‰ΩúÁî®ÊòØÂØπËæìÂÖ•ÁöÑÊï∞ÊçÆ<code>x</code>ÔºåÂØπÂÖ∂ÊúÄÂêé‰∏ÄÁª¥ËøõË°åÂΩí‰∏ÄÂåñÊìç‰Ωú„ÄÇ</p>
<p>ËÄåEncoderÊï¥‰∏™ÁöÑÂâç‰º†Êú∫Âà∂‰∏∫ÔºöÂØπ‰∫éÊûÑÈÄ†Êó∂ÁîüÊàêÁöÑModuleList,‰æùÊ¨°Â∞Ü<code>x</code>ÈÄöËøá<code>ModuleList</code>‰∏≠ÁöÑÊØè‰∏™ÁΩëÁªú<code>layer</code>ÔºåÂç≥<code>x = layer(x, mask)</code>, ÁÑ∂ÂêéÂÜçÂ∞ÜËæìÂá∫ÈÄöËøáLayerNormÂØπÊúÄÂêé‰∏ÄÁª¥ËøõË°åÂΩí‰∏ÄÂåñÊìç‰ΩúËøîÂõû„ÄÇ EncoderÁöÑÁΩëÁªúÁªìÊûÑÂõæÂ¶Ç‰∏ãÔºö</p>
<!-- raw HTML omitted -->
<p>Ëøõ‰∏ÄÊ≠•ÂÖ≥Ê≥®Ôºå<code>Encoder</code>Âú®<code>EncoderDecoder</code>ÊûÑÈÄ†Êó∂Ôºå‰º†ÂÖ•ÁöÑlayerÂèÇÊï∞‰∏∫Ôºö<code>EncoderLayer(self.emb_dims, c(attn), c(ff), self.dropout)</code>„ÄÇ Ëß£ËØª<code>EncoderLayer</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">)</span>
</code></pre></div><p><code>EncoderLayer</code>Âú®ÊûÑÈÄ†Êó∂ÈúÄË¶Å‰º†ÂÖ•ÁöÑÂèÇÊï∞ÊúâÔºö<code>size, self_attn, feed_forward, dropout</code>„ÄÇ ÂÖ∂‰∏≠<code>self_attn, feed_forward</code>‰∏§‰∏™ÁΩëÁªúÔºå‰ª•Âèä<code>size</code>‰Ωú‰∏∫ÊàêÂëòÂèòÈáèÂ≠òÂÇ®„ÄÇËÄåÂè¶‰∏Ä‰∏™ÊàêÂëòÂèòÈáè<code>sublayer</code>ÈÄöËøá<code>clones</code>ÊñπÊ≥ïÂ∞Ü<code>SublayerConnection(size, dropout)</code>Â§çÂà∂‰∏§ÈÅçÔºåÂ≠òÁùÄ‰∏Ä‰∏™ModuleList.</p>
<p>ËßÇÂØü<code>SublayerConnection</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SublayerConnection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div><p><code>SublayerConnection</code>Âú®ÊûÑÈÄ†Êó∂ÔºåÂè™‰º†ÂÖ•‰∫Ü<code>size, dropout</code>‰∏§‰∏™ÂèÇÊï∞ÔºåÁî®‰∫éÊûÑÈÄ†LayerNorm„ÄÇËÄåÂâç‰º†<code>forward</code>ÁöÑÊó∂ÂÄôÔºåËøîÂõûÁöÑÊòØ<code>x + sublayer(self.norm(x))</code>Ôºå Âç≥Â∞Ü<code>x</code>ÈÄöËøá‰∫Ü<code>LayerNorm</code>ÂêéÔºåÂÜçÈÄöËøá‰Ωú‰∏∫ÂèÇÊï∞‰º†ÂÖ•ÁöÑÁΩëÁªú<code>sublayer</code>ÔºåÊúÄÂêé‰∏é<code>x</code>Áõ∏Âä†ÔºåËøîÂõû„ÄÇ</p>
<p>ÊêûÊ∏ÖÊ•ö<code>SublayerConnection</code>ÁöÑÊú∫Âà∂ÂêéÔºåÊàë‰ª¨ÂõûÁúã<code>EncoderLayer</code>ÁöÑ<code>forward</code>Êú∫Âà∂Ôºö<code>x</code>ÂÖàÈÄöËøá‰∏Ä‰∏™‰º†ÂÖ•ÁΩëÁªú‰∏∫<code>attn</code>ÁöÑ<code>sublayer</code>ÔºåÁÑ∂ÂêéÂÜçÈÄöËøá‰∏Ä‰∏™‰º†ÂÖ•ÁΩëÁªú‰∏∫<code>feedforward</code>ÁöÑ<code>sublayer</code>. <code>EncoderLayer</code>ÁΩëÁªúÁªìÊûÑÂ¶ÇÂõæÔºö</p>
<!-- raw HTML omitted -->
<p>Êé•‰∏ãÊù•ÂÖ≥Ê≥®<code>attn</code>ÔºåÂç≥<code>MultiHeadedAttention</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>   <span class="c1"># (nbatches, h, num_points, num_points)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (nbatches, h, num_points, num_points)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>


<span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="c1"># h: number of heads ;  d_model: dims of model(emb_dims)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># we assume d_v always equals d_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span>         <span class="c1"># d_k ÊòØÊØè‰∏™headÁöÑdim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Same mask applied to all h heads</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))]</span>      <span class="c1"># (nbatches, h, num_points, d_k)</span>

        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span>
        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># 3) &#34;Concat&#34; using a view and apply a final linear.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</code></pre></div><p>MultiHeadAttentionÂÖ∂ÊûÑÈÄ†ÂáΩÊï∞Êúâ‰∏§‰∏™ÂèÇÊï∞Ôºö<code>h, d_model</code>. ÂÖ∂‰∏≠<code>h</code>ÊòØ<code>head</code>ÁöÑ‰∏™Êï∞ÔºåËÄå<code>d_model</code>ÂÆûÈôÖ‰∏äÂ∞±ÊòØ<code>emb_dims</code>„ÄÇÊàë‰ª¨ÊÄªÊòØËßÑÂÆö<code>emb_dims</code>ÊòØÂèØ‰ª•Êï¥Èô§<code>h</code>ÁöÑÔºåÂê¶ÂàôÊØè‰∏™<code>attention</code>ÁöÑÁª¥Â∫¶‰∏çÊòØÊï¥Êï∞„ÄÇ<code>d_k = d_model // h</code>Âç≥ÊòØÊØè‰∏™<code>head</code>ÁöÑÁª¥Â∫¶„ÄÇÂêåÊó∂Âú®ÊûÑÈÄ†Êó∂ÔºåÂú®self.linears‰∏≠Â≠òÂÇ®‰∫Ü‰∏Ä‰∏™<code>ModuleList</code>,<code>ModuleList</code>‰∏≠ÊúâÂõõ‰∏™<code>Linear</code>Á∫øÊÄßÊò†Â∞Ñ<code>d_model --&gt; d_model</code>„ÄÇ</p>
<p>MultiHeadAttentionÁöÑÂâç‰º†Êú∫Âà∂Ôºö</p>
<p>(1) ÈÄöËøáÁ∫øÊÄßÊò†Â∞Ñ<code>linear projection</code>ÔºåÁîüÊàê<code>query, key, value</code>.</p>
<p>Áî®<code>zip</code>ÊñπÊ≥ïÂ∞Ü‰∏â‰∏™Á∫øÊÄßÊò†Â∞ÑÁªëÂÆöÂà∞<code>query, key, value</code>‰∏äÔºåÁõ∏ÂΩì‰∫éÊåáÂÆö‰∫ÜÂÖ∂ÁîüÊàêÁöÑÁü©Èòµ„ÄÇÔºàËøôÈáåÁöÑ<code>query, key, value</code>Âè™ÊòØÁî®‰∫éÁîüÊàê<code>query, key, value</code>ÁöÑÂéüÂßãÊï∞ÊçÆÔºå‰∫ãÂÆû‰∏äÈÉΩÊòØ<code>x</code>Ôºâ„ÄÇ Â∞Ü<code>query, key, value</code>ÂàÜÂà´ÈÄöËøáÂØπÂ∫îÁöÑ<code>Linear Projection</code>ÊäïÂΩ±ÁîüÊàêÁúüÊ≠£ÁöÑ<code>query, key, value</code>„ÄÇ ËæìÂÖ•ÁöÑ<code>x</code>ÁöÑ<code>shape</code>ÊòØ<code>nbatches, num_points, emb_dims</code> (<code>emb_dim</code> Âç≥ <code>d_model</code>). ÁªèËøáÂØπÂ∫îÁöÑ<code>Linear Projection</code>ÂêéÔºåÁîüÊàêÁöÑ<code>shape</code>ÊòØ<code>nbatches, num_points, emb_dims</code>, ÈÄöËøá<code>view()</code>Âèò‰∏∫<code>nbatches, num_points, self.h, self.d_k</code>„ÄÇ ÈöèÂêéÂèàËøõË°å‰∫Ü<code>transpose(1,2).contiguous()</code>, ÈÇ£‰πàÊúÄÂêéÁîüÊàêÁöÑ<code>query, key, value</code>ÁöÑ<code>shape</code>ÊòØ<code>nbatches, self.h, num_points, self.d_k</code>.</p>
<p>ÈúÄË¶ÅËØ¥ÊòéÁöÑÊòØÔºåÊåâÁÖßTransformerÁöÑÁêÜËÆ∫ÔºåMultiHeadAttentionÁöÑÁîüÊàêÁü©Èòµ(Âç≥Êàë‰ª¨Âú®‰∏äÈù¢Áî®ÁöÑÊäïÂΩ±Â∫îËØ•ÊòØÊØè‰∏™headÊúâ‰∏Ä‰∏™ÂçïÁã¨ÁöÑprojection)Ôºå‰ΩÜÊòØÂõ†‰∏∫Ëøô‰∏™projectionÊòØÂ≠¶‰π†ÂæóÂà∞ÁöÑÔºåÊâÄ‰ª•Êàë‰ª¨Âè™Áî®‰∏Ä‰∏™<code>projection</code>ÁÑ∂ÂêéÂÜçËøõË°å<code>view()</code>ÂàÜÂâ≤ÂæóÂà∞<code>MultiHead</code>ÔºåÂú®ÁêÜËÆ∫‰∏äÂ∫îËØ•ËÉΩÂæóÂà∞Áõ∏ÂêåÁöÑÊïàÊûú„ÄÇ</p>
<p>(2) Ê†πÊçÆÂæóÂà∞ÁöÑ<code>query, key, value</code>ËÆ°ÁÆóSelf-Attention.</p>
<p>Self-AttentionÁöÑËÆ°ÁÆóÔºö $softmax(\frac{Q \times K^T}{\sqrt{d_k}}) V$</p>
<!-- raw HTML omitted -->
<p>ËøîÂõûÁöÑ<code>z</code>ÁöÑshape‰∏∫Ôºö<code>nbatches, self.h, num_points, self.d_k</code></p>
<p>(3) ÈÄöËøá<code>view</code>ËøõË°åÊâÄË∞ìÁöÑ<code>Concatenate</code>ÔºåÂ∞Ü‰πãÂ∫îÁî®‰∫é<code>Linear</code>ÁΩëÁªúÔºåËæìÂá∫„ÄÇ</p>
<p>È¶ñÂÖàËøõË°å‰∏Ä‰∏™<code>transpose(1,2)</code>,ÈöèÂêéÊîπÂèòÂÖ∂ÂÜÖÂ≠òÂàÜÂ∏É<code>contiguous</code>ÔºåÁÑ∂ÂêéÂÜçÈÄöËøá<code>view()</code>ÔºåÁõ∏ÂΩì‰∫éÊääÂ§ö‰∏™Â§¥ÁöÑ<code>attention</code>ÊãºÊé•Ëµ∑Êù•„ÄÇÊ≠§Êó∂ÁöÑshape‰∏∫Ôºö<code>nbatches, num_points, h * d_k</code>.</p>
<p>ÂÜçÂ∫îÁî®‰∫éÁ¨¨Âõõ‰∏™<code>Linear</code>‰∏äÔºåËæìÂá∫ÁöÑshape: (<code>nbatches, num_points, d_model</code>)</p>
<p>Êï¥‰∏™<code>Attention</code>ÁöÑÁΩëÁªúÁªìÊûÑÂ¶Ç‰∏ãÔºö
<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>‰∏ãÈù¢ÂÖ≥Ê≥®<code>PositionwiseFeedForward</code>ÁΩëÁªúÁªìÊûÑÔºö</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div><p><code>PositionwistFeedForward</code>ÁöÑÁΩëÁªúÁªìÊûÑÊØîËæÉÁÆÄÂçïÔºå‰∏çÂÜçÂçïÁã¨ÂàÜÊûê„ÄÇ
Êï¥‰∏™<code>Encoder</code>ÁöÑÁªìÊûÑÂàÜÊûêÂÆåÊØï„ÄÇ ‰∏ãÈù¢ÂÜçÂÖ≥Ê≥®‰∏Ä‰∏ã<code>Decoder</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Decoder is made of self-attn, src-attn, and feed forward(defined below)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">tgt_mask</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">src_mask</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
</code></pre></div><p>DecoderÁöÑÁΩëÁªúÁªìÊûÑÂ¶ÇÂõæÔºö</p>
<!-- raw HTML omitted -->
<p>ËÄåDecoderLayerÔºåÊé•Êî∂ÁöÑÂèòÈáèÊúâ<code>x</code>,<code>memory</code>,<code>src_mask</code>,<code>tgt_mask</code>ÔºåÁõ∏ÂΩì‰∫éÂÖàËÆ°ÁÆó<code>self-attention</code>,ÂÜçËÆ°ÁÆó<code>co-attention</code>ÔºåÊúÄÂêé<code>feed-forward</code>ÔºåÂÖ∂ÁΩëÁªúÁªìÊûÑ‰∏∫Ôºö
<!-- raw HTML omitted --></p>
<p>Ëá≥Ê≠§TransformerÂ∫îËØ•Â∑≤ÁªèÂàÜÊûêÊ∏ÖÊ•ö‰∫Ü„ÄÇ</p>
<h3 id="Áî®‰∫ésvdÊ±ÇËß£ÁöÑÊ®°Âùómodule3">Áî®‰∫éSVDÊ±ÇËß£ÁöÑÊ®°ÂùóModule3</h3>
<p>Êàë‰ª¨ÁöÑÊúÄÁªàÁõÆÁöÑÊòØÊ±ÇÂá∫Âàö‰ΩìÂèòÊç¢Áü©Èòµ„ÄÇ‰ΩøÁî®‰∏ä‰∏Ä‰∏™<code>Module</code>‰∏≠ËÆ°ÁÆóÂá∫ÁöÑ<code>soft pointer</code>ÔºåÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™Âπ≥ÂùáÊÑè‰πâ‰∏äÁöÑ<code>match point</code>„ÄÇ
$$
\hat{y_i} = (Y_m)^T m(x_i,Y)
$$
ËøôÈáåÔºå$Y_m$ÊòØÊåá‰∏Ä‰∏™$R^{N \times 3}$ÁöÑÁü©ÈòµÔºåÂåÖÂê´ÁùÄ$Y$‰∏≠ÊâÄÊúâÁÇπÁöÑ‰ø°ÊÅØ„ÄÇ Ê†πÊçÆËøôÊ†∑‰∏ÄÁßç<code>match</code>ÂÖ≥Á≥ªÔºå‰æøÂèØ‰ª•ÈÄöËøá<code>SVD</code>Ê±ÇËß£Âá∫Âàö‰ΩìÂèòÊç¢Áü©Èòµ„ÄÇ</p>
<p>‰∏∫‰∫ÜËÉΩÂ§ü‰ΩøÂæóÊ¢ØÂ∫¶ÂèçÂêë‰º†Êí≠ÔºåÊàë‰ª¨ÂøÖÈ°ªÂØπSVDËøõË°åÊ±ÇÂØºÔºåÁî±‰∫éÊàë‰ª¨‰ΩøÁî®SVDÊ±ÇËß£ÁöÑÂè™ÊòØ3x3ÔºåÂèØ‰ª•‰ΩøÁî®ÂÖ∂ÂØºÊï∞ÁöÑËøë‰ººÂΩ¢Âºè„ÄÇSVDÊ±ÇÂØºÁöÑËØ¶ÁªÜËØ∑ÂèÇËÄÉËÆ∫Êñá:</p>
<blockquote>
<p>Estimating the jacobian of the singular value decomposition: Theory and applications</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SVDHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SVDHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dims</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">emb_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reflect</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reflect</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">src_embedding</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tgt_embedding</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">src</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">d_k</span> <span class="o">=</span> <span class="n">src_embedding</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">src_embedding</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">tgt_embedding</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">src_corr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

        <span class="n">src_centered</span> <span class="o">=</span> <span class="n">src</span> <span class="o">-</span> <span class="n">src_mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">src_corr_centered</span> <span class="o">=</span> <span class="n">src_corr</span> <span class="o">-</span> <span class="n">src_corr</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">src_centered</span><span class="p">,</span> <span class="n">src_corr_centered</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

        <span class="n">U</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="p">[],[],[]</span>
        <span class="n">R</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
            <span class="n">r_det</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">r_det</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reflect</span><span class="p">)</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

            <span class="n">R</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

            <span class="n">U</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">S</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">V</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="o">-</span><span class="n">R</span><span class="p">,</span> <span class="n">src</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">))</span> <span class="o">+</span> <span class="n">src_corr</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">R</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
</code></pre></div><p><!-- raw HTML omitted -->ÁñëÊÉë<!-- raw HTML omitted -->Ôºö Ê≤°Âú®Ê∫êÁ†Å‰∏≠ÁúãÂà∞ÊúâÈíàÂØπSVDÊ±ÇÂØºÁöÑ‰ºòÂåñ„ÄÇ</p>
<h3 id="ÊçüÂ§±ÂáΩÊï∞">ÊçüÂ§±ÂáΩÊï∞</h3>
<p>$$
Loss = ||R_{XY}^TR_{XY}^g - I||^2 + ||t_{XY} - t_{XY}^g||^2 + \lambda||\theta||^2
$$</p>
<h3 id="ÂÖ≥‰∫édcp_v1Âíådcp_v2">ÂÖ≥‰∫éDCP_v1ÂíåDCP_v2</h3>
<p>DCP_v1 Ê≤°ÊúâÂ∫îÁî®<code>Attention</code>Êú∫Âà∂„ÄÇ</p>
<p>DCP_v2 Â∫îÁî®‰∫Ü<code>Attention</code>Êú∫Âà∂„ÄÇ</p>
<h2 id="Ê∑±ÂÖ•Êé¢Á©∂">Ê∑±ÂÖ•Êé¢Á©∂</h2>
<h3 id="ÂÖ≥‰∫éÁâπÂæÅÊèêÂèñÈÄâÊã©pointnetËøòÊòØdgcnn">ÂÖ≥‰∫éÁâπÂæÅÊèêÂèñÔºöÈÄâÊã©PointNetËøòÊòØDGCNN?</h3>
<p>PointNet Â≠¶‰π†ÁöÑÊòØÂÖ®Â±ÄÁâπÂæÅÔºå ËÄåDGCNNÈÄöËøáÊûÑÂª∫<code>k-NN</code> GraphÂ≠¶‰π†Âà∞ÁöÑÊòØÂ±ÄÈÉ®ÈõÜÂêàÁâπÂæÅ„ÄÇ</p>
<p>‰ªéÊñá‰∏≠ÁöÑÂÆûÈ™åÁªìÊûúÂèØ‰ª•ÁúãÂá∫Ôºå‰ΩøÁî®DGCNN‰Ωú‰∏∫emb_netÔºåÊØîPointNetÁöÑÊÄßËÉΩÂßãÁªàË¶ÅÂ•Ω„ÄÇ
<!-- raw HTML omitted --></p>
<h3 id="ÂÖ≥‰∫éËÆ°ÁÆóÂàö‰ΩìÂèòÊç¢ÈÄâÊã©mlpËøòÊòØsvd">ÂÖ≥‰∫éËÆ°ÁÆóÂàö‰ΩìÂèòÊç¢ÔºöÈÄâÊã©MLPËøòÊòØSVDÔºü</h3>
<p>MLPÂú®ÁêÜËÆ∫‰∏äÂèØ‰ª•Ê®°Êãü‰ªª‰ΩïÈùûÁ∫øÊÄßÊò†Â∞Ñ„ÄÇ ËÄåSVDÊòØÈíàÂØπ‰ªªÂä°ËøõË°åÊúâÁõÆÁöÑÊÄßËÆæËÆ°ÁöÑÁΩëÁªú„ÄÇ</p>
<p>‰ªéÊñá‰∏≠ÁöÑÂÆûÈ™åÁªìÊûúÂèØ‰ª•ÁúãÂá∫Ôºå‰ΩøÁî®SVDËÆ°ÁÆó<code>rigid transformation</code>ÊÄªÊòØÊõ¥‰ºò„ÄÇ
<!-- raw HTML omitted --></p>
<h1 id="ÁªìËØ≠">ÁªìËØ≠</h1>
<p>‰ª•‰∏äÂ∞±ÊòØÊàë‰∏™‰∫∫ÂØπDCPÁöÑÁ¨îËÆ∞ËÆ∞ÂΩï‰ª•Âèä‰∏Ä‰∫õËß£ËØª„ÄÇ</p>
<p>Â¶ÇÊûúËßâÂæóÊñáÁ´†ÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÊ¨¢ËøéÁÇπËµûÁïôË®Ä‰∫§ÊµÅ„ÄÇÁªô‰ΩúËÄÖ‰π∞ÊùØÂíñÂï°Â∞±Êõ¥ÊÑüÊøÄ‰∏çËøá‰∫ÜÔºÅ
<!-- raw HTML omitted --></p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/%E7%82%B9%E4%BA%91%E9%85%8D%E5%87%86/">ÁÇπ‰∫ëÈÖçÂáÜ</a>
        
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">Ê∑±Â∫¶Â≠¶‰π†</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.querySelector(`.article-content`));"></script>
<script>
var katex_config = {
    delimiters: 
    [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false}
    ]
};
</script>
<script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" onload="renderMathInElement(document.body,katex_config)"></script>

    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="">
    <a href="/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">
        
        

        <div class="article-details">
            <h2 class="article-title">DGCNNËÆ∫ÊñáËß£ËØª</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>


    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "codefmeister" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2020 Codefmeister
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="1.1.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true" style="display:none">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
            </main>
        </div>
        <script src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"
    integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin="anonymous"></script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<link rel="stylesheet" href="/css/highlight/light.min.css" media="(prefers-color-scheme: light)">
<link rel="stylesheet" href="/css/highlight/dark.min.css" media="(prefers-color-scheme: dark)">

    </body>
</html>
