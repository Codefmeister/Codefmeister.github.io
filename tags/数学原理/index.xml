<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数学原理 on Codefmeister</title>
    <link>https://codefmeister.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</link>
    <description>Recent content in 数学原理 on Codefmeister</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://codefmeister.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Stochastic Matrix, Doubly Stochastic Matrix, Permutation Matrix</title>
      <link>https://codefmeister.github.io/p/stochastic-matrix-doubly-stochastic-matrix-permutation-matrix/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/stochastic-matrix-doubly-stochastic-matrix-permutation-matrix/</guid>
      <description>Stochastic Matrix  Definition:
In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain. Each of its entries is a nonnegative real number representing a probability. It is also called a probability matrix, transition matrix, substitution matrix, or Markov matrix. There are several different definitions and types of stochastic matrices
A right stochastic matrix is a real square matrix, with each row summing to 1.</description>
    </item>
    
    <item>
      <title>Conv1d与Conv2d</title>
      <link>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</link>
      <pubDate>Wed, 16 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</guid>
      <description>Conv1d与Conv2d 本文分为几个部分来详解Conv2d与Conv1d。主要侧重于Conv2d
前言 本文记于2020年12月15日，起因是DGCNN中部分卷积使用了二维卷积，部分卷积使用了一维卷积。加之之前对Conv2d与Conv1d属于一种迷迷糊糊的状态，趁着这个机会弄清楚。
Conv2d原理（二维卷积层） 二维互相关运算 互相关运算与卷积运算 虽然卷积层得名于卷积(convolution)运算，但所有框架在实现卷积层的底层，都采用的是互相关运算。实际上，卷积运算与互相关运算类似，为了得到卷积运算的输出，我们只需要将核数组左右翻转并上下翻转，然后再与输入数组做互相关运算。所以这两种运算虽然类似，但是输出并不相同。
但是由于深度学习中核数组都是学习得到的，所以卷积层无论使用互相关运算还是卷积运算，都不影响模型预测时的输出。也就是说我们用卷积运算学出的核数组与用互相关运算学出的核数组两者之间可以通过上下翻转，左右翻转来相互转换。所以在框架乃至于绝大部分深度学习文献中，都使用互相关运算来代替了卷积运算。
互相关运算 在二维卷积层中，一个二维输入数组和一个二维核(kernel)数组通过互相关运算输出一个二维数组。举个例子来解释二维互相关运算：
假设输入数组的高和宽均为3， 核数组的高和宽均为2，该数组在卷积运算中又称为卷积核或者过滤器(filter)。 19是这样得出的： $19 = 0\times0 + 1\times1 + 3\times2 + 4\times3$ 。
卷积窗口从输入数组的最左上方开始，按照从左往右，从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和。得到输出数组中对应位置的元素。
二维卷积层 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包含了卷积核和标量偏差。我们在训练模型的时候，通常先对卷积层进行随机的初始化，然后不断迭代卷积核和偏差。
卷积窗口形状为$p \times q$的卷积层称为$p \times q$卷积层。
特征图与感受野 二维卷积层输出的二维数组可以看做是输入在空间维度上(宽和高)上某一级的表征，也叫特征图(feature map)。影响元素$x$的前向计算的所有可能输入区域(甚至可能大于输入的实际尺寸)叫做$x$的感受野(receptive field)。以上图为例，图中输入的阴影部分的四个元素就是输出数组中阴影部分元素的感受野。如果我们将该输出再和一个$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么$z$在输入上的感受野包含全部的9个元素。
可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。
填充与步幅 卷积层的输出形状由输入形状和卷积核窗口形状决定，通过填充与步幅，我们可以改变给定形状的输入和卷积层下的输出形状。
填充 填充padding是指在输入高和宽的两侧填充元素(通常是0元素)。如下图： 假设输入形状为$n_h \times n_w$， 卷积核窗口形状是$k_h \times k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是： $$ (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1) $$ 很多情况下我们会设置$p_h = k_h -1$和$p_w = k_w - 1$来使得输入输出具有相同的高和宽。</description>
    </item>
    
    <item>
      <title>MatrixCookBook Chapter1:矩阵的基础知识</title>
      <link>https://codefmeister.github.io/p/matrixcookbook-chapter1%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Sun, 15 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/matrixcookbook-chapter1%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid>
      <description>矩阵的基础知识（转置，逆，迹，行列式）  References: MatrixCookBook(Version 2012) Chapter1
 Chapter1: Basics 1 Basics 注：${A^H}$是A的Transposed and complex conjugated matrix (Hermitian)，即转置复共轭矩阵。
1.1 矩阵的迹(Trace) 式子(11)表明矩阵的迹是主对角线元素的和。
式子(12)表明矩阵的迹是矩阵的特征值的和。
式子(13)表明矩阵的迹等于其转置矩阵的迹。
式子(14)表明AB的迹等于BA的迹。
式子(15)表明A+B的迹等于A的迹加B的迹。 式子(16)表明ABC的迹等于BCA的迹等于CAB的迹。
式子(17)表明一个nx1的向量a，a的转置乘以a所得的常数等于a乘以a的转置所得矩阵的迹。
1.2 行列式(Determinant) 前提：此处的A是nxn矩阵。
式子(18)表明矩阵的行列式等于特征值的连乘积。
式子(19)表明cA的行列式等于A的行列式的${c^n}$倍。
式子(20)表明矩阵的行列式等于其转置矩阵的行列式。
式子(21)表明矩阵AB的行列式等于矩阵A的行列式乘以矩阵B的行列式。
式子(22)表明矩阵${A^{-1}}$的行列式等于矩阵A的倒数。
式子(23)表明矩阵${A^n}$的行列式等于矩阵A的行列式的n次幂。
式子(24)表明如果u和v是nx1向量，那么${I+uv^T}$的行列式等于${1+u^Tv}$的值。
式子(25)表明如果A是2x2矩阵，I+A的行列式等于${1+det(A)+Tr(A)}$,即1+A的行列式+A的迹。
式子(26)表明如果A是3x3矩阵，I+A的行列式等于${1+det(A)+Tr(A)+\frac{1}{2}Tr(A)^2-\frac{1}{2}Tr(A^2)}$。
式子(27)不表。
式子(28)表示对于微小扰动$\varepsilon$，可以将$\varepsilon A$近似作为2x2形式处理：
1.3 特例：2x2矩阵 2x2矩阵有着以上的性质与结论。</description>
    </item>
    
  </channel>
</rss>
