<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>torch on Codefmeister</title>
    <link>https://codefmeister.github.io/tags/torch/</link>
    <description>Recent content in torch on Codefmeister</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://codefmeister.github.io/tags/torch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>torch.nn.parameter.Parameter分析</title>
      <link>https://codefmeister.github.io/p/torch.nn.parameter.parameter%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.nn.parameter.parameter%E5%88%86%E6%9E%90/</guid>
      <description>torch.nn.parameter.Parameter 作用  a kind of Tensor that is to be considered a module parameter.
 Parameter是一种可以作为模型参数的Tensor.
 Parameters are Tensor subclasses, that have a very special property when used with Module S &amp;mdash;-when they&amp;rsquo;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn&amp;rsquo;t have such effect.
 Parameter是Tensor的子类，同时拥有一种非常特殊的性质：当他们与Module S一起使用时，也就是说当它们作为Module参数进行使用时，它们会自动添加到Module的参数列表中，并且出现在parameters()迭代器里。(这样就可以自动计算梯度等)
构造参数  data(Tensor)&amp;ndash; parameter tensor requires_grad(bool, optional)&amp;ndash; if the parameter requires gradient.</description>
    </item>
    
    <item>
      <title>torch.unsqueeze()解读</title>
      <link>https://codefmeister.github.io/p/torch.unsqueeze%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/torch.unsqueeze%E8%A7%A3%E8%AF%BB/</guid>
      <description>torch.unsqueeze()函数解读 语法 torch.unsqueeze(input, dim) --&amp;gt; Tensor
Parameters   input(Tensor) &amp;ndash; the input tensor
  dim(int) &amp;ndash; the index at which to insert the singleton dimension
  功能  Return a new tensor with a dimension of size one inserted at the specified position.
 返回一个新的tensor，在指定的位置插入维度为1的一维。
 The returned tensor shares the same underlaying data with this tensor.
 返回的这一Tensor在内存中是和原Tensor共享一个内存数据的。(可以用contiguous来重新分配)
 A dim value within the range [-input.dim() - 1, input.</description>
    </item>
    
    <item>
      <title>LeakyReLU函数解析</title>
      <link>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 14 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</guid>
      <description>LeakyReLU 语法  CLASS torch.nn.LeakyReLU(negative_slope: float = 0.01, inplace: bool = False)
 作用 Element-wise
对于每个x，应用函数如图： 函数图像 </description>
    </item>
    
    <item>
      <title>np.transpose()详解</title>
      <link>https://codefmeister.github.io/p/np.transpose%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Sat, 21 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/np.transpose%E8%AF%A6%E8%A7%A3/</guid>
      <description>ndarray的转置(transpose) 对于A是由np.ndarray表示的情况：
可以直接使用命令A.T。
也可以使用命令A.transpose()。
A.T 与 A.transpose()对比 结论: 在默认情况下，两者效果相同，但transpose()可以指定交换的axis维度。
对于一维数组，两者均不改变，返回原数组。
对于二维数组，默认进行标准的转置操作。
对于多维数组A,A.shape为(a,b,c,d,...,n)，则转置后的shape为(n,...,d,c,b,a)。
对于.transpose()，可以指定转置后的维度。语法：A.transpose((axisOrder1,...,axisOrderN))，其效果等同于np.transpose(A,(axisOrder1,...,axisOrderN)),(axisOrder)中是想要得到的索引下标顺序。效果详见例子。
Example： 二维默认情况下： A = np.array([[1,2],[3,4]]) print(A) print(A.T) print(A.transpose()) 结果如下：
多维默认情况下： a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]]) print(a.shape) print(a.T.shape) print(a.transpose().shape) 结果如下：
指定维度情况： a = np.array([[[1,2,3,4],[4,5,6,7]],[[2,3,4,5],[5,6,7,8]],[[3,4,5,6],[4,5,6,7]]]) print(a.shape) print(a.transpose(1,2,0).shape) A = np.transpose(a,(1,2,0)) print(A.shape) 结果如下：
从截图中可以看出，a.transpose(1,2,0)与np.transpose(a,(1,2,0))效果相同。代码段中给出的axes是(1,2,0)，这决定了transpose后的数组，其shape在第一个维度即shape[0]上是原来的shape[1]，第二维shape[1]是原来的shape[2]，第三维shape[2]是原来的shape[0]。所以原shape为(3,2,4)。新的shape为(2,4,3)。</description>
    </item>
    
    <item>
      <title>model.eval()作用分析</title>
      <link>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 14 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
      <description>model.eval() model.eval() 作用等同于 self.train(False)
简而言之，就是评估模式。而非训练模式。
在评估模式下，batchNorm层，dropout层等用于优化训练而添加的网络层会被关闭，从而使得评估时不会发生偏移。
总结 在对模型进行评估时，应该配合使用with torch.no_grad() 与 model.eval()：
 loop: model.train() # 切换至训练模式 train…… model.eval() with torch.no_grad(): Evaluation end loop </description>
    </item>
    
  </channel>
</rss>
