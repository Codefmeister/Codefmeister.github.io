<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on Codefmeister</title>
    <link>https://codefmeister.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on Codefmeister</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://codefmeister.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning Multiview 3D point Cloud Registration</title>
      <link>https://codefmeister.github.io/p/learning-multiview-3d-point-cloud-registration/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/learning-multiview-3d-point-cloud-registration/</guid>
      <description>Learning multiview 3D point cloud registration Abstract 提出了一种全新的，端到端的，可学习的多视角三维点云配准算法。 多视角配准往往需要两个阶段：第一个阶段进行初始化配准，给定点云各帧之间两两的初始化刚体变换关系；第二个阶段在全局意义上进行不断精细化处理。前者往往由于点云之间的低重叠率，对称性，或者重复的场景片段而导致配准精度较差。因此，紧随其后的全局优化（Global Refinement）的目标就是在多个点云帧中建立一种循环一致性(cyclic consistency).
而此文章提出了一种算法，将两个阶段融合在一起进行端对端的交替学习。在公认的基准数据集上的实验评估表明，其方法显著优于state-of-art，而且又是端到端可训练的，所需要的计算资源也更少。此外，其还进行了详细的分析和烧蚀试验去验证其方法的novel part.
1. Introduction 三维计算机视觉的下游任务，如语义分割和目标检测，通常需要场景的整体表示。因此，将仅覆盖环境一小部分的单个点云配准和融合为一个全局一致的完整表示的能力是十分重要的，而且在增强现实和机器人技术中有着不少用例。相邻片段之间的双视角配准是一个被深入研究过的问题，传统的基于几何约束[51, 66, 56] 和手工设计的特征描述符[37, 27, 54, 69]的配准方法在某种程度上取得了成功。然而， 近些年，用于双视角三维点云配准的局部描述符的研究聚焦于深度学习方法[67: 3DMatch, 38, 21: Ppfnet, 64, 19: Ppf-foldnet, 28: perfect match]，这些方法成功捕捉并编码了隐藏在手工特征符下的证据。在此基础上，一些全新的端到端的双视角点云配准方法最近被提出[62: DCP, 42: Deepvcp]。 虽然双视角配准在很多任务中展现出了不错的性能，但对场景中的多个点云帧进行配准时，其存在一些概念上的缺陷：(1) 相邻点云之间的低重合率会导致不精确或者错误的匹配。(2) 点云配准必须依赖于非常局部的特征，对于3D场景结构简单或者重复结构较多的情况十分有害。(3)在两两配准之后，需要进行单独的处理来将所有双视角配准结果组合为一个全局表示。与双视角配准相比，应用于无组织的点云片段上的全局一致的多视角配准方法能够更充分的从深度学习技术取得的进步中获益。 现有的领先方法仍然常常依赖于双视角映射的良好初始化（良好的初值），然后再在后续的步骤中通过一系列解耦的步骤进行全局优化。这种分层处理的一大缺点在于，姿态图所有节点上的全局噪声分布在配准结束后远不是随机的。也就是说，由于配准结果和初始的双视角映射高度相关，会存在着不可忽视的误差。
在这篇论文中，作者提出了第一个端到端的，数据驱动的多视角点云配准算法。其方法以可能存在重叠关系的点云集合为输入，对每个点云帧输出一个刚体变换矩阵。我们从传统的两阶段方法中跳脱出来，让各个阶段彼此分离，直接学习以一种全局一致的方式对所有点云帧进行配准。
其工作的主要贡献在于：
  将传统的两阶段方法用端到端的神经网络的方式阐述，在其前传过程中，主要解决了两个可微分的最优化问题：(i)对两两点云之间刚体变换参数估计的Procrustes问题。(ii) 刚体变换同步的谱松弛(spectral relaxation)问题
  提出了一个置信度估计模块，其使用了一个新颖的overlap pooling layer重叠池化层来预测估算得到的双视角刚体变换参数的可信度。
  将多视角三维点云配准问题转换为迭代重加权最小二乘问题(IRLS)，迭代地优化两两配准之间的刚体变换估计和绝对坐标意义下的刚体变换估计（全局）。
  因为以上所提到的工作，所提出的多视角点云配准算法是(i) 计算效率很高 (ii) 可以达到更加精确的配准结果，因为残差会以一种迭代的方式被送回双视角配准网络中去。 (iii)不论是双视角配准还是多视角配准，都比现有的方法的精度要高，效果要好。
2. Related Work Pairwise registration: 传统的双视角配准pipeline包含两个阶段： the coarse alignment stage（粗配准）， 为相对刚体变换参数提供一个初始估计；the refinement stage 通过迭代最小化配准误差，不断优化刚体变换参数。</description>
    </item>
    
    <item>
      <title>PointConv论文阅读笔记</title>
      <link>https://codefmeister.github.io/p/pointconv%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/pointconv%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>Abstract 本文发表于CVPR。 其主要内容正如标题，是提出了一个对点云进行卷积的Module，称为PointConv。由于点云的无序性和不规则性，因此应用卷积比较困难。
其主要的思路是，将卷积核当做是一个由权值函数和密度函数组成的三维点的局部坐标的非线性函数。通过MLP学习权重函数，然后通过核密度估计得到密度函数。
还有一个主要的贡献在于，使用了一种高效计算的方法，转换了公式的计算分时，使得PointConv的卷积操作变得memory efficient，从而加深网络的深度。
This paper first published on CVPR. In this paper, author proposed a novel convolution operation which can be directly used on point cloud. As we all know, unlike image whose pixels are fixed, point cloud is irregular and unordered. So directly extend convolution operation into 3D pointcloud can be difficult.
Author treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight function and density function.</description>
    </item>
    
    <item>
      <title>DCP论文阅读笔记</title>
      <link>https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/dcp%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>DCP论文阅读笔记 论文  Deep Closest Point: Learning Representations for Point Cloud Registration
Author: Wang, Yue; Solomon, Justin
 Main Attribution 基于ICP迭代最近点算法，提出基于深度学习的DCP算法。解决了ICP想要采用深度学习方法时遇到的一系列问题。
我们先回顾一下ICP算法的基本步骤：
for each iteration: find corresponding relations of points between two scan(using KNN) using SVD to solve Rotation Matrix and Translation vector update cloud Data 概括起来就是： 寻找最近点对关系，使用SVD求解刚体变换。如此循环往复。
结合论文，个人理解将ICP算法扩展到深度学习存在着以下的难点（可能存在各种问题，笔者深度学习的相关知识很薄弱）：
 首先，点对关系如果是确定的话，沿着网络反向传播可能存在问题。 SVD分解求解刚体变换，如何求梯度？(Confirmed by paper)  而文章克服了这些问题，主要有如下贡献：
 提出了能够解决传统ICP算法试图推广时存在的困难的一系列子网络架构。 提出了能进行pair-wise配准的网络架构 评估了在采用不同设置的情况下的网络表现 分析了是global feature有用还是local feature对配准更加有用  网络架构 模型包含三个部分：
(1) 一个将输入点云映射到高维空间embedding的模块，具有扰动不变性（指DGCNN当点云输入时点的前后顺序发生变化，输出不会有任何改变） 或者 刚体变换不变性（指PointNet对于旋转平移具有不变的特性）。该模块的作用是寻找两个输入点云之间的点的对应关系. 可选的模块有PointNet（Focus于全局特征）， DGCNN（结合局部特征和全局特征）。</description>
    </item>
    
    <item>
      <title>DGCNN论文解读</title>
      <link>https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Mon, 21 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/dgcnn%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</guid>
      <description>DGCNN 前言 因为关心的领域主要是配准，对于分类等网络的架构设计分析并没有侧重太多，主要侧重的是EdgeConv的思想。
论文  Dynamic Graph CNN for Learning on Point Clouds, Wang, Yue and Sun, Yongbin.
 核心思想:关于EdgeConv 将点云表征为一个图，${\rm{G}}(V,\xi )$ ,点云的每一个点对应图中的一个结点，而图中的每一条边对应的是点之间的特征feature，称为Edge-feature。举个例子，最简单的情景，可以通过KNN来构建图。Edge Feature用$e_{ij}$来表示，定义为： $$ e_{ij} = h_{\Theta}(x_i,x_j) $$ $$ h_{\Theta}: {R^F} \times {R^F} \to {R^{F&#39;}} $$ $h_{\Theta}$是一个非线性的映射，拥有一系列可学习的参数。
提出了一个名为EdgeConv的神经网络模块Module，该模块基于卷积神经网络，可以适应在点云上的高阶任务。EdgeConv的对于第i个顶点的输出为：
其中$□$代表的是一个对称聚合函数，如$\Sigma, max$。
可以将上述描述类比为在图像上的卷积操作。我们把$x_i$看作是中心像素点，而$x_j:(i,j) \in \xi$可以看做是围绕在点$x_i$周围的像素($x_j$事实上就是和$x_i$之间存在着feature edge的点）。所以类比这样的卷积操作，Edge-Conv可以将n个点的$F$维点云通过“卷积”转换为具有n个点的$F&#39;$维的点云。 所以选择$h$和$□$就变得十分关键。它会直接影响EdgeConv的性能特性。
一些其他的选择在下一个小part中讨论。在本文中，作者采用的： $$ h_{\Theta}(x_i,x_j) = {\bar h}_{\Theta}(x_i, x_j - x_i) $$
从这个表达式可以非常明显的看出，既结合了全局形状结构，也结合了局部的结构信息。Global shape structure通过$x_i$捕捉，local neighborhood information通过$x_j - x_i$来捕捉。
更具体一点的说，通过如下两个公式来计算edge_feature以及x&#39;： $$ e_{ijm}&#39; = ReLU(\theta_m \cdot(x_j - x_i) + \phi_m \cdot x_i) $$ $$ x_{im}&#39; = \mathop {\max }\limits_{j:(i,j) \in \xi }e_{ijm}&#39; $$</description>
    </item>
    
    <item>
      <title>图解Transformer</title>
      <link>https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/</link>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://codefmeister.github.io/p/%E5%9B%BE%E8%A7%A3transformer/</guid>
      <description>图解Transformer  Reference: The Illustrated Transformer
 本文自译用于加深理解与印象。
关于注意力机制，可以参考先前的Seq2Seq Model with Attention
Transformer是论文Attention is All You Need提出的。在这篇文章中，我们将尝试把事情弄得简单一点，逐个介绍概念，以便更好理解。
A High-Level Look 我们首先把模型看作是一个黑箱。在机器翻译领域的应用中，输入一种语言的一个句子，会输出另外其翻译结果。 揭开盖子，我们能够看到一个编码组件encoding component，一个解码组件decoding component，还有其之间的连接关系connections。 编码组件是一堆编码器构成的（Paper中堆叠了六个编码器，六个并没有什么说法，你也可以尝试其他数字）。解码组件也是由一堆解码器构成的（数量与编码器相同）。 所有编码器在结构上都是相同的，然而他们并不共享参数（或权重）。 每一个都可以被拆分为两个子层sub-layers。 编码器的输入首先流过self-attention层，self-attention层可以帮助我们在对某个特定的词进行编码的时候同时关注到句子中其他位置单词的影响。
self-attention层的输出被送往feed-foward neural network，即前馈神经网络层。完全相同的前馈网络，独立地作用于每一个位置position上。
解码器也有上述这两个层，但除此以外，在这两层之间，还有一个attention layer，帮助解码器更加关注输入句子中相关的部分。（作用类似于Seq2Seq中的注意力机制的作用。） Bringing The Tensor Into The Picture 现在，我们已经了解了模型的主要组件，下面让我们开始研究各种矢量/张量以及它们如何在这些组件之间流动，以将经过训练的模型的输入转换为输出。
首先我们将每一个输入单词通过embedding algorithm转换为一个词向量。 嵌入过程只发生在最底部的encoder。对于所有的编码器Encoder，他们都接受一个size为512的向量列表作为输入。只不过对于最底部的Encoder，其输入为单词经过嵌入后得到的词向量，而其他的Encoder的输入，是其下方一层Encoder的输出。列表的size是一个我们可以设定的超参数——通常来讲它会是我们训练集中最长的一个句子的长度。
在将输入序列中的单词进行Embedding之后，他们中的每一个都会流过编码器的两层。 从这里我们可以看到一个Transformer非常重要的特性，那便是每一个位置上的单词在Encoder中自己的路径上各自流动。在self-attention层中，这些路径之间存在相互依赖。而前馈层feed-forward中彼此间并无依赖。所以在流经前馈层的时候，可以进行并行化处理。
下面我们将举一个短句的例子，然后观察sub-layer上发生了什么。
Now We&amp;rsquo;re Encoding 像我们先前提到的，一个编码器接收一个向量列表作为输入。这个向量列表首先被送往self-attention层，然后再送往feed-forward前馈层。处理结束后将其output送往下一个Encoder。  每个位置的单词都被送往一个self attention层，然后再穿过一个前馈神经网络——每个向量独立穿过这个完全相同的网络。
 Self-Attention at a High Level self-attention是Paper中提出的一个全新概念，不要被其简单的命名给迷惑。
假设我们输入了如下一个句子，并试图进行翻译：
&amp;ldquo;The animal didn&amp;rsquo;t cross the street because it was too tired&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Conv1d与Conv2d</title>
      <link>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</link>
      <pubDate>Wed, 16 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/conv1d%E4%B8%8Econv2d/</guid>
      <description>Conv1d与Conv2d 本文分为几个部分来详解Conv2d与Conv1d。主要侧重于Conv2d
前言 本文记于2020年12月15日，起因是DGCNN中部分卷积使用了二维卷积，部分卷积使用了一维卷积。加之之前对Conv2d与Conv1d属于一种迷迷糊糊的状态，趁着这个机会弄清楚。
Conv2d原理（二维卷积层） 二维互相关运算 互相关运算与卷积运算 虽然卷积层得名于卷积(convolution)运算，但所有框架在实现卷积层的底层，都采用的是互相关运算。实际上，卷积运算与互相关运算类似，为了得到卷积运算的输出，我们只需要将核数组左右翻转并上下翻转，然后再与输入数组做互相关运算。所以这两种运算虽然类似，但是输出并不相同。
但是由于深度学习中核数组都是学习得到的，所以卷积层无论使用互相关运算还是卷积运算，都不影响模型预测时的输出。也就是说我们用卷积运算学出的核数组与用互相关运算学出的核数组两者之间可以通过上下翻转，左右翻转来相互转换。所以在框架乃至于绝大部分深度学习文献中，都使用互相关运算来代替了卷积运算。
互相关运算 在二维卷积层中，一个二维输入数组和一个二维核(kernel)数组通过互相关运算输出一个二维数组。举个例子来解释二维互相关运算：
假设输入数组的高和宽均为3， 核数组的高和宽均为2，该数组在卷积运算中又称为卷积核或者过滤器(filter)。 19是这样得出的： $19 = 0\times0 + 1\times1 + 3\times2 + 4\times3$ 。
卷积窗口从输入数组的最左上方开始，按照从左往右，从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和。得到输出数组中对应位置的元素。
二维卷积层 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包含了卷积核和标量偏差。我们在训练模型的时候，通常先对卷积层进行随机的初始化，然后不断迭代卷积核和偏差。
卷积窗口形状为$p \times q$的卷积层称为$p \times q$卷积层。
特征图与感受野 二维卷积层输出的二维数组可以看做是输入在空间维度上(宽和高)上某一级的表征，也叫特征图(feature map)。影响元素$x$的前向计算的所有可能输入区域(甚至可能大于输入的实际尺寸)叫做$x$的感受野(receptive field)。以上图为例，图中输入的阴影部分的四个元素就是输出数组中阴影部分元素的感受野。如果我们将该输出再和一个$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么$z$在输入上的感受野包含全部的9个元素。
可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。
填充与步幅 卷积层的输出形状由输入形状和卷积核窗口形状决定，通过填充与步幅，我们可以改变给定形状的输入和卷积层下的输出形状。
填充 填充padding是指在输入高和宽的两侧填充元素(通常是0元素)。如下图： 假设输入形状为$n_h \times n_w$， 卷积核窗口形状是$k_h \times k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是： $$ (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1) $$ 很多情况下我们会设置$p_h = k_h -1$和$p_w = k_w - 1$来使得输入输出具有相同的高和宽。</description>
    </item>
    
    <item>
      <title>LeakyReLU函数解析</title>
      <link>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 14 Dec 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/leakyrelu%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</guid>
      <description>LeakyReLU 语法  CLASS torch.nn.LeakyReLU(negative_slope: float = 0.01, inplace: bool = False)
 作用 Element-wise
对于每个x，应用函数如图： 函数图像 </description>
    </item>
    
    <item>
      <title>model.eval()作用分析</title>
      <link>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 14 Nov 2020 09:40:49 +0800</pubDate>
      
      <guid>https://codefmeister.github.io/p/model.eval%E4%BD%9C%E7%94%A8%E5%88%86%E6%9E%90/</guid>
      <description>model.eval() model.eval() 作用等同于 self.train(False)
简而言之，就是评估模式。而非训练模式。
在评估模式下，batchNorm层，dropout层等用于优化训练而添加的网络层会被关闭，从而使得评估时不会发生偏移。
总结 在对模型进行评估时，应该配合使用with torch.no_grad() 与 model.eval()：
 loop: model.train() # 切换至训练模式 train…… model.eval() with torch.no_grad(): Evaluation end loop </description>
    </item>
    
  </channel>
</rss>
